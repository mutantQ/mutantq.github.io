---
layout: post
title: "Day 13: Day 13 역전파의 계산"
date: 2023-01-20 10:00:00
description: 딥러닝의 기초 - Day 13
tags: deep-learning tutorial korean education series
categories: education
---


### CH4 | Backpropagation Calculus

[https://www.youtube.com/watch?v=tIeHLnjs5U8](https://www.youtube.com/watch?v=tIeHLnjs5U8)

### 문제 4.1 (HARD)

이 문제의 목표는 함수 $f_{\textrm{true}}$를 입출력 관계 데이터만을 사용하여 근사하는 것이다. 예를 들어, $f_{\textrm{true}}$가 점 $(1, 2),\:(4,-6),\:(7,3),\:(10,1),\cdots$을 지나간다는 것을 알고 있다면, 이 점들을 가깝게 지나가는 적절한 함수 $f$를 찾아 $f_{\textrm{true}}$를 근사할 수 있다. 우리는 인공신경망을 사용하여 이 근사 함수 $f$를 찾으려고 한다.

$f_{\textrm{true}}$의 입출력 순서쌍 $N$개, 즉 고정된 $(x_1,y_1),\:(x_2,y_2),\cdots,\:(x_N,y_N)$를 알고 있다고 가정하자. 그러면 $f_{\textrm{true}}$가 이 데이터 점들을 지나가야 하므로,

$$

y_i=f_{\textrm{true}}(x_i), \;\;i=1,2,\cdots,N.
$$

문제를 간단하게 하기 위해 각 층의 너비(= 각 층의 뉴런 수)가 $1$인 $L$개의 층을 쌓아서 만든 심층 신경망 $f_\theta$를 고려하자. 또한 $L=3$이라고 가정하자. 이 신경망이 기본 함수 $f_{\textrm{true}}$를 근사할 수 있기를 바란다. 이 신경망은 단순히 $L$개의 미분 가능한 스칼라 일변수 함수 $f^{(l)}\;(l=1,2,\cdots,L)$의 합성으로 간주할 수 있다:

$$
f=f^{(L)} \circ f^{(L-1)} \circ \cdots \circ f^{(1)}.
$$

각 $f^{(l)}$는 이전 활성도 $a^{(l-1)}$을 새로운 활성도 $a^{(l)}$로 대응시킨다. 이는 다음 공식에 의해 이루어진다:

$$
a^{(l)}=\rho (w^{(l)}a^{(l-1)}+b^{(l)})=: f^{(l)}(a^{(l-1)}),
$$

여기서 학습 가능한 매개변수 $w^{(l)}$와 $b^{(l)}$는 각각 가중치와 편향이다. 활성화 함수 $\rho$는 $\rho(x)=\textrm{ReLU}(x)=\max\{0,x\}$로 주어진다. 편의를 위해 중간 변수 $z^{(l)} := w^{(l)}a^{(l-1)}+b^{(l)}$를 정의하여 $a^{(l)}=\rho\left(z^{(l)}\right)$로 나타내자. 입력은 초기 활성도와 같아야 하므로 $a^{(0)}=x_i$이다. 신경망 $f_\theta$의 첨자 $\theta$는 신경망의 모든 학습 가능한 매개변수의 순서쌍을 나타낸다.

- **학습 가능한 매개변수란?**
    
    학습 가능한 매개변수는 신경망에서 자유롭게 선택할 수 있어 (고정된) 데이터에 적합하게 조정할 수 있는 매개변수이다. 각 매개변수는 주어진 데이터셋에서 신경망의 성능을 높일 수 있는 조절기로 생각할 수 있다. 여기서 $x_i$ 및 $y_i$는 데이터이므로 학습 가능한 매개변수가 아니다. 대부분의 경우 학습 가능한 매개변수는 가중치와 편향을 의미한다.
    

비용 함수 $C$는 마지막 활성도와 실제 함수 값 사이의 제곱 거리의 합으로 정의된다. $C$를 최소화하는 데 성공하면 우린 자신 있게 신경망 $f_\theta$로 함수 $f_{\textrm{true}}$를 근사했다고 말할 수 있다.

$$
C(\theta):=\sum_{i=1}^{N}(a_{L,i}-y_i)^2=\sum_{i=1}^{N}(f_\theta(x_i)-f_{\textrm{true}}(x_i))^2=\sum_{i=1}^{N}(f_\theta(x_i)-y_i)^2
$$

다음 질문에 답하시오:

(a) 학습 가능한 매개변수는 몇 개인가? 즉, 순서쌍 $\theta$의 길이는 얼마인가?

(b) $L=3$일 때 신경망 $f_\theta$의 전체 트리 다이어그램을 그리시오 (2:14 참고).

(c) 데이터 포인트가 $(1, 2),\:(4,-6),\:(7,3),\:(10,1)$ 네 개 있다고 가정하라. 모든 가중치와 편향이 $1$과 같으면, 즉 $\theta=(1,1,\cdots,1)$이면, 비용 함수 $C(\theta)$의 값은 얼마인가?

(d) $C(\theta)$를 최소화하려면 경사 하강법을 사용하여 매개변수 $\theta$를 업데이트해야 한다. $\theta=(1,1,\cdots,1)$에서의 그래디언트 $\nabla C(\theta)$를 계산하시오. 연쇄 법칙을 사용하시오.

(e) 매개변수가 $\theta$에서 $\theta'=\theta-0.1\nabla C(\theta)$로 업데이트되었다. $\theta'$을 계산하시오.

### 문제 4.2 (HARD)

이제, 마지막 층 $f^{(L)}$을 제외한 각 층의 너비가 $1$에서 $2$로 증가했다고 가정하자. 이 경우 $L=2$이다.

너비가 증가했으므로, 각 층 $f^{(l)}\;(l=1,2,\cdots,L)$는 이제 다음 공식을 따르는 미분 가능한 다변수 벡터 함수이다.

$$
\mathbf{a}^{(l)}=\rho (W^{(l)}\mathbf{a}^{(l-1)}+\mathbf{b}^{(l)})=:f^{(l)}(\mathbf{a}^{(l-1)})
$$

여기서 $W^{(l)}$와 $\mathbf{b}^{(l)}$는 $l$-번째 층의 가중치 행렬과 바이어스(편향) 벡터이다.

(a) 학습 가능한 파라미터가 몇 개인지 구하라. 순서쌍 $\theta$의 길이를 결정하시오.

(b) 신경망 $f_\theta$의 전체 트리 다이어그램을 그리시오.

(c) 데이터 포인트가 $(1, 2),\:(4,-6)$ 두 개 있다고 가정하라. 모든 가중치와 바이어스가 $1$이라고 가정했을 때, 비용 함수 $C(\theta)$를 계산하시오.

(d) $\theta=(1,1,\cdots,1)$에서의 기울기 $\nabla C(\theta)$를 계산하라. 다변수 연쇄 법칙을 사용하시오.

(e) $\theta'=\theta-0.1\nabla C(\theta)$일 때, $\theta'$를 계산하시오.

### 문제 4.3 (모라벡의 역설)

![보이저엑스(VoyagerX) 인공지능 스타트업의 남세동 대표가 최근 작성한 페이스북 게시물](/assets/img/blog/deep-learning/untitled_day_13_161f0f24f93180a4a025d44.png)

보이저엑스(VoyagerX) 인공지능 스타트업의 남세동 대표가 최근 작성한 페이스북 게시물

2010년대에 들어서자 본격적으로 주목을 끌만한 인공지능 연구 결과들이 발표되기 시작하였고, 감정, 직관, 통찰과 같은 요소도 기계어의 형태로 처리가 가능하다는 주장에 무게가 더해졌다. 모라벡의 역설로 널리 알려져 있는, “컴퓨터는 인간이 잘 하는 것을 못하는 현상“도 이제는유효하지 않게 된 것이다. 모라벡늬 역설과 보편 근사 정리(Universal Approximation Theorem)에 대해 조사하고, 다음 질문에 답하여라:

(a) 보편 근사 정리를 간략히 설명하여라. 보편 근사 정리의 가장 큰 의의는 무엇인가?

- 예시 답안
    
    $\mathbb{R}^n$에서 $\mathbb{R}^m$으로 가는 임의의 연속함수는 무한한 깊이 또는 무한한 너비를 가지는 인공신경망에 의해 근사 가능하다.
    
    인간이 처리하는 정보가 모두 벡터로 표현 가능하므로, 벡터를 다른 벡터로 변환하는 임의의 함수를 만들 수만 있다면 인간의 기능을 모두 컴퓨터로 구현 가능할 것임을 의미한다.
    

(b) 보편 근사 정리는 이미 1960년대에 인공신경망이 사람의 기능을 포함한 임의의 기능을 근사할 수 있다는 이론적인 근거를 제시하고 있었다. 그럼에도 불구하고 모라벡의 역설이 최근까지 지속된 이유는 무엇일까?