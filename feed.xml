<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mutantq.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mutantq.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-21T13:36:37+00:00</updated><id>https://mutantq.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yejun Jang is an AI researcher specializing in reinforcement learning and deep learning at Seoul National University. </subtitle><entry xml:lang="ko"><title type="html">í•˜ë“œì›¨ì–´ ì—”ì§€ë‹ˆì–´ ê³µë™ì°½ì—…ìë¥¼ ì°¾ìŠµë‹ˆë‹¤! â€” mutual</title><link href="https://mutantq.github.io/blog/2025/mutual-cofounder-hardware-kr/" rel="alternate" type="text/html" title="í•˜ë“œì›¨ì–´ ì—”ì§€ë‹ˆì–´ ê³µë™ì°½ì—…ìë¥¼ ì°¾ìŠµë‹ˆë‹¤! â€” mutual"/><published>2025-12-05T03:00:00+00:00</published><updated>2025-12-05T03:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/mutual-cofounder-hardware-kr</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/mutual-cofounder-hardware-kr/"><![CDATA[<p><img src="/assets/img/mutual-logo.jpg" alt="mutual ë¡œê³ "/></p> <p><strong>â€œì´ ì˜ìƒ, ì§„ì§œì•¼?â€ ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ì„ ë§Œë“­ë‹ˆë‹¤.</strong></p> <p>AIê°€ ë§Œë“  ì˜ìƒê³¼ ì§„ì§œ ì˜ìƒ, ì´ì œ ëˆˆìœ¼ë¡œëŠ” êµ¬ë¶„ì´ ì•ˆ ë©ë‹ˆë‹¤. ì†Œí”„íŠ¸ì›¨ì–´ë¡œ ê°€ë ¤ë‚´ëŠ” ê±´ í•œê³„ê°€ ìˆê³ ìš”.<br/> mutualì€ ë‹¤ë¥´ê²Œ ì ‘ê·¼í•©ë‹ˆë‹¤. ì¹´ë©”ë¼ê°€ ì°ëŠ” ìˆœê°„, í•˜ë“œì›¨ì–´ ì•ˆì—ì„œ ë°”ë¡œ ì„œëª…í•©ë‹ˆë‹¤. ì†Œí”„íŠ¸ì›¨ì–´ê°€ ì†ëŒˆ í‹ˆì´ ì—†ì–´ìš”.</p> <p>íšŒì‚¬ ê°„ëµ ì†Œê°œ: <a href="/blog/2025/introducing-mutual-kr/">mutualì„ ì†Œê°œí•©ë‹ˆë‹¤</a> <br/> í™”ì´íŠ¸í˜ì´í¼(PDF): <a href="/assets/pdf/SRA-2025-10-05.pdf">Signing Right Away</a></p> <h2 id="ëˆ„ê°€-íˆ¬ìí–ˆë‚˜ìš”">ëˆ„ê°€ íˆ¬ìí–ˆë‚˜ìš”?</h2> <p><strong>ìš°ê²½ì‹ (Kay Kyungsik Woo)</strong></p> <p>ë¸”ë¡ì²´ì¸ ê¸°ë°˜ ëª¨ë¹Œë¦¬í‹° í”Œë«í¼ <a href="https://mvlchain.io">MVL Foundation</a>ì˜ ì°½ì—…ìì´ì CEOì…ë‹ˆë‹¤. MVLì´ ë§Œë“  TADAëŠ” 200ë§Œ ìœ ì €, 20ë§Œ ë“œë¼ì´ë²„ê°€ ì“°ëŠ” ì„œë¹„ìŠ¤ì˜ˆìš”. 400ì–µ ì´ìƒ íˆ¬ì ìœ ì¹˜, ê¸€ë¡œë²Œ 250ëª… ê·œëª¨, 2022ë…„ í‘ì ì „í™˜. ì‹¤ì œë¡œ ìŠ¤ì¼€ì¼í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ì–´ë³¸ ë¶„ì…ë‹ˆë‹¤.</p> <h2 id="ë¬´ìŠ¨-ì¼ì„-í•˜ê²Œ-ë˜ë‚˜ìš”">ë¬´ìŠ¨ ì¼ì„ í•˜ê²Œ ë˜ë‚˜ìš”?</h2> <ul> <li>FPGAë¡œ ì•”í˜¸í™” ê°€ì†ê¸° ì„¤ê³„ (AES-GCM/CCM ê°™ì€ ê²ƒë“¤)</li> <li>ì´ë¯¸ì§€ ì„¼ì„œì—ì„œ TEEê¹Œì§€, ì•ˆì „í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•</li> <li>SRA ì•„í‚¤í…ì²˜ì˜ ë ˆí¼ëŸ°ìŠ¤ êµ¬í˜„</li> </ul> <h2 id="ì´ëŸ°-ë¶„ì„-ì°¾ìŠµë‹ˆë‹¤">ì´ëŸ° ë¶„ì„ ì°¾ìŠµë‹ˆë‹¤</h2> <ul> <li>Verilogë‚˜ HDL ë‹¤ë¤„ë³¸ ê²½í—˜</li> <li>2025ë…„ ê²¨ìš¸ ~ 2026ë…„ ë´„ ì‚¬ì´ì— í’€íƒ€ì„ ê°€ëŠ¥</li> <li>ì˜ì–´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì— ë¬¸ì œ ì—†ëŠ” ë¶„ (í•´ì™¸ ì¶œì¥, ê¸€ë¡œë²Œ íŒŒíŠ¸ë„ˆ ë¯¸íŒ… ìˆì–´ìš”)</li> </ul> <h2 id="ìˆìœ¼ë©´-ì¢‹ì•„ìš”">ìˆìœ¼ë©´ ì¢‹ì•„ìš”</h2> <ul> <li>ì•”í˜¸í™” ê°€ì†ê¸° ì„¤ê³„í•´ë³¸ ì  ìˆë‹¤ë©´ ìµœê³ </li> <li>MIPI CSI-2ë‚˜ ì¹´ë©”ë¼ ì¸í„°í˜ì´ìŠ¤ ì•Œë©´ í”ŒëŸ¬ìŠ¤</li> </ul> <h2 id="ë“œë¦´-ìˆ˜-ìˆëŠ”-ê²ƒ">ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒ</h2> <ul> <li><strong>ì§€ë¶„ ê¸°ë³¸ 10%, ìµœëŒ€ 20%</strong> â€” ê³µë™ì°½ì—…ì ë ˆë²¨ì…ë‹ˆë‹¤</li> <li><strong>í•´ì™¸ ì¶œì¥</strong> â€” ì»¨í¼ëŸ°ìŠ¤, íŒŒíŠ¸ë„ˆ ë¯¸íŒ… ë“±</li> <li>ì–´ë µì§€ë§Œ ì˜ë¯¸ ìˆëŠ” ë¬¸ì œ</li> </ul> <hr/> <p><strong>ê¶ê¸ˆí•˜ì‹  ê²Œ ìˆë‹¤ë©´</strong> â†’ <a href="mailto:jangyejun@snu.ac.kr">jangyejun@snu.ac.kr</a></p>]]></content><author><name></name></author><category term="hiring"/><category term="startup"/><category term="hiring"/><category term="cofounder"/><category term="hardware"/><category term="fpga"/><category term="verilog"/><summary type="html"><![CDATA[AI ì‹œëŒ€ì˜ ì‹ ë¢° ì¸í”„ë¼ë¥¼ êµ¬ì¶•í•´ë‚˜ê°ˆ ì•¼ë§ ìˆëŠ” ë¶„ì„ ëª¨ì‹­ë‹ˆë‹¤]]></summary></entry><entry xml:lang="en"><title type="html">Co-Founder (Hardware Engineer) â€” mutual</title><link href="https://mutantq.github.io/blog/2025/mutual-cofounder-hardware/" rel="alternate" type="text/html" title="Co-Founder (Hardware Engineer) â€” mutual"/><published>2025-12-05T03:00:00+00:00</published><updated>2025-12-05T03:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/mutual-cofounder-hardware</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/mutual-cofounder-hardware/"><![CDATA[<p><img src="/assets/img/mutual-logo.jpg" alt="mutual logo"/></p> <p><strong>We make digital content provably authentic at the hardware level.</strong></p> <p>AI-generated media is indistinguishable from reality. Software detection doesnâ€™t work. <br/> At mutual, weâ€™re building cryptographic signing directly into camera hardwareâ€”so content is authenticated the moment itâ€™s captured, before it ever touches software.</p> <p>Corporate introduction: <a href="/blog/2025/introducing-mutual/">Introducing mutual</a> <br/> Whitepaper (PDF): <a href="/assets/pdf/SRA-2025-10-05.pdf">Signing Right Away</a></p> <h2 id="backed-by">Backed By</h2> <p><strong>Kay Kyungsik Woo</strong> â€” Founder &amp; CEO of <a href="https://mvlchain.io">MVL Foundation</a>, the company behind TADA, a blockchain-based ride-hailing platform operating across Southeast Asia. Kay brings deep experience in building trust infrastructure for real-world applications.</p> <h2 id="what-youll-build">What Youâ€™ll Build</h2> <ul> <li>Cryptographic accelerators on FPGA (AES-GCM/CCM, authenticated encryption)</li> <li>Secure pipelines between image sensors and trusted execution environments</li> <li>Reference implementation for our SRA (Signing Right Away) architecture</li> </ul> <h2 id="requirements">Requirements</h2> <ul> <li>Verilog or HDL experience</li> <li>Can go full-time between Winter 2025 â€“ Spring 2026</li> </ul> <h2 id="nice-to-have">Nice to Have</h2> <ul> <li>Crypto accelerator design experience</li> <li>MIPI CSI-2 / camera interface knowledge</li> </ul> <h2 id="what-we-offer">What We Offer</h2> <ul> <li><strong>Default 10%, up to 20% equity</strong> â€” true co-founder stake</li> <li><strong>Hybrid-remote</strong> â€” work remotely, but able to be in Seoul periodically for in-person collaboration</li> <li>Hard technical problem with real-world impact</li> </ul> <hr/> <p><strong>Interested?</strong> â†’ <a href="mailto:jangyejun@gmail.com">jangyejun@gmail.com</a></p>]]></content><author><name></name></author><category term="hiring"/><category term="startup"/><category term="hiring"/><category term="cofounder"/><category term="hardware"/><category term="fpga"/><category term="verilog"/><summary type="html"><![CDATA[We're building the trust layer for the post-AI media era. Looking for a technical co-founder.]]></summary></entry><entry xml:lang="ko"><title type="html">mutualì„ ì†Œê°œí•©ë‹ˆë‹¤</title><link href="https://mutantq.github.io/blog/2025/introducing-mutual-kr/" rel="alternate" type="text/html" title="mutualì„ ì†Œê°œí•©ë‹ˆë‹¤"/><published>2025-12-05T01:00:00+00:00</published><updated>2025-12-05T01:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/introducing-mutual-kr</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/introducing-mutual-kr/"><![CDATA[<p><img src="/assets/img/mutual-logo.jpg" alt="mutual ë¡œê³ "/></p> <div style="max-width: 960px; margin: 0 auto 2rem auto;"> <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"> <iframe src="https://www.youtube.com/embed/Ev9knl_Zbsg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" class="rounded z-depth-1" frameborder="0" allowfullscreen=""></iframe> </div> </div> <p>ìŠ¤ë§ˆíŠ¸í° ì¹´ë©”ë¼ ì…”í„°ë¥¼ ëˆ„ë¥´ëŠ” ìˆœê°„ë¶€í„° ì‚¬ì§„ì´ ê°¤ëŸ¬ë¦¬ì— ëœ¨ê¸°ê¹Œì§€, ì•ˆì—ì„œëŠ” ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚ ê¹Œ.</p> <p>ë‚˜ëŠ” ìš”ì¦˜ "ì¹´ë©”ë¼ì™€ ë§ˆë”ë³´ë“œ ì‚¬ì´ì˜ ë³´ì•ˆ"ì„ ë³´ê³  ìˆë‹¤. ì´ í•œ ì¤„ì„ ì´ì•¼ê¸°í•˜ë©´ ì‚¬ëŒë“¤ì€ ë³´í†µ ì¹´ë©”ë¼ í•´í‚¹, ë„ì´¬, ëœì„¬ì›¨ì–´ ê°™ì€ ê²ƒì„ ë– ì˜¬ë¦°ë‹¤. í•˜ì§€ë§Œ ë‚´ê°€ ì •ë§ë¡œ ì‹ ê²½ ì“°ëŠ” ê±´ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤.</p> <p>ì§ˆë¬¸ì„ í•˜ë‚˜ ë˜ì ¸ë³´ì.</p> <blockquote> <p>"ì•ìœ¼ë¡œ 5ë…„ ë’¤, ìš°ë¦¬ê°€ ë³´ëŠ” ì˜ìƒê³¼ ì‚¬ì§„ ì¤‘ ì–¼ë§ˆë‚˜ ë§ì€ ë¹„ìœ¨ì´ ìƒì„±í˜• AIê°€ ë§Œë“  ê²ƒì¼ê¹Œ?"</p> </blockquote> <p>ì •ë‹µì„ ì•„ëŠ” ì‚¬ëŒì€ ì—†ì§€ë§Œ, í•œ ê°€ì§€ëŠ” ë¶„ëª…í•˜ë‹¤. ì–´ë–¤ ì´ë¯¸ì§€ê°€ ì§„ì§œì¸ì§€ ì•„ë‹Œì§€ <strong>í†µê³„ì ìœ¼ë¡œ</strong> êµ¬ë¶„í•˜ëŠ” ê²ƒì€ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ì´ ì¶©ë¶„íˆ ë°œì „í•˜ë©´ ë¶ˆê°€ëŠ¥í•´ì§ˆ ê²ƒì´ë‹¤. â€œì¸ê³µì§€ëŠ¥ ëª¨ë¸ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€â€ì˜ ë¶„í¬ê°€ â€œì‹¤ì œ ì¹´ë©”ë¼ë¡œ ì°íŒ ì´ë¯¸ì§€â€ì˜ ë¶„í¬ë¡œ ìˆ˜ë ´í•  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤.</p> <p>ì´ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í’€ ìˆ˜ ìˆì„ê¹Œ. ìµœê·¼ ëª‡ ë…„ ì‚¬ì´, ë¯¸êµ­ê³¼ ìœ ëŸ½ì˜ ë¹…í…Œí¬ ê¸°ì—…ë“¤ì´ í•˜ë‚˜ì˜ ë°©í–¥ì„ ì¡ì•˜ë‹¤. Adobeê°€ ì£¼ë„í•˜ê³  Google, Sony, OpenAI ë“±ì´ ì°¸ì—¬í•˜ëŠ” ì—°í•©ì´ ìˆë‹¤. <strong>C2PA(Coalition for Content Provenance and Authenticity)</strong>ë¼ëŠ” ì´ë¦„ì˜ ê¸°ìˆ  í‘œì¤€ì´ë‹¤.</p> <p>C2PAì˜ ëª©í‘œëŠ” ë‹¨ìˆœí•˜ë‹¤. ì½˜í…ì¸ ê°€ ë§Œë“¤ì–´ì§€ëŠ” ì „ ê³¼ì •ì„ ê¸°ë¡í•˜ê³ , ê·¸ ê¸°ë¡ì„ ì•”í˜¸í•™ì ìœ¼ë¡œ ë³´í˜¸í•œ ë’¤, ë‚˜ì¤‘ì— ëˆ„ê°€ ì–¸ì œë“  ê²€ì¦í•  ìˆ˜ ìˆê²Œ í•˜ìëŠ” ê²ƒì´ë‹¤. ë§í•˜ìë©´ ë””ì§€í„¸ ì½˜í…ì¸ ì˜ "ì œì‘ ì´ë ¥ì„œ"ë¥¼ ë‚¨ê¸°ëŠ” ì¼ì´ë‹¤.</p> <p>ì—¬ê¸°ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ì§ˆë¬¸ì´ ë‚˜ì˜¨ë‹¤.</p> <blockquote> <p>"ê·¸ë ‡ë‹¤ë©´, ê·¸ ê¸°ë¡ì˜ ê°€ì¥ ì²˜ìŒì€ ì–´ë””ì—¬ì•¼ í• ê¹Œ?"</p> </blockquote> <p>ë¬¸ì„œ í¸ì§‘ê¸°? ì‚¬ì§„ í¸ì§‘ í”„ë¡œê·¸ë¨? í´ë¼ìš°ë“œ ì„œë²„?</p> <p>ìš°ë¦¬ëŠ” ê·¸ ë‹µì´ <strong>ì„¼ì„œ</strong>, ì¦‰ ì¹´ë©”ë¼ê°€ ì„¸ìƒì„ ì²˜ìŒ ë°›ì•„ë“¤ì´ëŠ” ê·¸ ì§€ì ì— ìˆë‹¤ê³  ë¯¿ëŠ”ë‹¤.</p> <p>í•œ ë²ˆ ì„¼ì„œë¥¼ ë– ë‚œ ì‹ í˜¸ëŠ”, ì´ë¡ ìƒ ì–¼ë§ˆë“ ì§€ ë³µì œë˜ê³  ì¡°ì‘ë  ìˆ˜ ìˆë‹¤. ë°˜ëŒ€ë¡œ ë§í•˜ë©´, <strong>ì„¼ì„œì—ì„œ ì¹´ë©”ë¼ í”„ë¡œì„¸ì„œë¡œ ë„˜ì–´ê°€ëŠ” ê·¸ ìˆœê°„</strong>ì— ì‹ í˜¸ë¥¼ ì ê·¸ê³  ì„œëª…í•  ìˆ˜ ìˆë‹¤ë©´, ì´í›„ì˜ ëª¨ë“  ë‹¨ê³„ëŠ” ê·¸ ì„œëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦í•  ìˆ˜ ìˆë‹¤.</p> <p>mutualì€ ë°”ë¡œ ê·¸ ì§€ì ì„ ë‹¤ë£¨ëŠ” íšŒì‚¬ë‹¤.</p> <h2 id="ìš°ë¦¬ê°€-ë³´ëŠ”-ë¬¸ì œ">ìš°ë¦¬ê°€ ë³´ëŠ” ë¬¸ì œ</h2> <p>ë ˆëª¬ ë§ˆì¼“(lemon market)ì€ ì¢‹ì€ ìƒí’ˆê³¼ ë‚˜ìœ ìƒí’ˆì´ ì„ì—¬ ìˆëŠ” ì‹œì¥ì„ ëœ»í•œë‹¤. ê²‰ìœ¼ë¡œëŠ” í’ˆì§ˆì„ ì•Œê¸° ì–´ë µê¸° ë•Œë¬¸ì—, êµ¬ë§¤ìëŠ” í•­ìƒ â€œìµœì•…ì˜ ê²½ìš°â€ë¥¼ ì—¼ë‘ì— ë‘ê³  ë‚®ì€ ê°€ì¹˜ë¥¼ ë¶€ì—¬í•˜ê²Œ ë˜ê³ , ê·¸ ê²°ê³¼ë¡œ í’ˆì§ˆì´ ì¢‹ì€ ìƒí’ˆ(ì˜ˆ: ì§„ì§œ ì˜ìƒ)ì˜ ê°€ì¹˜ë„ í•¨ê»˜ ë–¨ì–´ì§€ê²Œ ëœë‹¤.</p> <p>ì˜¤ëŠ˜ì˜ ì¸í„°ë„·ì€ ì¼ì¢…ì˜ ë ˆëª¬ ë§ˆì¼“ì— ê°€ê¹ë‹¤. ì¢‹ì€ ì •ë³´ì™€ ë‚˜ìœ ì •ë³´, ì§„ì§œ ì˜ìƒê³¼ ê°€ì§œ ì˜ìƒì´ ì„ì—¬ ìˆê³ , ë‘˜ì˜ ì°¨ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ë‹¤ë©´ ì†Œë¹„ìë“¤ì€ ëª¨ë“  ì»¨í…ì¸ ë¥¼ ë¶ˆì‹ í•˜ê²Œ ëœë‹¤. ì •ë³´ ë¹„ëŒ€ì¹­ì´ ì‹¬í•´ì§ˆìˆ˜ë¡ ì‹ ë¢°ëŠ” ë–¨ì–´ì§€ê³ , ê²°êµ­ ëª¨ë‘ê°€ ì†í•´ë¥¼ ë³´ëŠ” êµ¬ì¡°ë‹¤.</p> <p>ì§€ê¸ˆê¹Œì§€ì˜ ë§ì€ ì‹œë„ëŠ” "ì‚¬í›„ íƒì§€"ì— ì´ˆì ì„ ë§ì·„ë‹¤. ì´ë¯¸ ìƒì„±ëœ ì½˜í…ì¸ ë¥¼ ë³´ê³  ì§„ì§œ/ê°€ì§œë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì´ë‹¤. í•˜ì§€ë§Œ ìƒì„±í˜• ëª¨ë¸ì´ ê³ ë„í™”ë ìˆ˜ë¡, í™”ë©´ì— ë“œëŸ¬ë‚œ ê²°ê³¼ë§Œ ë³´ê³  ì§„ìœ„ë¥¼ ê°€ë¦¬ëŠ” ì¼ì€ ì ì  ë” ì–´ë ¤ì›Œì§„ë‹¤.</p> <p>ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì§ˆë¬¸ì„ ë°”ê¿”ë³´ê¸°ë¡œ í–ˆë‹¤.</p> <blockquote> <p>"ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ ë‚¸ <strong>ì¶œì²˜</strong>ë¥¼ ì¦ëª…í•  ìˆ˜ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œ?"</p> </blockquote> <h2 id="mutualì˜-ì ‘ê·¼-signing-right-away">mutualì˜ ì ‘ê·¼: Signing Right Away</h2> <p>mutualì˜ í•µì‹¬ ê¸°ìˆ ì€ <strong>SRA(Signing Right Away)</strong>ë¼ëŠ” ì•„í‚¤í…ì²˜ë‹¤. ì´ë¦„ ê·¸ëŒ€ë¡œ, <strong>ë§Œë“¤ì–´ì§€ëŠ” ê·¸ ìˆœê°„ì— ë°”ë¡œ ì„œëª…í•˜ëŠ” ê²ƒ</strong>ì´ ëª©í‘œë‹¤.</p> <p>ìƒê°í•˜ëŠ” íë¦„ì€ ë‹¨ìˆœí•˜ë‹¤.</p> <ol> <li>ì´ë¯¸ì§€ ì„¼ì„œì—ì„œ ë‚˜ì˜¤ëŠ” ì‹ í˜¸ê°€ ì¹´ë©”ë¼ í”„ë¡œì„¸ì„œë¡œ ë“¤ì–´ê°„ë‹¤.</li> <li>ì´ êµ¬ê°„ì„ í•˜ë“œì›¨ì–´ ìˆ˜ì¤€ì—ì„œ ì•”í˜¸í™”í•˜ê³ , ìœ„ë³€ì¡°ë¥¼ ë§‰ëŠ”ë‹¤.</li> <li>ë³´ì•ˆ ì˜ì—­(TEE) ì•ˆì—ì„œë§Œ ë³µí˜¸í™”í•˜ê³ , ê±°ê¸°ì„œ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì„œëª…í•œë‹¤.</li> <li>ìµœì¢…ì ìœ¼ë¡œ C2PA í‘œì¤€ì„ ë”°ë¥´ëŠ” ì½˜í…ì¸  ìê²© ì¦ëª…ì„ ë¶™ì—¬ íŒŒì¼ì„ ë§Œë“ ë‹¤.</li> </ol> <p>ì´ë ‡ê²Œ í•˜ë©´ ë‚˜ì¤‘ì— ëˆ„ê°€ ì´ë¯¸ì§€ë¥¼ ì—´ì–´ë³´ë”ë¼ë„, "ì´ íŒŒì¼ì´ ì‹¤ì œ ì„¼ì„œì—ì„œ ì‹œì‘ëœ ê²ƒì¸ì§€", "ì¤‘ê°„ì— ì¡°ì‘ì´ ìˆì—ˆëŠ”ì§€"ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆë‹¤. ì¤‘ìš”í•œ ê±´, ì´ ì‹ ë¢°ì˜ ë¿Œë¦¬ê°€ ì†Œí”„íŠ¸ì›¨ì–´ê°€ ì•„ë‹ˆë¼ <strong>í•˜ë“œì›¨ì–´</strong>ì— ë†“ì¸ë‹¤ëŠ” ì ì´ë‹¤.</p> <p>ë³´ë‹¤ ê¸°ìˆ ì ì¸ êµ¬ì¡°ì™€ ì„¸ë¶€ ë‚´ìš©ì€ í™”ì´íŠ¸í˜ì´í¼(PDF) <a href="/assets/pdf/SRA-2025-10-05.pdf">Signing Right Away</a>ì— ì •ë¦¬í•´ ë‘ì—ˆë‹¤.</p> <h2 id="ì–´ë–»ê²Œ-ì—¬ê¸°ê¹Œì§€-ì™”ëŠ”ê°€">ì–´ë–»ê²Œ ì—¬ê¸°ê¹Œì§€ ì™”ëŠ”ê°€</h2> <p>2024ë…„ ë´„, SRA ì•„ì´ë””ì–´ë¥¼ ë…¼ë¬¸ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í–ˆë‹¤. ì´ ê³¼ì •ì—ì„œ KAIST ACSS Labì˜ í•œìˆ˜ì§„ êµìˆ˜ë‹˜ì˜ ì¡°ì–¸ì„ ë§ì´ ë°›ì•˜ë‹¤. ì•„ì´ë””ì–´ë¥¼ í™”ì´íŠ¸í˜ì´í¼ í˜•íƒœë¡œ ì •ë¦¬í•´ ë³´ë¼ê³  ê¶Œìœ í•´ ì£¼ì…¨ê³ , ì´ˆê¸° í”„ë¡œí† íƒ€ì´í•‘ì— í•„ìš”í–ˆë˜ Trion T20 FPGA ë³´ë“œë„ ì§ì ‘ ì§€ì›í•´ ì£¼ì…¨ë‹¤. êµ° ë³µë¬´ë¡œ í”„ë¡œì íŠ¸ëŠ” ì ì‹œ ë©ˆì·„ê³ , ì „ì—­ í›„ ì¹œêµ¬ë“¤ê³¼ ë‹¤ì‹œ ëª¨ì—¬ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ê¸° ì‹œì‘í–ˆë‹¤. ê³µì‹ ë¬¸ì„œë„ ì—†ëŠ” ìƒíƒœì—ì„œ MIPI CSI-2 ì¹´ë©”ë¼ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì§ì ‘ ë¶„ì„í•´ ë³´ì•ˆ ì „ì†¡ì„ ì–¹ì–´ ë³´ë ¤ í–ˆë‹¤.</p> <p>ê²°ê³¼ëŠ” ì‹¤íŒ¨ì— ê°€ê¹Œì› ë‹¤. ì“°ë˜ FPGAëŠ” ë©”ëª¨ë¦¬ê°€ í„±ì—†ì´ ë¶€ì¡±í–ˆê³ , í”„ë¡œí† ì½œì„ ì¶”ì¸¡ì— ê¸°ëŒ€ì–´ êµ¬í˜„í•˜ë‹¤ ë³´ë‹ˆ ì˜ìƒì´ ê°„í—ì ìœ¼ë¡œ ê¹¨ì¡Œë‹¤. ë¬´ì—‡ì´ ë¬¸ì œì¸ì§€, ì–´ë–¤ ìì›ì´ í•„ìš”í•œì§€ëŠ” ë¶„ëª…í•´ì¡Œì§€ë§Œ, ìš°ë¦¬ê°€ ê°€ì§„ ì¥ë¹„ì™€ ì‹œê°„ìœ¼ë¡œëŠ” ë” ì´ìƒ ë‚˜ì•„ê°€ê¸° ì–´ë ¤ì› ë‹¤.</p> <p>ê·¸ë•Œ MVL Foundationì˜ ìš°ê²½ì‹ ëŒ€í‘œë‹˜ì„ ë§Œë‚¬ë‹¤. MVLì€ ë™ë‚¨ì•„ì‹œì•„ì—ì„œ TADAë¼ëŠ” ëª¨ë¹Œë¦¬í‹° ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•œë‹¤. 200ë§Œ ëª…ì´ ì“°ëŠ” ì„œë¹„ìŠ¤ì´ê³ , ìˆ˜ì‹­ë§Œ ëª…ì˜ ë“œë¼ì´ë²„ê°€ ë§¤ì¼ ê·¸ ìœ„ì—ì„œ ì¼í•œë‹¤. ë¸”ë¡ì²´ì¸ ê¸°ë°˜ ëª¨ë¹Œë¦¬í‹°ë¼ëŠ” ë‚¯ì„  ì˜ì—­ì—ì„œ ì‹¤ì œë¡œ ì‚¬ì—…ì„ í‚¤ì›Œ ì˜¨ íŒ€ì´ë‹¤.</p> <p>ìš°ê²½ì‹ ëŒ€í‘œë‹˜ ì•ì—ì„œ SRAë¥¼ ì„¤ëª…í–ˆë‹¤. ì™œ ì„¼ì„œê°€ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ëŠ”ì§€, ì™œ ì§€ê¸ˆ ì´ ì‹œì ì— ì´ ê¸°ìˆ ì´ í•„ìš”í•˜ë‹¤ê³  ë³´ëŠ”ì§€, ì™œ í•˜ë“œì›¨ì–´ë¥¼ ê±´ë“œë ¤ì•¼ í•˜ëŠ”ì§€.</p> <p>ì´ì•¼ê¸° ëì—, mutualì€ ì²« ì—”ì ¤ íˆ¬ìë¥¼ ë°›ì•˜ë‹¤.</p> <h2 id="mutualì´ë¼ëŠ”-ì´ë¦„">mutualì´ë¼ëŠ” ì´ë¦„</h2> <p>íšŒì‚¬ ì´ë¦„ì„ ì •í•  ë•Œ ê°€ì¥ ë¨¼ì € ë– ì˜¬ë ¸ë˜ ë‹¨ì–´ê°€ "ì‹ ë¢°"ì˜€ë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ í’€ê³  ì‹¶ì€ ë¬¸ì œëŠ” í•œìª½ì´ ë‹¤ë¥¸ í•œìª½ì„ ì¼ë°©ì ìœ¼ë¡œ ë¯¿ëŠ” êµ¬ì¡°ê°€ ì•„ë‹ˆë‹¤. ì„œë¡œê°€ ì„œë¡œë¥¼ ë¯¿ì„ ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ ê¸°ìˆ ë¡œ ë§Œë“œëŠ” ì¼ì´ë‹¤.</p> <p>ê·¸ë˜ì„œ <strong>mutual</strong>ì´ë¼ëŠ” ì´ë¦„ì„ ê³¨ëë‹¤.</p> <blockquote> <p>ì •ë³´ ë¹„ëŒ€ì¹­ì´ ì¡´ì¬í•˜ëŠ” ê³³ì— ê¸°ìˆ ì  ì‹ ë¢°ë¥¼ ì œê³µí•´, ìƒí˜¸ ì‹ ë¢°ë¥¼ íšŒë³µí•˜ëŠ” ê²ƒ.</p> </blockquote> <p>ì´ ë¬¸ì¥ì„ ì¡°ê¸ˆì”© ë‹¤ë“¬ì–´ê°€ë©°, ìš°ë¦¬ê°€ ë§Œë“¤ê³  ì‹¶ì€ íšŒì‚¬ì˜ ë°©í–¥ì„ ì¡ì•„ ê°€ê³  ìˆë‹¤.</p> <h2 id="ì•ìœ¼ë¡œ">ì•ìœ¼ë¡œ</h2> <p>Qualcomm Snapdragon 8 Gen 3ëŠ” ì´ë¯¸ C2PAë¥¼ í•˜ë“œì›¨ì–´ ë ˆë²¨ì—ì„œ ì§€ì›í•˜ê¸° ì‹œì‘í–ˆë‹¤. Truepic ê°™ì€ íšŒì‚¬ëŠ” Qualcommì˜ TEEë¥¼ í™œìš©í•´ ë¹„ìŠ·í•œ ì•„ì´ë””ì–´ë¥¼ ì‹œì¥ì— ë‚´ë†“ê³  ìˆë‹¤. ì´ ë°©í–¥ì´ í‹€ë¦¬ì§€ ì•Šì•˜ë‹¤ëŠ” ì‹ í˜¸ë‹¤.</p> <p>mutualì€ íŠ¹ì • ì¹©ì´ë‚˜ íŠ¹ì • íšŒì‚¬ì— ì¢…ì†ë˜ì§€ ì•ŠëŠ” <strong>ë ˆí¼ëŸ°ìŠ¤ ì•„í‚¤í…ì²˜</strong>ë¥¼ ë§Œë“¤ê³ ì í•œë‹¤. ì–´ë–¤ SoCë“ , í•„ìš”í•œ ë³´ì•ˆ ë¸”ë¡ë§Œ ì œê³µí•œë‹¤ë©´ ê·¸ ìœ„ì— ì˜¬ë¦´ ìˆ˜ ìˆëŠ” í˜•íƒœì˜ ì„¤ê³„ë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì€ ARMì²˜ëŸ¼ IPë¥¼ ë¼ì´ì„ ì‹±í•˜ëŠ” êµ¬ì¡°ë¥¼ ì—¼ë‘ì— ë‘ê³  ìˆë‹¤.</p> <p>ì§€ê¸ˆì€ ì—°êµ¬ì™€ í”„ë¡œí† íƒ€ì…, ê¸°ìˆ  í‘œì¤€ ë¬¸ì„œì™€ í•˜ë“œì›¨ì–´ ìŠ¤í™ íŒŒí—¤ì¹˜ê¸°ê°€ í•˜ë£¨ ëŒ€ë¶€ë¶„ì„ ì±„ìš´ë‹¤. ê·¸ë¦¬ê³  ê³§, ì´ ê¸¸ì„ í•¨ê»˜ ê±¸ì–´ê°ˆ í•˜ë“œì›¨ì–´ ì—”ì§€ë‹ˆì–´ ê³µë™ì°½ì—…ìë¥¼ ë§ì´í•˜ê²Œ ë˜ê¸°ë¥¼ ê¸°ëŒ€í•˜ê³  ìˆë‹¤.</p> <blockquote> <p><strong>mutual ë§ˆì¼“ ë¦¬ì„œì¹˜ ì•ˆë‚´</strong></p> <p>mutualì€ ìŠ¤ë§ˆíŠ¸í° ì¹´ë©”ë¼ì™€ C2PA ì¸ì¦ ê¸°ëŠ¥ì— ëŒ€í•œ ë‹ˆì¦ˆë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì„¤ë¬¸ì„ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.</p> <ul> <li><strong>ì°¸ì—¬ í˜œíƒ</strong>: ìŠ¤íƒ€ë²…ìŠ¤ ê¸°í”„í‹°ì½˜ 5,000ì› (ì „ì›)</li> <li><strong>ì¶”ê°€ ì‚¬ë¡€ë¹„</strong>: ë„¤íŠ¸ì›Œí‚¹ ì‘ë‹µ ì‹œ 30,000ì›</li> </ul> <p><a href="https://forms.gle/cokN73ZnkHVe8Lbg9">ì„¤ë¬¸ ì°¸ì—¬í•˜ê¸° (Google Forms)</a></p> <hr/> <p><strong>ë„¤íŠ¸ì›Œí‚¹ì„ ìš”ì²­ë“œë¦½ë‹ˆë‹¤</strong></p> <p>mutualì˜ ë¹„ì „ì„ í˜„ì‹¤ë¡œ ë§Œë“¤ê¸° ìœ„í•´, ì•„ë˜ ë¶„ì•¼ì˜ í˜„ì—… ì „ë¬¸ê°€ë¶„ë“¤ê³¼ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìŠµë‹ˆë‹¤.</p> <ul> <li><strong>ì‚¼ì„±ì „ì MXì‚¬ì—…ë¶€ / Apple Hardware Engineering</strong> ë‚´ ì˜ì‚¬ê²°ì •ì ë° ê°œë°œì</li> <li><strong>ì¹´ë©”ë¼ ëª¨ë“ˆ ê°œë°œì‚¬</strong> (LGì´ë…¸í…, ì‚¼ì„±ì „ê¸°, ì— ì”¨ë„¥ìŠ¤, íŒŒíŠ¸ë¡ , ë‚˜ë¬´ê°€ ë“±)ì˜ ê°œë°œ/ì„¤ê³„ì</li> <li><strong>ë””ì§€í„¸ íšŒë¡œ ì„¤ê³„ ë° Tape-out</strong> ê²½í—˜ì´ í’ë¶€í•œ ì—”ì§€ë‹ˆì–´</li> </ul> <p>ì£¼ë³€ì— ì í•©í•œ ë¶„ì´ ê³„ì‹œë‹¤ë©´ ì†Œê°œë¥¼ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì—°ê²°í•´ ì£¼ì‹  ë¶„ê»˜ëŠ” ê°ì‚¬ì˜ ë§ˆìŒì„ ë‹´ì•„ <strong>3ë§Œ ì›ì˜ ì‚¬ë¡€ë¹„</strong>ë¥¼ ë“œë¦½ë‹ˆë‹¤.</p> <ul> <li><strong>ë°©ë²• 1</strong>: <a href="https://forms.gle/cokN73ZnkHVe8Lbg9">ì„¤ë¬¸ ë§í¬</a> ë‚´ 6-3ë²ˆ ë¬¸í•­ ì‘ì„±</li> <li><strong>ë°©ë²• 2</strong>: ì´ë©”ì¼(<a href="mailto:jangyejun@snu.ac.kr">jangyejun@snu.ac.kr</a>)ë¡œ ì§ì ‘ ì—°ë½</li> </ul> </blockquote> <blockquote> <p>mutualì´ ì •ì˜í•œ ë¬¸ì œì— ê¹Šì´ ê³µê°í•˜ê³ , ê¸°ìˆ ë¡œ ì‹ ë¢°ë¥¼ ì¬ê±´í•˜ëŠ” ì—¬ì •ì— í•¨ê»˜í•˜ê³  ì‹¶ë‹¤ë©´ <a href="/blog/2025/mutual-cofounder-hardware-kr/">ì±„ìš© ê³µê³ </a>ë¥¼ ê¼­ í™•ì¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ê°€ë²¼ìš´ ì»¤í”¼ì±— ìš”ì²­ë„ ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤.</p> </blockquote> <hr/> <p><strong>ì—°ë½ì²˜</strong>: <a href="mailto:jangyejun@snu.ac.kr">jangyejun@snu.ac.kr</a></p>]]></content><author><name></name></author><category term="announcement"/><category term="startup"/><category term="mutual"/><category term="content-authenticity"/><category term="hardware-security"/><summary type="html"><![CDATA[AI ì‹œëŒ€ì˜ ì‹ ë¢° ì¸í”„ë¼ë¥¼ ë§Œë“­ë‹ˆë‹¤]]></summary></entry><entry xml:lang="en"><title type="html">Introducing mutual</title><link href="https://mutantq.github.io/blog/2025/introducing-mutual/" rel="alternate" type="text/html" title="Introducing mutual"/><published>2025-12-05T01:00:00+00:00</published><updated>2025-12-05T01:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/introducing-mutual</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/introducing-mutual/"><![CDATA[<p><img src="/assets/img/mutual-logo.jpg" alt="mutual logo"/></p> <div style="max-width: 960px; margin: 0 auto 2rem auto;"> <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"> <iframe src="https://www.youtube.com/embed/Ev9knl_Zbsg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" class="rounded z-depth-1" frameborder="0" allowfullscreen=""></iframe> </div> </div> <p>Smartphone cameras capture billions of photos and videos every day. Between the moment light hits the sensor and the moment a file is stored or uploaded, that data passes through a long chain of hardware and software. mutual focuses on the earliest part of this pipeline, where physical signals are first converted into digital data.</p> <p>Over the coming years, a growing share of the media we consume will be generated or heavily modified by AI systems. At that scale, <strong>human perception alone is no longer enough to reliably distinguish authentic content from fabricated content.</strong></p> <p>mutual exists to answer a practical question: how do we build infrastructure that makes it possible to trust what we see again?</p> <h3 id="following-the-breadcrumbs">Following the breadcrumbs</h3> <p>Over the last few years, a group of companies in the US and Europe has converged on a particular answer. Adobe, together with Google, Sony, OpenAI and others, is leading a technical coalition called <strong>C2PA (Coalition for Content Provenance and Authenticity)</strong>.</p> <p>The idea behind C2PA is simple to state and hard to implement: record the entire lifecycle of a piece of content, protect that record cryptographically, and make it transparently verifiable to anyone who consumes it later. In other words, give every piece of media a <strong>provenance trail</strong>.</p> <p>If you follow that logic, a natural question appears:</p> <blockquote> <p>If we want a trustworthy record of how content was created, <strong>where should that record start?</strong></p> </blockquote> <p>In a photo editor? In a video export step? On a cloud server?</p> <p>Our answer is: it should start at the <strong>sensor</strong>â€”at the point where the physical world first becomes digital.</p> <p>Once a signal has left the sensor, it can, in principle, be copied and modified arbitrarily. The inverse is also true: if we can <strong>lock down and sign the signal at the moment it leaves the sensor and enters the camera pipeline</strong>, every later step can be checked against that original commitment.</p> <p>mutual is a company built around that belief.</p> <h2 id="the-problem-we-see">The problem we see</h2> <p>The internet today behaves like a kind of lemon market. High-quality and low-quality information, real and fake media, all sit side by side. As the information asymmetry grows, trust erodes, and everyone ends up paying for it.</p> <p>Most current efforts attack the problem at the end of the pipeline: they try to classify already-generated content as real or fake. As generative models improve, this becomes a losing game. If you only ever see the final pixels, the detector is always chasing the generator.</p> <p>So we decided to change the question:</p> <blockquote> <p>What if, instead of trying to detect fakes, we could <strong>prove the origin</strong> of genuine content?</p> </blockquote> <h2 id="mutuals-approach-signing-right-away">mutualâ€™s approach: Signing Right Away</h2> <p>Our core architecture is called <strong>SRA (Signing Right Away)</strong>. As the name suggests, the goal is to <strong>sign content at the moment it is created</strong>.</p> <p>Conceptually, the flow is straightforward:</p> <ol> <li>A signal comes off the image sensor and enters the camera pipeline.</li> <li>That path is protected at the hardware level using authenticated encryption (AES-GCM/CCM).</li> <li>Only inside a secure enclave (TEE) is the data decrypted and combined with metadata for signing.</li> <li>The final file is produced with C2PA-compliant content credentials attached.</li> </ol> <p>This way, when someone opens an image later, they can ask: "Did this really originate from a physical sensor? Was it tampered with along the way?"â€”and get a cryptographically grounded answer. The root of trust sits in <strong>hardware</strong>, not just in software that can be bypassed.</p> <p>For a deeper dive into the architecture, you can read the whitepaper (PDF): <a href="/assets/pdf/SRA-2025-10-05.pdf">Signing Right Away</a>.</p> <h2 id="how-we-got-here">How we got here</h2> <p>In spring 2024, I wrote up the first version of SRA as a kind of whitepaper. Around that time I also benefited a lot from guidance by Prof. Soojean Han at KAIST ACSS Lab, who encouraged me to formalize the work in this format and generously provided the first Trion T20 FPGA board that made our early prototyping possible. Then I left for mandatory military service. When I came back, I gathered a few friends, and we tried to turn the idea into a prototype.</p> <p>We aimed high: without official documentation, we tried to reverse-engineer the MIPI CSI-2 camera interface and bolt a secure transport layer on top. It mostly failed. Our FPGA board didnâ€™t have enough memory. Our best-effort understanding of the protocol led to streams that would randomly break. We learned exactly what was wrong and what we neededâ€”but we didnâ€™t have the resources to get there.</p> <p>Around that time, I met <strong>Kay Kyungsik Woo</strong>, founder and CEO of MVL Foundation. MVL runs TADA, a mobility service used by 2 million riders and hundreds of thousands of drivers in Southeast Asia. Itâ€™s a team that has actually built and scaled a blockchain-based mobility ecosystem.</p> <p>I walked him through SRA: why the sensor matters, why the timing is right, why this has to live in hardware.</p> <p>At the end of that conversation, mutual got its first angel investment.</p> <h2 id="the-name">The name</h2> <p>When I started thinking about what kind of company I wanted to build, one word kept coming back: trust. But the kind of trust we care about isnâ€™t one-sided. Itâ€™s not "just trust us"; itâ€™s creating the <strong>conditions</strong> under which two sides can rationally trust each other.</p> <p>Thatâ€™s what "mutual" is about.</p> <blockquote> <p>Our mission is to provide technical trust wherever information asymmetry exists, so that mutual trust can be restored.</p> </blockquote> <p>We are still refining that sentence, but it captures the direction of the company.</p> <h2 id="looking-ahead">Looking ahead</h2> <p>Qualcommâ€™s Snapdragon 8 Gen 3 already supports C2PA at the hardware level. Companies like Truepic are shipping systems built on top of secure camera pipelines. To us, these are strong signals that the ecosystem is moving in the right direction.</p> <p>mutualâ€™s goal is to build a <strong>reference architecture</strong> that is not tied to any single chip or vendor. If a System-on-Chip exposes the right security primitives, our design should be able to run on top of it. On the business side, we are thinking in terms of an ARM-like IP licensing model.</p> <p>For now, weâ€™re a small team based in Seoul. Our days are filled with reading specs, building prototypes, and translating standards into running code. Weâ€™re looking forward to adding a a hardware engineer co-founder to the team.</p> <p>If any of this resonates with you, you might enjoy reading the <a href="/blog/2025/mutual-cofounder-hardware/">co-founder role description</a>.</p> <hr/> <p><strong>Contact</strong>: <a href="mailto:jangyejun@gmail.com">jangyejun@gmail.com</a></p>]]></content><author><name></name></author><category term="announcement"/><category term="startup"/><category term="mutual"/><category term="content-authenticity"/><category term="hardware-security"/><summary type="html"><![CDATA[Building trust infrastructure for the post-AI era]]></summary></entry><entry><title type="html">íë¹„íŠ¸ì˜ ì´í•´</title><link href="https://mutantq.github.io/blog/2025/understanding-qubits/" rel="alternate" type="text/html" title="íë¹„íŠ¸ì˜ ì´í•´"/><published>2025-10-17T10:00:00+00:00</published><updated>2025-10-17T10:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/understanding-qubits</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/understanding-qubits/"><![CDATA[<p><em>ë³¸ ê¸€ì€ ì´ê³µê³„ ì¡ë‹´ ë° ì¡°ì–¸ë°©(ì¹´ì¹´ì˜¤í†¡ ì˜¤í”ˆì±„íŒ…)ì—ì„œ ì‹œì‘ëœ 1ì¼ 1ê¸€ í”„ë¡œì íŠ¸ì¸ <a href="https://publish.obsidian.md/fomakase/">í¬ê³µë°© ì˜¤ë§ˆì¹´ì„¸</a>ì˜ ì¼í™˜ìœ¼ë¡œ ì‘ì„±ëœ ê¸€ì…ë‹ˆë‹¤.</em></p> <h2 id="ì–‘ìì»´í“¨í„°ë¥¼-ì´í•´í•˜ê³ -ì‹¶ë‹¤ë©´-íë¹„íŠ¸ë¶€í„°-ì´í•´í•˜ì">ì–‘ìì»´í“¨í„°ë¥¼ ì´í•´í•˜ê³  ì‹¶ë‹¤ë©´ íë¹„íŠ¸ë¶€í„° ì´í•´í•˜ì</h2> <p>ì–‘ìì»´í“¨í„°ëŠ” <strong>ì–‘ìë¬¼ë¦¬í•™ì  í˜„ìƒì„ í™œìš©í•˜ì—¬ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¥ì¹˜</strong>ì´ë‹¤. ë¦¬ì²˜ë“œ íŒŒì¸ë§Œì´ ë§í–ˆë˜ ê²ƒì²˜ëŸ¼, ì–‘ìë¬¼ë¦¬í•™ì„ ì œëŒ€ë¡œ ì´í•´í•œ ì‚¬ëŒì€ â€œì´ ì„¸ìƒì— ì•„ë¬´ë„ ì—†ë‹¤â€. ì–‘ìì—­í•™ì´ ê·¸í† ë¡ ì•…ëª… ë†’ê²Œ ì–´ë ¤ìš´ë°, ì–‘ìì»´í“¨í„°ê°€ ì´í•´í•˜ê¸° ì‰½ë‹¤ê³  ë§í•˜ë©´ ê·¸ê±´ ê±°ì§“ë§ì´ë‹¤. ê·¸ë ‡ì§€ë§Œ ê·¸ ê°œë…ì´ ìš°ë¦¬ê°€ ìµíˆ ì•„ëŠ” ì»´í“¨í„°, ì¦‰ â€œê³ ì „â€ ì»´í“¨í„°ì— ë¹„í•´ ì¡°ê¸ˆì€ ì–´ë µê¸°ì—, ê·¸ ì•ˆì—ì„œ ë”ìš± í¥ë¯¸ì§„ì§„í•œ ë…¼ì˜ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆë‹¤. ì´ì œë¶€í„° ê·¸ ë…¼ì˜ì˜ ì¶œë°œì ì¸ íë¹„íŠ¸(Qubit)ì— ëŒ€í•´ ì•Œì•„ë³´ì.</p> <h2 id="íë¹„íŠ¸-qubit">íë¹„íŠ¸ (Qubit)</h2> <p>ì¼ë°˜ì ì¸ ì»´í“¨í„°ì™€ëŠ” ë‹¬ë¦¬, ì–‘ìì»´í“¨í„°ì—ì„œëŠ” ì •ë³´ì²˜ë¦¬ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ <strong>íë¹„íŠ¸(Qubit)</strong>ë¥¼ ì‚¬ìš©í•œë‹¤. ë”°ë¼ì„œ ì–‘ìì»´í“¨í„°ê°€ ì •í™•í•˜ê²Œ ë¬´ì—‡ì¸ì§€, ê¹Šì´ìˆê²Œ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € íë¹„íŠ¸ë¥¼ ì˜ ì´í•´í•´ì•¼ í•œë‹¤. ì´ê²ƒì€ ê³ ì „ ì»´í“¨í„°ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì´ì§„ìˆ˜ë¥¼ ìˆ™ë‹¬í•´ì•¼ í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì´ì¹˜ì´ë‹¤.</p> <h3 id="íë¹„íŠ¸ì˜-ì •ì˜">íë¹„íŠ¸ì˜ ì •ì˜</h3> <p>íë¹„íŠ¸ëŠ” ì–‘ì ë¹„íŠ¸(Quantum Bit)ì˜ ì¤„ì„ë§ë¡œ, <strong>$0$ ë˜ëŠ” $1$ì´ë¼ê³  ë¶€ë¥´ëŠ” ë‘ ìƒíƒœì˜ ì–‘ìì—­í•™ì ì¸ ì¤‘ì²©(superposition)ì„ ë§í•œë‹¤.</strong> ì¸¡ì •(measurement)ì„ í†µí•´, ì¤‘ì²© ìƒíƒœì—ì„œ ë²—ì–´ë‚˜ $0$ ë˜ëŠ” $1$ ì¤‘ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ê²°ì •ëœë‹¤.</p> <p>ì—„ë°€í•˜ì§€ëŠ” ì•Šì§€ë§Œ, ë™ì „ì´ ì±…ìƒ ìœ„ì—ì„œ ë¹ ë¥´ê²Œ íšŒì „í•˜ê³  ìˆëŠ” ìƒí™©ì„ ìƒìƒí•˜ê³ , ë™ì „ì„ ì†ë°”ë‹¥ìœ¼ë¡œ ë‚´ë¦¬ì³ì„œ ë©ˆì·„ì„ ë•Œ ìœ—ë©´ì´ ë™ì „ì˜ ì•ë©´ì¸ì§€ ë’·ë©´ì¸ì§€ ê´€ì°°í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì´í•´í•˜ë©´ (ì´ˆë°˜ì—ëŠ”) ë„ì›€ì´ ëœë‹¤. ê³ ì „ ë¹„íŠ¸ì™€ ë¹„êµí•˜ìë©´ ì•„ë˜ì™€ ê°™ë‹¤:</p> <table> <thead> <tr> <th>êµ¬ë¶„</th> <th>ê³ ì „ ë¹„íŠ¸</th> <th>íë¹„íŠ¸</th> </tr> </thead> <tbody> <tr> <td>ìƒíƒœ</td> <td>$0$ ë˜ëŠ” $1$ (ë‘˜ ì¤‘ í•˜ë‚˜)</td> <td>$\alpha\ket{0} + \beta\ket{1}$ (ì¤‘ì²© ìƒíƒœ)</td> </tr> <tr> <td>ì¸¡ì • ì „</td> <td>í•­ìƒ í™•ì •ëœ ê°’</td> <td>í™•ë¥ ì  ì¤‘ì²©</td> </tr> <tr> <td>ì¸¡ì •</td> <td>ê°’ í™•ì¸</td> <td>ì¤‘ì²© ë¶•ê´´, $0$ ë˜ëŠ” $1$ë¡œ í™•ì •</td> </tr> <tr> <td>í‘œí˜„</td> <td>1ê°œì˜ ì‹¤ìˆ˜ (0 ë˜ëŠ” 1)</td> <td>2ê°œì˜ ë³µì†Œìˆ˜ ($\alpha, \beta$)</td> </tr> <tr> <td>Â </td> <td>Â </td> <td>Â </td> </tr> </tbody> </table> <h3 id="íë¹„íŠ¸ì˜-ìˆ˜í•™ì -í‘œí˜„">íë¹„íŠ¸ì˜ ìˆ˜í•™ì  í‘œí˜„</h3> <p>ì–‘ìì»´í“¨íŒ…ì—ì„œëŠ” ìƒíƒœ $0$ê³¼ $1$ì„ ì¡°ê¸ˆ í™”ë ¤í•˜ê²Œ $\ket{0}$ê³¼ $\ket{1}$ë¡œ ì“°ê³ , ì½ì„ ë•ŒëŠ” â€œì¼“ 0â€, â€œì¼“ 1â€ì´ë¼ê³  ì½ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì´ë“¤ì„ ì¤‘ì²©ì‹œí‚¬ ë•ŒëŠ” ê°ê°ì˜ ìƒíƒœì— ì ì ˆí•œ ìˆ˜ë¥¼ ê³±í•´ì„œ ë”í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•œë‹¤:</p> \[\ket{q}=\alpha\ket{0}+\beta\ket{1}\] <p>ì´ê²ƒì´ ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” <strong>ì„ í˜• ê²°í•©(linear combination)</strong>ì˜ ê°œë…ì´ë‹¤. 2ì°¨ì› í‰ë©´ ìƒì˜ ë²¡í„° $\mathbf{v}$ë¥¼ $x$ì¶• ë°©í–¥ ë‹¨ìœ„ ë²¡í„° $\mathbf{i}$ì™€ $y$ì¶• ë°©í–¥ ë‹¨ìœ„ ë²¡í„° $\mathbf{j}$ì˜ ì„ í˜• ê²°í•© $\mathbf{v}=x\mathbf{i}+y\mathbf{j}$ë¡œ í‘œí˜„í•˜ë“¯, íë¹„íŠ¸ ì—­ì‹œë„ ìƒíƒœ $\ket{0}$ê³¼ ìƒíƒœ $\ket{1}$ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì•ìœ¼ë¡œëŠ” ${\ket{0}, \ket{1}}$ì„ ì„œë¡œ ìˆ˜ì§ì¸ ë‘ ë‹¨ìœ„ ë²¡í„°ì²˜ëŸ¼ ìƒê°í•´ì£¼ê¸°ë¥¼ ë°”ë€ë‹¤.</p> <h4 id="ë²¡í„°-í‘œê¸°ë²•">ë²¡í„° í‘œê¸°ë²•</h4> <p>ë²¡í„°ëŠ” ì—¬ëŸ¬ ìˆ˜ì˜ ë¬¶ìŒ, ì¦‰ ìˆœì„œìŒìœ¼ë¡œë„ ì´í•´í•  ìˆ˜ ìˆë‹¤. íë¹„íŠ¸ ì—­ì‹œë„ ë‘ ìˆ˜ $\alpha,\:\beta$ì˜ ìˆœì„œìŒìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 2ì°¨ì› ë²¡í„°ì´ë‹¤. ì—¬ê¸°ì„œ $\ket{0}=\begin{bmatrix} 1 \\ 0\end{bmatrix}$, $\ket{1}=\begin{bmatrix} 0 \\ 1\end{bmatrix}$ë¡œ ì •ì˜í•˜ë©´ ì•„ë˜ì²˜ëŸ¼ í‘œí˜„í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤:</p> \[\begin{align*} \ket{q}=\alpha \ket{0} + \beta \ket{1}=\begin{bmatrix} \alpha\\\beta \end{bmatrix} \end{align*}\] <h2 id="ì˜ˆì‹œë¡œ-ì•Œì•„ë³´ëŠ”-íë¹„íŠ¸">ì˜ˆì‹œë¡œ ì•Œì•„ë³´ëŠ” íë¹„íŠ¸</h2> <p>ì•„ë˜ì˜ $\ket{q_1}, \ket{q_2}, \ket{q_3}$ëŠ” ëª¨ë‘ íë¹„íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë“¤ì´ë‹¤:</p> \[\begin{align}\ket{q_1}&amp;=\frac{1}{\sqrt{2}}\ket0+\frac{i}{\sqrt{2}}\ket1=\frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ i \end{bmatrix} \\ \ket{q_2}&amp;=\frac{i}{\sqrt{3}}\ket0+\sqrt{\frac{2}{3}}\ket1=\frac{1}{\sqrt{3}} \begin{bmatrix} i \\ \sqrt{2} \end{bmatrix} \\ \ket{q_3}&amp;=\frac{1}{2}(1+i)\ket0 + \frac{1}{2}(1-i)\ket1=\frac{1}{2}\begin{bmatrix}1+i \\ 1-i \end{bmatrix}\end{align}\] <p>ì‹ì„ 2ë¶„ ì •ë„ ì§€ê·¸ì‹œ ê´€ì°°í•´ë³´ê¸°ë¥¼ ë°”ë€ë‹¤.</p> <p>ê´€ì°°í•˜ë‹¤ë³´ë©´, ë‹¤ì†Œ ì´ìƒí•œ ì ì„ ë°œê²¬í–ˆì„ ê²ƒì´ë‹¤ - í—ˆìˆ˜ $i$ê°€ ì™œ ë²¡í„° ì•ˆì— ë“¤ì–´ ìˆëŠ” ê²ƒì¸ê°€? ê·¸ ì´ìœ ëŠ” íë¹„íŠ¸ê°€ <strong>ë³µì†Œ ë²¡í„° ê³µê°„(Complex Vector Space)</strong>ì˜ ì›ì†Œì´ê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰, íë¹„íŠ¸ê°€ ê¸°ë³¸ì ìœ¼ë¡œ $(ë³µì†Œìˆ˜, ë³µì†Œìˆ˜)$ì˜ í˜•íƒœë¥¼ ê°€ì§„ë‹¤.</p> <p>íë¹„íŠ¸ê°€ ë³µì†Œ ë²¡í„° ê³µê°„ì˜ ì›ì†Œë¼ëŠ” ì‚¬ì‹¤ì€ íë¹„íŠ¸ê°€ ë‹¨ìˆœí•˜ê²Œ â€œ$0$ê³¼ $1$ ì‚¬ì´ì˜ ê·¸ ë¬´ì–¸ê°€â€ë³´ë‹¤ëŠ” ì¡°ê¸ˆ ë” ë§ì€ ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆìŒì„ ì‹œì‚¬í•œë‹¤. ê·¼ë° ë˜ ì¬ë°ŒëŠ” ê²ƒì€, íë¹„íŠ¸ê°€ $(ë³µì†Œìˆ˜, ë³µì†Œìˆ˜)$ì˜ í˜•íƒœë¼ë©´ ê° ë³µì†Œìˆ˜ëŠ” $a + bi$ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì— ì‹¤ìˆ˜ì¸ ë³€ìˆ˜ $4$ê°œë¥¼ ì¨ì„œ 4ì°¨ì› ê³µê°„ ìƒì˜ ì ìœ¼ë¡œ í‘œí˜„í•´ì•¼ í•  ê²ƒ ê°™ì§€ë§Œ, <strong>ì‹¤ì œë¡œëŠ” ê²½ë„ì™€ ìœ„ë„ë¥¼ ì´ìš©í•˜ì—¬ êµ¬ë©´ ìœ„ì˜ ì ìœ¼ë¡œ í‘œí˜„í•œë‹¤ëŠ” ê²ƒì´ë‹¤</strong>. ì•„ë˜ì™€ ê°™ì´, íë¹„íŠ¸ì˜ ìƒíƒœë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ êµ¬ë©´ì„ ë¸”ë¡œí êµ¬ë©´(Bloch Sphere)ì´ë¼ê³  í•œë‹¤.</p> <p><img src="/assets/img/blog/qubit/untitled_qubit-understanding.png" alt="ë¸”ë¡œí êµ¬ë©´ (Bloch Sphere)"/> <em>ê·¸ë¦¼: ë¸”ë¡œí êµ¬ë©´ - íë¹„íŠ¸ ìƒíƒœë¥¼ 3ì°¨ì› êµ¬ë©´ ìœ„ì˜ ì ìœ¼ë¡œ í‘œí˜„</em></p> <h2 id="ë‘-ê°€ì§€-í•µì‹¬-ì§ˆë¬¸">ë‘ ê°€ì§€ í•µì‹¬ ì§ˆë¬¸</h2> <p>ì—¬ê¸°ê¹Œì§€ í•´ì„œ ë‘ ê°€ì§€ ì •ë„ì˜ ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆê² ë‹¤:</p> <ol> <li><strong>íë¹„íŠ¸ê°€ $(ë³µì†Œìˆ˜, ë³µì†Œìˆ˜)$ë©´ ì‹¤ìˆ˜ ë³€ìˆ˜ $4$ê°œë¥¼ ì¨ì•¼ í‘œí˜„ì´ ê°€ëŠ¥í•  ê²ƒ ê°™ì€ë°, íë¹„íŠ¸ë¥¼ êµ¬ë©´ ìœ„ì˜ ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</strong></li> <li><strong>ì• ì´ˆì— íë¹„íŠ¸ë¥¼ í‘œí˜„í•˜ëŠ”ë° ë³µì†Œìˆ˜ê°€ ì™œ í•„ìš”í•œê±¸ê¹Œ?</strong></li> </ol> <p>ì´ ë‘ ì§ˆë¬¸ì€ ì„œë¡œ ë°€ì ‘í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆë‹¤. ìˆœì„œëŒ€ë¡œ ë‹µí•´ë³´ì.</p> <h3 id="ì§ˆë¬¸-1-íë¹„íŠ¸ë¥¼-êµ¬ë©´-ìœ„ì˜-ì ìœ¼ë¡œ-í‘œí˜„í• -ìˆ˜-ìˆëŠ”-ì´ìœ ">ì§ˆë¬¸ 1: íë¹„íŠ¸ë¥¼ êµ¬ë©´ ìœ„ì˜ ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì´ìœ </h3> <p>ìš°ì„ ì€ ë‘ë²ˆì§¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ì ì‹œ ë’¤ë¡œ ë¯¸ë£¨ê³ , ì²«ë²ˆì§¸ ì§ˆë¬¸ì— ì§‘ì¤‘í•´ë³´ì. íë¹„íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ ì‹¤ë³€ìˆ˜ ë‘ ê°œë©´ ì¶©ë¶„í•˜ë‹¤ëŠ”ë° ê·¸ ì´ìœ ê°€ ë­˜ê¹Œ?</p> <p>ê·¸ ì´ìœ ëŠ” ì§€ê¸ˆê¹Œì§€ ì–¸ê¸‰í•˜ì§€ ì•Šì€ ë‘ ê°œì˜ <strong>ì œì•½ ì¡°ê±´(constraint)</strong>ì— ì˜í•´, í•„ìš”í•œ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ì¤„ì–´ë“¤ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. ì—¬ê¸°ì„œ ì²«ë²ˆì§¸ ì œì•½ ì¡°ê±´ì€ í™•ë¥ ì˜ <strong>ì •ê·œí™”(Normalization)</strong>ì™€ ê´€ë ¨ì´ ìˆê³ , ë‘ë²ˆì§¸ ì œì•½ ì¡°ê±´ì€ <strong>ì „ì—­ ìœ„ìƒ(Global phase)</strong>ê³¼ ê´€ë ¨ë˜ì–´ ìˆë‹¤.</p> <p>ìœ„ì˜ ë‚´ìš©ì„ ë…¼í•˜ë ¤ë©´ ë³µì†Œ ê³„ìˆ˜ì˜ ì˜ë¯¸ì— ëŒ€í•´ì„œ ì´í•´í•´ì•¼ í•œë‹¤. íë¹„íŠ¸ì˜ ì •ì˜ì—ì„œ $\alpha$ëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ê³  $\beta$ëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í• ê¹Œ? $\alpha$ëŠ” $\ket{0}$ê³¼ $\ket{1}$ì˜ ì„ í˜• ê²°í•©ì—ì„œ $\ket{0}$ ì•ì— ë¶™ì€ ê³„ìˆ˜ì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— $\alpha$ì˜ ê°’ì´ â€œì»¤ì§„ë‹¤ë©´â€, íë¹„íŠ¸ì—ì„œ ìƒíƒœ $\ket{0}$ì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì´ ëŠ˜ì–´ë‚œë‹¤ëŠ” ëœ»ì´ë‹¤. ì¦‰, íë¹„íŠ¸ê°€ ìƒíƒœ $\ket{0}$ìœ¼ë¡œ ì¸¡ì •ë  í™•ë¥ ì´ ì»¤ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.</p> <p>ê·¸ë ‡ë‹¤ë©´ ë³µì†Œìˆ˜ì˜ â€œí¬ê¸°â€ëŠ” ì–´ë–»ê²Œ í‘œí˜„í• ê¹Œ? ë‹µì€ ê°„ë‹¨í•˜ë‹¤: ë³µì†Œìˆ˜ì˜ ì ˆëŒ“ê°’ì„ ì´ìš©í•˜ë©´ ëœë‹¤. íë¹„íŠ¸ë¥¼ ì¸¡ì •í–ˆì„ ì‹œ ìƒíƒœ $\ket{0}$ ë˜ëŠ” $\ket{1}$ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ê° ìƒíƒœ ì•ì— ë¶™ì€ ë³µì†Œê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì˜ ì œê³±ì´ë‹¤. ì´ë¥¼ ë³¸ì˜ ê·œì¹™(Bornâ€™s Rule)ì´ë¼ ë¶€ë¥¸ë‹¤:</p> <blockquote> <p><strong>ë³¸ì˜ ê·œì¹™ (Bornâ€™s Rule)</strong></p> <p>íë¹„íŠ¸ $\ket{q}=\alpha\ket{0}+\beta\ket{1}$ì— ëŒ€í•´, ì¸¡ì • ì´í›„ íë¹„íŠ¸ì˜ ìƒíƒœëŠ” $\ket{0}$ ë˜ëŠ” $\ket{1}$ì´ë©°:</p> <ul> <li>$\ket{0}$ìœ¼ë¡œ ì¸¡ì •ë  í™•ë¥ : $P(0) = |\alpha|^2$</li> <li>$\ket{1}$ë¡œ ì¸¡ì •ë  í™•ë¥ : $P(1) = |\beta|^2$</li> </ul> </blockquote> <p>ì´ë ‡ê²Œ ì•ì˜ ë³µì†Œ ê³„ìˆ˜ê°€ ê° ìƒíƒœì˜ ì¸¡ì • í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤ëŠ” ê²ƒì„ ì•Œê³  ë‚˜ë©´, ì •ê·œí™” ì¡°ê±´ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤. ì¦‰, ì…ìëŠ” ë°˜ë“œì‹œ $\ket{0}$ ë˜ëŠ” $\ket{1}$ë¡œ ì¸¡ì •ë˜ì–´ì•¼ í•˜ë¯€ë¡œ, ë‘ ë°œê²¬ í™•ë¥ ì„ ë”í•˜ë©´ $1$ì´ ë˜ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì•„ë˜ ì‹ì„ ì–»ëŠ”ë‹¤:</p> \[\begin{equation} |\alpha| ^2 + |\beta| ^2=1 \end{equation}\] <p>ì´ê²ƒì´ ì²«ë²ˆì§¸ ì œì•½ì¡°ê±´ì´ë‹¤. $\alpha = \alpha_1+\alpha_2 i$, $\beta = \beta_1 + \beta_2 i$ë¼ ë‘ë©´ ì•„ë˜ì™€ ê°™ì´ ë³€í˜•í•  ìˆ˜ ìˆë‹¤.</p> \[\begin{equation} \alpha_1^2+\alpha_2^2+\beta_1^2+\beta_2^2=1 \end{equation}\] <p>ì•„ë˜ì˜ ì›ì˜ ë°©ì •ì‹ê³¼ êµ¬ì˜ ë°©ì •ì‹, ê·¸ë¦¬ê³  ì‹ $(6)$ì„ í•¨ê»˜ ë³´ì.</p> \[\begin{aligned}\textrm{ì›ì˜ ë°©ì •ì‹:}&amp;\;\;x^2+y^2=1 \\ \textrm{êµ¬ì˜ ë°©ì •ì‹:}&amp;\;\;x^2+y^2+z^2=1 \end{aligned}\] <p>ì‹ $(6)$ì´ íŠ¹ë³„í•˜ê²Œ ëŠê»´ì§€ì§€ ì•ŠëŠ”ê°€? ì›ì˜ ë°©ì •ì‹ê³¼ êµ¬ì˜ ë°©ì •ì‹ê³¼ í•¨ê»˜ ë³´ë©´ ì‹ $(6)$ì´ 4ì°¨ì› ê³µê°„ ìƒì˜ êµ¬, ì¦‰ ì´ˆêµ¬(Hypersphere)ì˜ ë°©ì •ì‹ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ì•Œì•„ì°¨ë¦´ ìˆ˜ ìˆë‹¤. ì œì•½ ì¡°ê±´(=ë“±ì‹)ì´ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ììœ ë„ëŠ” ì¤„ì–´ë“ ë‹¤. ì¦‰, ë°©ê¸ˆ íë¹„íŠ¸ëŠ” $2$ì°¨ì› ë³µì†Œ ë²¡í„°ì´ê¸°ì— ì‹¤ë³€ìˆ˜ë¡œëŠ” 4ì°¨ì›ì¸ ê²ƒ ê°™ìœ¼ë©´ì„œë„, ì œì•½ ì¡°ê±´ì— ì˜í•´ 1ê°œì˜ ììœ ë„ê°€ ë‚ ì•„ê°€ë©´ì„œ 3ê°œì˜ ë³€ìˆ˜ë§Œìœ¼ë¡œë„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ëœ ê²ƒì´ë‹¤.</p> <p>ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ ì‹ì˜ ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ë‹¤. ììœ ë„ê°€ 4ê°œì—ì„œ 3ê°œë¡œ ì¤„ì–´ë“¤ê¸´ í–ˆì§€ë§Œ, êµ¬ë©´ ìœ„ì—ì„œ í‘œí˜„í•˜ë ¤ë©´ ì œì•½ ì¡°ê±´ì´ í•œ ê°œê°€ ë” í•„ìš”í•œ ìƒí™©ì´ë‹¤. <strong>ë§ˆì§€ë§‰ ì œì•½ ì¡°ê±´ì€ ì–´ë””ì—ì„œ ì˜¤ëŠ”ê±¸ê¹Œ?</strong> ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì€ ìš°ë¦¬ì˜ ë‘ë²ˆì§¸ ì§ˆë¬¸ì´ì—ˆë˜ â€œíë¹„íŠ¸ë¥¼ ë³µì†Œìˆ˜ë¡œ í‘œí˜„í•´ì•¼ í•˜ëŠ” ì´ìœ â€ì— ëŒ€í•´ ë‹µí•˜ê³  ë‚˜ë©´ ëª…ì¾Œí•˜ê²Œ ë‹µí•  ìˆ˜ ìˆìœ¼ë‹ˆ, ë¨¼ì € ë‘ë²ˆì§¸ ì§ˆë¬¸ì— ë‹µí•´ë³´ì.</p> <h3 id="ì§ˆë¬¸-2-íë¹„íŠ¸ë¥¼-ë³µì†Œìˆ˜ë¡œ-í‘œí˜„í•´ì•¼ë§Œ-í•˜ëŠ”-ì´ìœ ">ì§ˆë¬¸ 2: íë¹„íŠ¸ë¥¼ ë³µì†Œìˆ˜ë¡œ í‘œí˜„í•´ì•¼ë§Œ í•˜ëŠ” ì´ìœ </h3> <p>ì´ì œ ë‘ë²ˆì§¸ ì§ˆë¬¸ì— ë‹µí•´ë³´ì. ì™œ íë¹„íŠ¸ëŠ” ë³µì†Œìˆ˜ë¡œ í‘œí˜„í•´ì•¼ë§Œ í•˜ëŠ”ê°€?</p> <p>ë‘ê´„ì‹ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ìë©´ <strong>â€œí¬ê¸°ì™€ ìœ„ìƒì„ ë™ì‹œì— í‘œí˜„í•˜ê¸° ìœ„í•´ì„œâ€</strong>ì´ë‹¤. ì—¬ê¸°ì„œ ìœ„ìƒ(phase)ì´ ë¬´ì—‡ì¸ì§€ ì´í•´í•˜ë ¤ë©´ (1) ë°˜ëŒ€ ìœ„ìƒ($180Â°$ ìœ„ìƒì°¨)ë¥¼ ê°€ì§€ëŠ” ë‘ ë³µì†Œìˆ˜ì˜ ì˜ˆì‹œì™€ (2) $90Â°$ì˜ ìœ„ìƒì°¨ë¥¼ ê°€ì§€ëŠ” ë‘ ë³µì†Œìˆ˜ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë„ì›€ì´ ëœë‹¤.</p> <p>ë¨¼ì € ë³µì†Œìˆ˜ì— ê´€í•œ ëª‡ ê°€ì§€ ì‚¬ì‹¤ì„ ì •ë¦¬í•´ë³´ì. ì˜¤ì¼ëŸ¬ì˜ ê³µì‹(Eulerâ€™s formula)ì— ì˜í•´, ì„ì˜ì˜ ë³µì†Œìˆ˜ $z=a+bi$ëŠ” ê·¸ í¬ê¸° $R=\sqrt{a^2+b^2}$ë¡œ ë‚˜ëˆ„ì—ˆì„ ë•Œ í¸ê° $\theta$ê°€ ì¡´ì¬í•˜ì—¬</p> \[z/R=(a/R)+i(b/R)=\cos{\theta}+i\sin{\theta}=e^{i\theta}\] <p>ë¥¼ ë§Œì¡±í•œë‹¤. ì–‘ë³€ì— $R$ì„ ê³±í•˜ë©´ ì„ì˜ì˜ ë³µì†Œìˆ˜ëŠ” $z=Re^{i\theta}$ì˜ ê¼´ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í¬ê¸°ì™€ ìœ„ìƒì„ ë™ì‹œì— ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.</p> <h4 id="ìœ„ìƒì°¨ì˜-ì˜ˆì‹œ">ìœ„ìƒì°¨ì˜ ì˜ˆì‹œ</h4> <p>ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ì„¸ ë³µì†Œìˆ˜ë¥¼ ìƒê°í•´ë³´ì: \(\begin{align} z_0&amp;=1+i = \sqrt{2}e^{i\pi/4} \\ z_1&amp;=-1-i = \sqrt{2}e^{i5\pi/4} = \sqrt{2}e^{i\pi/4} \cdot e^{i\pi} \\ z_2&amp;=-1+i = \sqrt{2}e^{i3\pi/4} = \sqrt{2}e^{i\pi/4} \cdot e^{i\pi/2} \end{align}\)</p> <p>ì—¬ê¸°ì„œ $z_1 = -z_0$ì´ë¯€ë¡œ $z_0$ì™€ $z_1$ì€ $180Â°$ ìœ„ìƒì°¨ë¥¼ ê°€ì§€ê³ , $z_2 = iz_0$ì´ë¯€ë¡œ $z_0$ì™€ $z_2$ëŠ” $90Â°$ ìœ„ìƒì°¨ë¥¼ ê°€ì§„ë‹¤.</p> <p>ì´ì œ íë¹„íŠ¸ì—ì„œ ì´ ê°œë…ì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ë³´ì. ë‹¤ìŒ íë¹„íŠ¸ë¥¼ ìƒê°í•´ë³´ì: \(\ket{q} = (1+i)\ket{0} + (-1-i)\ket{1} = z_0\ket{0} + z_1\ket{1}\)</p> <p>ì´ íë¹„íŠ¸ë¥¼ ë²¡í„°ë¡œ ì“°ë©´: \(\begin{align*} \ket{q} = \begin{bmatrix} 1+i \\ -1-i \end{bmatrix} = \begin{bmatrix} \sqrt{2}e^{i\pi/4} \\ \sqrt{2}e^{i5\pi/4} \end{bmatrix} \end{align*}\)</p> <p>ê·¹ì¢Œí‘œ í˜•ì‹ì—ì„œ ê³µí†µ ì¸ìˆ˜ë¥¼ ë¹¼ë‚´ë©´: \(\begin{align*} \ket{q} = e^{i\pi/4}\begin{bmatrix} \sqrt{2} \\ \sqrt{2}e^{i\pi} \end{bmatrix} = e^{i\pi/4}\begin{bmatrix} \sqrt{2} \\ -\sqrt{2} \end{bmatrix} \end{align*}\)</p> <p>ì—¬ê¸°ì„œ í¥ë¯¸ë¡œìš´ ì‚¬ì‹¤ì„ ë°œê²¬í•  ìˆ˜ ìˆë‹¤. ì „ì²´ ìƒíƒœì— ê³µí†µìœ¼ë¡œ $e^{i\pi/4}$ê°€ ê³±í•´ì ¸ ìˆë‹¤. ì´ë¥¼ <strong>ì „ì—­ ìœ„ìƒ(Global Phase)</strong>ì´ë¼ ë¶€ë¥¸ë‹¤.</p> <h4 id="ì •ê·œí™”-ì¡°ê±´-ì ìš©">ì •ê·œí™” ì¡°ê±´ ì ìš©</h4> <p>ê·¸ëŸ°ë° ìœ„ ìƒíƒœëŠ” ì•„ì§ ì •ê·œí™”ë˜ì§€ ì•Šì•˜ë‹¤. í™•ë¥ ì˜ í•©ì„ ê³„ì‚°í•˜ë©´: \(|\sqrt{2}|^2 + |-\sqrt{2}|^2 = 2 + 2 = 4 \neq 1\)</p> <p>ë”°ë¼ì„œ $\sqrt{4} = 2$ë¡œ ë‚˜ëˆ ì„œ ì •ê·œí™”í•´ì•¼ í•œë‹¤: \(\begin{align*} \ket{q}_{\text{normalized}} = e^{i\pi/4} \frac{1}{2}\begin{bmatrix} \sqrt{2} \\ -\sqrt{2} \end{bmatrix} = e^{i\pi/4} \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix} \end{align*}\)</p> <p>ì´ì œ ì •ê·œí™” ì¡°ê±´ì„ í™•ì¸í•˜ë©´: \(\left|\frac{1}{\sqrt{2}}\right|^2 + \left|\frac{-1}{\sqrt{2}}\right|^2 = \frac{1}{2} + \frac{1}{2} = 1 \;\checkmark\)</p> <p>ì „ì—­ ìœ„ìƒì„ ë¬´ì‹œí•˜ë©´, ì´ íë¹„íŠ¸ëŠ” ë¸”ë¡œí êµ¬ë©´ì˜ ì ë„ ìœ„ì— ìˆëŠ” $\ket{-} = \frac{1}{\sqrt{2}}(\ket{0} - \ket{1})$ ìƒíƒœì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p> <h3 id="ì „ì—­-ìœ„ìƒì˜-ì˜ë¯¸">ì „ì—­ ìœ„ìƒì˜ ì˜ë¯¸</h3> <p>ì „ì—­ ìœ„ìƒì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” <strong>ë¬¼ë¦¬ì ìœ¼ë¡œ ê´€ì¸¡ ë¶ˆê°€ëŠ¥í•˜ë‹¤</strong>ëŠ” ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´ ì¸¡ì • í™•ë¥ ì€ ë³µì†Œê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì˜ ì œê³±ìœ¼ë¡œ ê²°ì •ë˜ëŠ”ë°, ì „ì—­ ìœ„ìƒ $e^{i\theta}$ì˜ ì ˆëŒ“ê°’ì€ í•­ìƒ $1$ì´ê¸° ë•Œë¬¸ì´ë‹¤:</p> \[|e^{i\theta}|^2 = (\cos\theta + i\sin\theta)(\cos\theta - i\sin\theta) = \cos^2\theta + \sin^2\theta = 1\] <p>ë”°ë¼ì„œ $\ket{q}$ì™€ $e^{i\theta}\ket{q}$ëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ê²ƒì´ ìš°ë¦¬ê°€ ì°¾ë˜ ë‘ë²ˆì§¸ ì œì•½ ì¡°ê±´ì´ë‹¤!</p> <p>íë¹„íŠ¸ $\ket{q}=\alpha\ket{0}+\beta\ket{1}$ì„ ìƒê°í•´ë³´ì. $\alpha$ì™€ $\beta$ë¥¼ ê·¹ì¢Œí‘œë¡œ í‘œí˜„í•˜ë©´: \(\begin{align} \ket{q}&amp;=|\alpha|e^{i\theta_\alpha}\ket{0}+|\beta|e^{i\theta_\beta}\ket{1} \\ &amp;=e^{i\theta_\alpha}\left(|\alpha|\ket{0}+|\beta|e^{i(\theta_\beta-\theta_\alpha)}\ket{1}\right) \end{align}\)</p> <p>ì—¬ê¸°ì„œ $e^{i\theta_\alpha}$ëŠ” ì „ì—­ ìœ„ìƒì´ë¯€ë¡œ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì‹¤ì œë¡œ ë‹¤ìŒ ì„¸ ê°œì˜ ë³€ìˆ˜ë§Œ ìˆìœ¼ë©´ ëœë‹¤:</p> <ul> <li>$|\alpha|$ (í¬ê¸° 1)</li> <li>$|\beta|$ (í¬ê¸° 1)</li> <li>$\theta_\beta - \theta_\alpha$ (ìƒëŒ€ ìœ„ìƒ)</li> </ul> <p>ê·¸ëŸ°ë° ì •ê·œí™” ì¡°ê±´ $|\alpha|^2 + |\beta|^2 = 1$ì— ì˜í•´ $|\alpha|$ì™€ $|\beta|$ëŠ” ë…ë¦½ì ì´ì§€ ì•Šë‹¤. ë”°ë¼ì„œ ì‹¤ì œë¡œ í•„ìš”í•œ ë³€ìˆ˜ëŠ” <strong>2ê°œ</strong>ë¿ì´ë‹¤!</p> <p>ì´ê²ƒì´ ë°”ë¡œ íë¹„íŠ¸ë¥¼ ë¸”ë¡œí êµ¬ë©´(Bloch Sphere)ì´ë¼ëŠ” 2ì°¨ì› êµ¬ë©´ ìœ„ì˜ ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì´ìœ ì´ë‹¤. êµ¬ë©´ ìœ„ì˜ í•œ ì ì€ ê²½ë„ì™€ ìœ„ë„, ì¦‰ ë‘ ê°œì˜ ê°ë„ $\theta$(ê·¹ê°)ì™€ $\phi$(ë°©ìœ„ê°)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤:</p> \[\ket{q} = \cos\frac{\theta}{2}\ket{0} + e^{i\phi}\sin\frac{\theta}{2}\ket{1}\] <p>ì—¬ê¸°ì„œ $\theta \in [0, \pi]$, $\phi \in [0, 2\pi)$ì´ë‹¤.</p> <h4 id="ë¸”ë¡œí-êµ¬ë©´-ìœ„ì˜-ì¤‘ìš”í•œ-ìƒíƒœë“¤">ë¸”ë¡œí êµ¬ë©´ ìœ„ì˜ ì¤‘ìš”í•œ ìƒíƒœë“¤</h4> <p>ë¸”ë¡œí êµ¬ë©´ì—ì„œ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ íë¹„íŠ¸ ìƒíƒœë“¤ì„ ì‚´í´ë³´ì:</p> <p><strong>ë¶ê·¹ê³¼ ë‚¨ê·¹ (Zì¶•)</strong> \(\begin{align} \ket{0} &amp;= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \quad (\theta=0) \\ \ket{1} &amp;= \begin{bmatrix} 0 \\ 1 \end{bmatrix} \quad (\theta=\pi) \end{align}\)</p> <p><strong>ì ë„ ìœ„ì˜ ìƒíƒœë“¤ (X-Y í‰ë©´)</strong> \(\begin{align} \ket{+} &amp;= \frac{1}{\sqrt{2}}(\ket{0}+\ket{1}) = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix} \quad (\theta=\pi/2, \phi=0) \\ \ket{-} &amp;= \frac{1}{\sqrt{2}}(\ket{0}-\ket{1}) = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix} \quad (\theta=\pi/2, \phi=\pi) \\ \ket{+i} &amp;= \frac{1}{\sqrt{2}}(\ket{0}+i\ket{1}) = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ i \end{bmatrix} \quad (\theta=\pi/2, \phi=\pi/2) \\ \ket{-i} &amp;= \frac{1}{\sqrt{2}}(\ket{0}-i\ket{1}) = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -i \end{bmatrix} \quad (\theta=\pi/2, \phi=3\pi/2) \end{align}\)</p> <p>ì´ë“¤ ìƒíƒœëŠ” ê°ê° Xì¶•, Yì¶• ë°©í–¥ì˜ ì¸¡ì • ê¸°ì €ë¡œ ì‚¬ìš©ë˜ë©°, ì–‘ì ì•Œê³ ë¦¬ì¦˜ì—ì„œ ìì£¼ ë“±ì¥í•œë‹¤.</p> <h3 id="ë³µì†Œìˆ˜ì™€-ì–‘ì-ê°„ì„­">ë³µì†Œìˆ˜ì™€ ì–‘ì ê°„ì„­</h3> <p>ì•ì„œ íë¹„íŠ¸ë¥¼ ë³µì†Œìˆ˜ë¡œ í‘œí˜„í•´ì•¼ í•˜ëŠ” ì´ìœ ë¥¼ â€œí¬ê¸°ì™€ ìœ„ìƒì„ ë™ì‹œì— í‘œí˜„í•˜ê¸° ìœ„í•´ì„œâ€ë¼ê³  í–ˆë‹¤. ì´ì œ ê·¸ ì´ìœ ë¥¼ ë” ê¹Šì´ ì´í•´í•´ë³´ì.</p> <p>ë‹¨ìˆœíˆ í™•ë¥ ë§Œ í‘œí˜„í•˜ë ¤ë©´ ì‹¤ìˆ˜ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì–‘ìì»´í“¨íŒ…ì—ì„œëŠ” <strong>ì–‘ì ê°„ì„­(Quantum Interference)</strong> í˜„ìƒì„ í™œìš©í•œë‹¤. ë‘ ê²½ë¡œë¥¼ í†µí•´ ê°™ì€ ìƒíƒœì— ë„ë‹¬í•  ë•Œ, ìœ„ìƒì´ ê°™ìœ¼ë©´ ë³´ê°• ê°„ì„­(constructive interference)ì´ ì¼ì–´ë‚˜ í™•ë¥ ì´ ì¦ê°€í•˜ê³ , ìœ„ìƒì´ ë°˜ëŒ€ë©´ ìƒì‡„ ê°„ì„­(destructive interference)ì´ ì¼ì–´ë‚˜ í™•ë¥ ì´ ê°ì†Œí•œë‹¤.</p> <h4 id="ê°„ì„­ì˜-ì˜ˆì‹œ">ê°„ì„­ì˜ ì˜ˆì‹œ</h4> <p>ì•ì„œ ì •ì˜í•œ ë³µì†Œìˆ˜ $z_0 = 1+i$ì™€ $z_1 = -1-i$ë¥¼ ë‹¤ì‹œ ë³´ì. ì´ ë‘ ë³µì†Œìˆ˜ëŠ” $180Â°$ ìœ„ìƒì°¨ë¥¼ ê°€ì§€ê³  ìˆë‹¤ ($z_1 = -z_0$). ë§Œì•½ ì–‘ì ì—°ì‚°ì„ í†µí•´ ì´ ë‘ ì§„í­ì´ ë”í•´ì§„ë‹¤ë©´ $z_0 + z_1 = 0$ì´ ë˜ì–´ ì™„ì „íˆ ìƒì‡„ëœë‹¤.</p> <p>ë°˜ë©´ $z_0 = 1+i$ì™€ $z_2 = -1+i$ëŠ” $90Â°$ ìœ„ìƒì°¨ë¥¼ ê°€ì§„ë‹¤ ($z_2 = iz_0$). ì´ë“¤ì„ ë”í•˜ë©´ $z_0 + z_2 = 2i$ë¡œ ì†Œë©¸í•˜ì§€ ì•ŠëŠ”ë‹¤.</p> <p>ì´ëŸ¬í•œ <strong>ìœ„ìƒ ì •ë³´ë¥¼ í‘œí˜„í•˜ê³  ê°„ì„­ì„ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë³µì†Œìˆ˜ê°€ í•„ìˆ˜ì </strong>ì´ë‹¤. ì‹¤ìˆ˜ë§Œìœ¼ë¡œëŠ” ìœ„ìƒ ì •ë³´ë¥¼ ë‹´ì„ ìˆ˜ ì—†ê³ , ë”°ë¼ì„œ ì–‘ì ê°„ì„­ì´ë¼ëŠ” ê°•ë ¥í•œ í˜„ìƒì„ í™œìš©í•  ìˆ˜ ì—†ë‹¤. ë°”ë¡œ ì´ê²ƒì´ ì–‘ìì»´í“¨í„°ê°€ íŠ¹ì • ë¬¸ì œì—ì„œ ê³ ì „ ì»´í“¨í„°ë³´ë‹¤ ë¹ ë¥¼ ìˆ˜ ìˆëŠ” ê·¼ë³¸ì ì¸ ì´ìœ  ì¤‘ í•˜ë‚˜ì´ë‹¤.</p> <h2 id="ì‹¬í™”-ì‹¤ì œ-ì–‘ì-ì‹œìŠ¤í…œê³¼-ííŠ¸ë¦¬íŠ¸">ì‹¬í™”: ì‹¤ì œ ì–‘ì ì‹œìŠ¤í…œê³¼ ííŠ¸ë¦¬íŠ¸</h2> <blockquote> <p><strong>ğŸ’¡ ì´ ì„¹ì…˜ì€ ì‹¬í™” ë‚´ìš©ì…ë‹ˆë‹¤</strong><br/> íë¹„íŠ¸ì˜ ê¸°ë³¸ ê°œë…ë§Œ ì´í•´í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ì„¹ì…˜ì„ ê±´ë„ˆë›°ì–´ë„ ì¢‹ìŠµë‹ˆë‹¤.</p> </blockquote> <h3 id="ì‹¤ì œ-ì–‘ì-ì‹œìŠ¤í…œì—ì„œì˜-ì—ë„ˆì§€-ì¤€ìœ„">ì‹¤ì œ ì–‘ì ì‹œìŠ¤í…œì—ì„œì˜ ì—ë„ˆì§€ ì¤€ìœ„</h3> <p>ì§€ê¸ˆê¹Œì§€ íë¹„íŠ¸ë¥¼ $\ket{0}$ê³¼ $\ket{1}$ì˜ 2ì¤€ìœ„ ì‹œìŠ¤í…œìœ¼ë¡œ ë‹¤ë¤˜ì§€ë§Œ, ì‹¤ì œ ë¬¼ë¦¬ ì‹œìŠ¤í…œ(ì´ˆì „ë„ íë¹„íŠ¸, ì´ì˜¨ íŠ¸ë©, ì›ì ë“±)ì—ì„œëŠ” $\ket{0}$, $\ket{1}$ ì™¸ì—ë„ <strong>$\ket{2}$, $\ket{3}$, $\ket{4}$, â€¦ ë“±ì˜ ê³ ì°¨ ì—ë„ˆì§€ ì¤€ìœ„ê°€ í•­ìƒ ì¡´ì¬</strong>í•œë‹¤.</p> <p>ì˜ˆë¥¼ ë“¤ì–´, ì¡°í™”ì§„ë™ì(harmonic oscillator)ì˜ ì—ë„ˆì§€ ì¤€ìœ„ëŠ” ë“±ê°„ê²©ì´ë‹¤: \(E_n = \hbar\omega\left(n + \frac{1}{2}\right), \quad n = 0, 1, 2, 3, \ldots\)</p> <p>ì´ ê²½ìš° $\ket{0} \rightarrow \ket{1}$ ì „ì´ì™€ $\ket{1} \rightarrow \ket{2}$ ì „ì´ì˜ ì—ë„ˆì§€ ì°¨ì´ê°€ ê°™ë‹¤ ($\hbar\omega$). ë”°ë¼ì„œ $\ket{0}$ê³¼ $\ket{1}$ì—ë§Œ ì‘ìš©í•˜ë ¤ë˜ ë§ˆì´í¬ë¡œíŒŒê°€ ì˜ë„ì¹˜ ì•Šê²Œ $\ket{2}$ ìƒíƒœë„ ë“¤ëœ¨ê²Œ í•  ìˆ˜ ìˆë‹¤.</p> <h4 id="ë¹„ì¡°í™”-ì§„ë™ì-ì„¤ê³„">ë¹„ì¡°í™” ì§„ë™ì ì„¤ê³„</h4> <p>ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong>ë¹„ì¡°í™” ì§„ë™ì(anharmonic oscillator)</strong>ë¥¼ ì„¤ê³„í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ˆì „ë„ íë¹„íŠ¸ëŠ” ì¡°ì…‰ìŠ¨ ì ‘í•©(Josephson junction)ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì¡°í™”ì„±ì„ ë„ì…í•œë‹¤:</p> \[E_n \approx \hbar\omega_0 n - \frac{\hbar\alpha}{2}n(n-1)\] <p>ì—¬ê¸°ì„œ $\alpha &gt; 0$ëŠ” ë¹„ì¡°í™”ì„±(anharmonicity)ì´ë‹¤. ì´ë ‡ê²Œ í•˜ë©´:</p> <ul> <li>$\ket{0} \leftrightarrow \ket{1}$ ì „ì´ ì£¼íŒŒìˆ˜: $\omega_{01} = \omega_0$</li> <li>$\ket{1} \leftrightarrow \ket{2}$ ì „ì´ ì£¼íŒŒìˆ˜: $\omega_{12} = \omega_0 - \alpha$</li> </ul> <p>ë‘ ì „ì´ ì£¼íŒŒìˆ˜ê°€ ë‹¬ë¼ì§€ë¯€ë¡œ, $\omega_{01}$ì— ê³µëª…í•˜ëŠ” ì œì–´ í„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ $\ket{0}$ê³¼ $\ket{1}$ë§Œ ì„ íƒì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆë‹¤.</p> <blockquote> <p><strong>ì¤‘ìš”</strong>: ê³ ì°¨ ì¤€ìœ„ëŠ” ì™„ì „íˆ ì‚¬ë¼ì§€ì§€ ì•Šìœ¼ë©°, í•­ìƒ ì¡´ì¬í•œë‹¤. ë‹¤ë§Œ ì—ë„ˆì§€ ê°„ê²©ì„ ë‹¤ë¥´ê²Œ ë§Œë“¤ì–´ ì„ íƒì  ì œì–´ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤. ì¼ë¶€ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ¬í•œ ê³ ì°¨ ì¤€ìœ„ë¥¼ ì ê·¹ í™œìš©í•˜ê¸°ë„ í•œë‹¤.</p> </blockquote> <h3 id="ííŠ¸ë¦¬íŠ¸-qutrit">ííŠ¸ë¦¬íŠ¸ (Qutrit)</h3> <p>ê³ ì°¨ ìƒíƒœì˜ ì¡´ì¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´, ì„¸ ìƒíƒœ $\ket{0}, \ket{1}, \ket{2}$ì˜ ì¤‘ì²©ì¸ <strong>ííŠ¸ë¦¬íŠ¸(Qutrit)</strong>ë¥¼ ì‚´í´ë³´ì. ííŠ¸ë¦¬íŠ¸ëŠ” 3ì¤€ìœ„ ì–‘ì ì‹œìŠ¤í…œìœ¼ë¡œ:</p> \[\ket{q} = \alpha\ket{0} + \beta\ket{1} + \gamma\ket{2}\] <p>ì•ì„œ ì‚¬ìš©í•œ ë³µì†Œìˆ˜ $z_0, z_1, z_2$ë¡œ ì˜ˆì‹œë¥¼ ë§Œë“¤ë©´: \(\ket{q} = z_0\ket{0} + z_1\ket{1} + z_2\ket{2} = (1+i)\ket{0} + (-1-i)\ket{1} + (-1+i)\ket{2}\)</p> <p>ë²¡í„° í‘œê¸°ë¡œ ì“°ê³  ê·¹ì¢Œí‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ë©´: \(\begin{align} \ket{q} &amp;= \begin{bmatrix} 1+i \\ -1-i \\ -1+i \end{bmatrix} = \begin{bmatrix} \sqrt{2}e^{i\pi/4} \\ \sqrt{2}e^{i5\pi/4} \\ \sqrt{2}e^{i3\pi/4} \end{bmatrix} \\ &amp;= e^{i\pi/4}\begin{bmatrix} \sqrt{2} \\ \sqrt{2}e^{i\pi} \\ \sqrt{2}e^{i\pi/2} \end{bmatrix} = e^{i\pi/4}\begin{bmatrix} \sqrt{2} \\ -\sqrt{2} \\ \sqrt{2}i \end{bmatrix} \end{align}\)</p> <p>íë¹„íŠ¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì „ì—­ ìœ„ìƒ $e^{i\pi/4}$ê°€ ë‚˜íƒ€ë‚œë‹¤.</p> <h4 id="ì •ê·œí™”">ì •ê·œí™”</h4> <p>ì´ ìƒíƒœë„ ì •ê·œí™”ë˜ì§€ ì•Šì•˜ë‹¤. í™•ë¥ ì˜ í•©ì„ ê³„ì‚°í•˜ë©´: \(|\sqrt{2}|^2 + |-\sqrt{2}|^2 + |\sqrt{2}i|^2 = 2 + 2 + 2 = 6 \neq 1\)</p> <p>ë”°ë¼ì„œ $\sqrt{6}$ë¡œ ë‚˜ëˆ ì„œ ì •ê·œí™”í•œë‹¤: \(\begin{align*} \ket{q}_{\text{normalized}} = e^{i\pi/4} \frac{1}{\sqrt{6}}\begin{bmatrix} \sqrt{2} \\ -\sqrt{2} \\ \sqrt{2}i \end{bmatrix} = e^{i\pi/4} \sqrt{\frac{1}{3}}\begin{bmatrix} 1 \\ -1 \\ i \end{bmatrix} \end{align*}\)</p> <p>í™•ì¸: \(\left|\sqrt{\frac{1}{3}}\right|^2 + \left|-\sqrt{\frac{1}{3}}\right|^2 + \left|i\sqrt{\frac{1}{3}}\right|^2 = \frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 1 \;\checkmark\)</p> <p>ì „ì—­ ìœ„ìƒì„ ë¬´ì‹œí•˜ë©´, ê° ìƒíƒœê°€ ì¸¡ì •ë  í™•ë¥ ì€ ëª¨ë‘ $\frac{1}{3}$ë¡œ ë™ì¼í•˜ë‹¤.</p> <h3 id="ì—°ìŠµ-ë¬¸ì œ-ííŠ¸ë¦¬íŠ¸ì˜-ììœ ë„">ì—°ìŠµ ë¬¸ì œ: ííŠ¸ë¦¬íŠ¸ì˜ ììœ ë„</h3> <p>íë¹„íŠ¸ì— ëŒ€í•´ ë°°ìš´ ì •ê·œí™” ì¡°ê±´ê³¼ ì „ì—­ ìœ„ìƒ ê°œë…ì„ ííŠ¸ë¦¬íŠ¸ì— í™•ì¥í•´ë³´ì.</p> <p><strong>ë¬¸ì œ</strong>: ì„¸ ìƒíƒœ $\ket{0}, \ket{1}, \ket{2}$ì˜ ì¤‘ì²©ì¸ ííŠ¸ë¦¬íŠ¸ $\ket{q} = \alpha\ket{0} + \beta\ket{1} + \gamma\ket{2}$ë¥¼ í‘œí˜„í•˜ëŠ”ë° ëª‡ ê°œì˜ <strong>ì‹¤ìˆ˜ ë§¤ê°œë³€ìˆ˜</strong>ê°€ í•„ìš”í•œê°€?</p> <p><strong>íŒíŠ¸</strong></p> <p>ë‹¤ìŒ ì§ˆë¬¸ë“¤ì„ ìˆœì„œëŒ€ë¡œ ìƒê°í•´ë³´ì:</p> <ol> <li>ííŠ¸ë¦¬íŠ¸ëŠ” ëª‡ ì°¨ì› ë³µì†Œ ë²¡í„°ì¸ê°€?</li> <li>ì´ˆê¸°ì—ëŠ” ëª‡ ê°œì˜ ì‹¤ìˆ˜ ë³€ìˆ˜ê°€ í•„ìš”í•œê°€?</li> <li>ì •ê·œí™” ì¡°ê±´ì€ ì–´ë–»ê²Œ ë˜ëŠ”ê°€? ì´ê²ƒì´ ììœ ë„ë¥¼ ëª‡ ê°œ ì¤„ì´ëŠ”ê°€?</li> <li>ì „ì—­ ìœ„ìƒì€ ì—¬ì „íˆ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¬´ê´€í•œê°€? ì´ê²ƒì´ ììœ ë„ë¥¼ ëª‡ ê°œ ì¤„ì´ëŠ”ê°€?</li> </ol> <p><strong>ë‹µì•ˆ ë° í’€ì´</strong></p> <p><strong>1ë‹¨ê³„: ì´ˆê¸° ììœ ë„</strong> ííŠ¸ë¦¬íŠ¸ëŠ” 3ì°¨ì› ë³µì†Œ ë²¡í„°ì´ë¯€ë¡œ 3ê°œì˜ ë³µì†Œìˆ˜ $(\alpha, \beta, \gamma)$ê°€ í•„ìš”í•˜ë‹¤.<br/> ê° ë³µì†Œìˆ˜ëŠ” 2ê°œì˜ ì‹¤ìˆ˜ë¡œ í‘œí˜„ë˜ë¯€ë¡œ: <strong>6ê°œì˜ ì‹¤ë³€ìˆ˜</strong></p> <p><strong>2ë‹¨ê³„: ì •ê·œí™” ì¡°ê±´</strong> ì¸¡ì • í™•ë¥ ì˜ í•©ì´ 1ì´ì–´ì•¼ í•˜ë¯€ë¡œ: \(|\alpha|^2 + |\beta|^2 + |\gamma|^2 = 1\) ì´ ì¡°ê±´ì´ 1ê°œì˜ ë“±ì‹ì´ë¯€ë¡œ: <strong>-1 ììœ ë„</strong></p> <p><strong>3ë‹¨ê³„: ì „ì—­ ìœ„ìƒ</strong> $\ket{q}$ì™€ $e^{i\theta}\ket{q}$ëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ë™ì¼í•œ ìƒíƒœì´ë¯€ë¡œ: <strong>-1 ììœ ë„</strong></p> <p><strong>ìµœì¢… ë‹µ</strong>: \(6 - 1 - 1 = \boxed{4} \text{ ê°œì˜ ì‹¤ìˆ˜ ë§¤ê°œë³€ìˆ˜}\)</p> <p><strong>ì¼ë°˜í™”</strong>: $n$ì°¨ì› ì–‘ì ì‹œìŠ¤í…œ(qudit)ì€ <strong>$2n - 2$ê°œì˜ ì‹¤ìˆ˜ ë§¤ê°œë³€ìˆ˜</strong>ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p> <table> <thead> <tr> <th>ì‹œìŠ¤í…œ</th> <th>ì°¨ì› $n$</th> <th>ì‹¤ìˆ˜ ë§¤ê°œë³€ìˆ˜</th> <th>ë¹„ê³ </th> </tr> </thead> <tbody> <tr> <td>íë¹„íŠ¸ (Qubit)</td> <td>2</td> <td>$2(2) - 2 = 2$</td> <td>ë¸”ë¡œí êµ¬ë©´ ($\theta, \phi$) âœ“</td> </tr> <tr> <td>ííŠ¸ë¦¬íŠ¸ (Qutrit)</td> <td>3</td> <td>$2(3) - 2 = 4$</td> <td>4ì°¨ì› ê³µê°„</td> </tr> <tr> <td>íì¿¼íŠ¸ (Ququart)</td> <td>4</td> <td>$2(4) - 2 = 6$</td> <td>6ì°¨ì› ê³µê°„</td> </tr> <tr> <td>ì¼ë°˜ Qudit</td> <td>$n$</td> <td>$2n - 2$</td> <td>$(2n-2)$ì°¨ì› ê³µê°„</td> </tr> <tr> <td>Â </td> <td>Â </td> <td>Â </td> <td>Â </td> </tr> </tbody> </table> <p><strong>ë¬¼ë¦¬ì  ì˜ë¯¸</strong>: ííŠ¸ë¦¬íŠ¸ëŠ” 4ì°¨ì› ê³µê°„ì˜ ì ìœ¼ë¡œ í‘œí˜„ë˜ë¯€ë¡œ ë¸”ë¡œí êµ¬ë©´ì²˜ëŸ¼ 3ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•˜ê¸° ì–´ë µë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ì–‘ìì»´í“¨í„°ì—ì„œëŠ” ì£¼ë¡œ 2ì¤€ìœ„ ì‹œìŠ¤í…œ(íë¹„íŠ¸)ì„ ì‚¬ìš©í•œë‹¤. ê·¸ëŸ¬ë‚˜ ìµœê·¼ì—ëŠ” <strong>qutrit ì–‘ìì»´í“¨íŒ…</strong> ì—°êµ¬ë„ í™œë°œí•˜ë‹¤. ííŠ¸ë¦¬íŠ¸ëŠ” ê°™ì€ ê°œìˆ˜ì˜ ë¬¼ë¦¬ì  ê°ì²´ë¡œ ë” ë§ì€ ì •ë³´ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìœ¼ë©°, ì¼ë¶€ ì–‘ì ì•Œê³ ë¦¬ì¦˜ì—ì„œ íš¨ìœ¨ì„±ì´ ë” ë†’ì„ ìˆ˜ ìˆë‹¤.</p> <h2 id="ì¸¡ì •ì˜-ì˜ˆì‹œ">ì¸¡ì •ì˜ ì˜ˆì‹œ</h2> <p>íë¹„íŠ¸ì˜ ê°œë…ì„ ì™„ì „íˆ ì´í•´í•˜ê¸° ìœ„í•´, ê°„ë‹¨í•œ ì¸¡ì • ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.</p> <p>íë¹„íŠ¸ $\ket{q} = \frac{1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1}$ (ë¸”ë¡œí êµ¬ë©´ì˜ ì ë„ ìœ„ $\ket{+}$ ìƒíƒœ)ë¥¼ ìƒê°í•´ë³´ì.</p> <p>ë³¸ì˜ ê·œì¹™ì— ì˜í•´:</p> <ul> <li>$\ket{0}$ìœ¼ë¡œ ì¸¡ì •ë  í™•ë¥ : $\left|\frac{1}{\sqrt{2}}\right|^2 = \frac{1}{2}$</li> <li>$\ket{1}$ë¡œ ì¸¡ì •ë  í™•ë¥ : $\left|\frac{1}{\sqrt{2}}\right|^2 = \frac{1}{2}$</li> </ul> <p>ì´ íë¹„íŠ¸ë¥¼ ì¸¡ì •í•˜ë©´ <strong>50%ì˜ í™•ë¥ ë¡œ $\ket{0}$, 50%ì˜ í™•ë¥ ë¡œ $\ket{1}$ì´ ê´€ì¸¡</strong>ëœë‹¤. ì¸¡ì • ì „ì—ëŠ” ë‘ ìƒíƒœì˜ ì¤‘ì²©ì´ì—ˆì§€ë§Œ, ì¸¡ì • í›„ì—ëŠ” ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ í™•ì •ëœë‹¤.</p> <p>ë‹¤ë¥¸ ì˜ˆì‹œë¡œ, $\ket{qâ€™} = \frac{1}{2}\ket{0} + \frac{\sqrt{3}}{2}\ket{1}$ì„ ìƒê°í•´ë³´ì:</p> <ul> <li>$\ket{0}$ìœ¼ë¡œ ì¸¡ì •ë  í™•ë¥ : $\left|\frac{1}{2}\right|^2 = \frac{1}{4} = 25\%$</li> <li>$\ket{1}$ë¡œ ì¸¡ì •ë  í™•ë¥ : $\left|\frac{\sqrt{3}}{2}\right|^2 = \frac{3}{4} = 75\%$</li> </ul> <p>ì´ì²˜ëŸ¼ ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ í¬ê¸°ì— ë”°ë¼ ì¸¡ì • í™•ë¥ ì´ ê²°ì •ëœë‹¤.</p> <blockquote> <p><strong>ì¤‘ìš”</strong>: ì¸¡ì •ì€ íë¹„íŠ¸ì˜ ìƒíƒœë¥¼ <strong>íŒŒê´´</strong>í•œë‹¤. ì¸¡ì • ì „ ì¤‘ì²© ìƒíƒœì— ìˆë˜ íë¹„íŠ¸ëŠ” ì¸¡ì • í›„ $\ket{0}$ ë˜ëŠ” $\ket{1}$ë¡œ í™•ì •ë˜ë©°, ì›ë˜ì˜ ì¤‘ì²© ìƒíƒœë¡œ ëŒì•„ê°ˆ ìˆ˜ ì—†ë‹¤.</p> </blockquote> <h2 id="ê²°ë¡ ">ê²°ë¡ </h2> <p>ì§€ê¸ˆê¹Œì§€ ì–‘ìì»´í“¨íŒ…ì˜ ê¸°ë³¸ ë‹¨ìœ„ì¸ íë¹„íŠ¸ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ë‹¤. í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´:</p> <h3 id="í•µì‹¬-ìš”ì•½">í•µì‹¬ ìš”ì•½</h3> <p><strong>1. ì¤‘ì²© (Superposition)</strong></p> <ul> <li>íë¹„íŠ¸ëŠ” $\ket{0}$ê³¼ $\ket{1}$ì˜ ì–‘ì ì¤‘ì²© ìƒíƒœ: $\ket{q}=\alpha\ket{0}+\beta\ket{1}$</li> <li>ê³ ì „ ë¹„íŠ¸ëŠ” 0 ë˜ëŠ” 1, íë¹„íŠ¸ëŠ” â€œ0ê³¼ 1ì˜ ì¡°í•©â€</li> </ul> <p><strong>2. ë³µì†Œ ë²¡í„° ê³µê°„</strong></p> <ul> <li>íë¹„íŠ¸ëŠ” 2ì°¨ì› ë³µì†Œ ë²¡í„° ê³µê°„ì˜ ì›ì†Œ</li> <li>ë³µì†Œìˆ˜ëŠ” <strong>ìœ„ìƒ ì •ë³´</strong>ë¥¼ ë‹´ì•„ ì–‘ì ê°„ì„­ í˜„ìƒì„ ê°€ëŠ¥í•˜ê²Œ í•¨</li> </ul> <p><strong>3. ë³¸ì˜ ê·œì¹™ (Bornâ€™s Rule)</strong></p> <ul> <li>ì¸¡ì • í™•ë¥ : $P(0)=|\alpha|^2$, $P(1)=|\beta|^2$</li> <li>ì •ê·œí™” ì¡°ê±´: $|\alpha|^2 + |\beta|^2 = 1$</li> </ul> <p><strong>4. ë¸”ë¡œí êµ¬ë©´ (Bloch Sphere)</strong></p> <ul> <li>4ê°œì˜ ì‹¤ë³€ìˆ˜ â†’ ì •ê·œí™” ì¡°ê±´ê³¼ ì „ì—­ ìœ„ìƒ ë¬´ê´€ì„± â†’ 2ê°œì˜ ë§¤ê°œë³€ìˆ˜ ($\theta$, $\phi$)</li> <li>íë¹„íŠ¸ëŠ” 3ì°¨ì› êµ¬ë©´ ìœ„ì˜ ì ìœ¼ë¡œ ì‹œê°í™” ê°€ëŠ¥</li> <li>ì¼ë°˜ì ìœ¼ë¡œ $n$ì°¨ì› ì–‘ì ì‹œìŠ¤í…œì€ $2n-2$ê°œì˜ ì‹¤ìˆ˜ ë§¤ê°œë³€ìˆ˜ë¡œ í‘œí˜„ (ì‹¬í™” ì„¹ì…˜ ì°¸ê³ )</li> </ul> <p><strong>5. ì–‘ì ê°„ì„­</strong></p> <ul> <li>ìœ„ìƒì´ ê°™ìœ¼ë©´ ë³´ê°• ê°„ì„­ (í™•ë¥  ì¦ê°€)</li> <li>ìœ„ìƒì´ ë°˜ëŒ€ë©´ ìƒì‡„ ê°„ì„­ (í™•ë¥  ê°ì†Œ)</li> <li>ì´ê²ƒì´ ì–‘ìì»´í“¨í„°ì˜ ê³„ì‚° ëŠ¥ë ¥ì˜ í•µì‹¬</li> </ul> <h3 id="íë¹„íŠ¸ì˜-ì˜ì˜">íë¹„íŠ¸ì˜ ì˜ì˜</h3> <p>íë¹„íŠ¸ëŠ” ê³ ì „ ë¹„íŠ¸ë³´ë‹¤ ë³µì¡í•˜ì§€ë§Œ, ë°”ë¡œ ê·¸ ë³µì¡ì„±ì´ ì–‘ìì»´í“¨í„°ì˜ ê°•ë ¥í•œ ê³„ì‚° ëŠ¥ë ¥ì˜ ì›ì²œì´ ëœë‹¤.</p> <ul> <li><strong>ì¤‘ì²©</strong>: ì—¬ëŸ¬ ìƒíƒœë¥¼ ë™ì‹œì— í‘œí˜„</li> <li><strong>ê°„ì„­</strong>: ì›í•˜ëŠ” ë‹µì˜ í™•ë¥  ì¦í­, ì˜ëª»ëœ ë‹µì˜ í™•ë¥  ê°ì†Œ</li> <li><strong>ì–½í˜</strong> (Entanglement, ë‹¤ìŒ ê¸€ì—ì„œ): ì—¬ëŸ¬ íë¹„íŠ¸ ê°„ì˜ ìƒê´€ê´€ê³„</li> </ul> <p>ì´ ì„¸ ê°€ì§€ ì–‘ìì—­í•™ì  í˜„ìƒì„ í™œìš©í•˜ì—¬, ì–‘ìì»´í“¨í„°ëŠ” íŠ¹ì • ë¬¸ì œë“¤(ì¸ìˆ˜ë¶„í•´, ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰, ì–‘ì ì‹œë®¬ë ˆì´ì…˜ ë“±)ì—ì„œ ê³ ì „ ì»´í“¨í„°ë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.</p>]]></content><author><name></name></author><category term="quantum"/><category term="quantum-computing"/><category term="qubits"/><category term="bloch-sphere"/><category term="complex-numbers"/><category term="physics"/><category term="korean"/><summary type="html"><![CDATA[ì–‘ìì»´í“¨íŒ…ì˜ ê¸°ë³¸: íë¹„íŠ¸ì™€ ë¸”ë¡œí êµ¬ë©´, ê·¸ë¦¬ê³  ë³µì†Œìˆ˜ì˜ í•„ìš”ì„±]]></summary></entry><entry xml:lang="en"><title type="html">Toward a Unified Theory</title><link href="https://mutantq.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science/" rel="alternate" type="text/html" title="Toward a Unified Theory"/><published>2025-10-11T05:00:00+00:00</published><updated>2025-10-11T05:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science/"><![CDATA[<p><em>This essay synthesizes ideas from ongoing research on information compression theory, neural network approximation, and AI for science. It is based on my (unorganized) research notes on AI for science, written in a mix of both Korean and English. The writing has been auto-generated by Claude 4.5 Sonnet, and has been reviewed and revised by myself. Special thanks to Hongchul Nam, Rocky Kim, Seunghwan Jang, and other collaborators for discussions on Gordonâ€™s escape theorem, information bottleneck, and optimal transport approaches.</em></p> <p><em>EDIT: Title &amp; description edited on Oct 17, 2025</em></p> <h2 id="introduction-a-question-of-representation">Introduction: A Question of Representation</h2> <p>What does it mean to understand something? At its core, understanding requires finding an efficient representationâ€”a way to capture the essence of a phenomenon while discarding irrelevant details. This principle appears everywhere: in how we communicate through language, how we compress data, and perhaps most intriguingly, in how neural networks learn.</p> <p>Consider two seemingly different compression algorithms. Huffman coding eliminates redundancy by representing frequently repeated characters with fewer bitsâ€”a lossless technique where no information is lost. JPEG compression, by contrast, removes high-frequency signals that humans cannot readily perceive, achieving much higher compression ratios at the cost of some information loss. Both succeed because they identify and exploit patterns: one in the statistics of character frequency, the other in the structure of human perception.</p> <p>Now consider a more provocative question: <strong>What if neural networks are performing a form of nonlinear compression?</strong></p> <p>It is well established that sufficiently deep neural networks can approximate arbitrary continuous functionsâ€”the universal approximation theorem. But hereâ€™s the twist: the number of weights needed to approximate a function within a given error bound might serve as a measure of that functionâ€™s intrinsic complexity. Just as Huffman coding reveals the statistical structure of text, and JPEG reveals the perceptual structure of images, perhaps neural network capacity reveals something fundamental about the information content of functions themselves.</p> <p>This observation opens a deeper question. In linear algebra, there exists a one-to-one correspondence between matrices and linear transformationsâ€”every linear function is, ultimately, just an arrangement of numbers. If this principle extends to nonlinear functions, then <strong>information compression theory and neural network approximation theory might be two facets of the same underlying phenomenon</strong>. The possibility of such a unification would not merely be elegantâ€”it could fundamentally change how we understand learning, representation, and even scientific discovery itself.</p> <h2 id="intelligence-as-compression-the-principle-of-condensed-expression">Intelligence as Compression: The Principle of Condensed Expression</h2> <p>If compression algorithms reveal structure in data, perhaps intelligence itself is fundamentally about discovering and exploiting structure. I propose that <strong>a central function of intelligence is the generation of condensed expressions</strong>â€”representations that capture essential patterns while eliminating redundancy.</p> <p>This principle manifests at multiple levels:</p> <h3 id="level-1-statistical-redundancy">Level 1: Statistical Redundancy</h3> <p>The most basic form of condensation eliminates repetition. Just as Huffman coding represents frequent characters with fewer bits, saying â€œrepeat â€˜aâ€™ 20 timesâ€ is more efficient than writing out â€œaaaaaaaaaaaaaaaaaaaa.â€ This isnâ€™t merely about efficiencyâ€”itâ€™s about recognizing that the pattern itself (repeated â€˜aâ€™) contains the essential information, not its mechanical expansion.</p> <h3 id="level-2-shared-knowledge-as-implicit-compression">Level 2: Shared Knowledge as Implicit Compression</h3> <p>Hereâ€™s a deeper insight: <strong>we can eliminate redundancy not just within a message, but between minds</strong>. When you and I communicate, we implicitly compress messages by omitting our shared knowledge. A single wordâ€”â€home,â€ â€œdanger,â€ â€œbeautifulâ€â€”can invoke vast networks of shared experience and understanding without transmitting the underlying data.</p> <p>This explains phenomena that seem puzzling from a pure information-theoretic perspective:</p> <ul> <li>Why communication across cultural contexts is so difficult (different shared knowledge = different compression schemes)</li> <li>Why expert teams achieve seemingly telepathic coordination (extensive shared knowledge enables extreme compression)</li> <li>Why the same phrase can mean entirely different things to different people (decompression depends on the receiverâ€™s knowledge base)</li> </ul> <h3 id="level-3-context-as-adaptive-compression">Level 3: Context as Adaptive Compression</h3> <p>Not all information is equally relevant. A waiter focuses on menu items, not customer names. JPEG compression discards high-frequency visual information that humans barely perceive. Both examples illustrate <strong>context-dependent compression</strong>â€”the ability to adaptively discard information based on its relevance to current goals.</p> <p>This suggests something profound: <strong>what counts as â€œinformationâ€ is not absolute but goal-relative</strong>. The same physical stimulus contains different amounts of relevant information depending on what youâ€™re trying to achieve. Intelligence, then, involves not just pattern recognition but pattern relevance assessment.</p> <h3 id="level-4-world-models-as-compressed-reality">Level 4: World Models as Compressed Reality</h3> <p>Finally, consider the grandest form of condensation: our internal models of reality. The universe is incomprehensibly complexâ€”approximately $10^{80}$ atoms, each with position, momentum, quantum state. Yet our brains, with their roughly $10^{11}$ neurons and $10^{15}$ synapses, can predict, plan, and navigate this complexity.</p> <p>How? <strong>By learning a compressed representation</strong>â€”a world model that captures causal structure, regularities, and patterns while discarding the vast majority of microscopic details. From an evolutionary perspective, consciousness itself may have emerged as an adaptive compression algorithm: organisms that could efficiently represent absent prey, remember past events, and imagine future scenarios had survival advantages.</p> <p>Crucially, humans donâ€™t just compressâ€”we share compressed representations through language, enabling collective intelligence that transcends individual cognitive limits. A scientific theory is perhaps the ultimate condensed expression: a few equations capturing patterns that span countless observations.</p> <p>This multilevel view of intelligence-as-compression naturally leads us to ask: <strong>How do these principles manifest in artificial neural networks?</strong></p> <h2 id="neural-networks-as-nonlinear-basis-learners">Neural Networks as Nonlinear Basis Learners</h2> <p>The connection between compression and neural networks becomes clearer when we examine linear algebra through a compression lens. Consider two classical techniques:</p> <p><strong>Diagonalization</strong> finds a basis where a matrix becomes diagonalâ€”representing the linear transformation with only diagonal entries. This is lossless: every bit of information is preserved, just reorganized for maximum parsimony.</p> <p><strong>Singular Value Decomposition (SVD)</strong> goes further: it identifies principal components ordered by importance. By truncating small singular values, we achieve lossy compressionâ€”trading perfect accuracy for massive dimension reduction.</p> <p>These arenâ€™t just computational tricks. They reveal something fundamental: <strong>finding the right basis is equivalent to discovering compressible structure</strong>. Diagonalization finds a basis revealing perfect sparsity. SVD finds a basis revealing approximate low-rank structure.</p> <p>But hereâ€™s the limitation: these techniques only work for linear transformations. The world, however, is decidedly nonlinear.</p> <h3 id="the-nonlinear-generalization">The Nonlinear Generalization</h3> <p>This raises a tantalizing question: <strong>What would â€œSVD for nonlinear functionsâ€ look like?</strong></p> <p>I hypothesize that neural networks are precisely this generalization. Just as SVD learns an optimal linear basis for data compression, neural networks learn optimal nonlinear basesâ€”hierarchical representations that progressively extract and compress task-relevant features. The networkâ€™s architecture and weights together define an adaptive, nonlinear coordinate system optimized for the task at hand.</p> <p>This perspective reframes fundamental questions in deep learning:</p> <ul> <li><strong>Network capacity</strong> â†’ How much can we compress in this representation space?</li> <li><strong>Training</strong> â†’ Finding the basis that maximizes compression of training data</li> <li><strong>Generalization</strong> â†’ Whether the learned compression captures true underlying structure or merely memorizes noise</li> <li><strong>Transfer learning</strong> â†’ Reusing a learned basis across related compression problems</li> </ul> <h3 id="toward-a-mathematical-framework">Toward a Mathematical Framework</h3> <p>To formalize this intuition, consider rate-distortion theory. For a matrix $X$ sampled from distribution $\mathcal{D}$ and compressed representation $\tilde{X}$ (e.g., quantized singular values):</p> \[\begin{aligned} &amp;\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})\\ &amp;\text{subject to} \; \left&lt; d(x,\tilde{x}) \right&gt;_{p(x,\tilde{x})}\le D \end{aligned}\] <p>For deterministic SVD, $p(\tilde{x}|x)$ is deterministic, so $I(X; \tilde{X})=H(\tilde{X})-H(\tilde{X}|X)=H(\tilde{X})$â€”we simply minimize the entropy of our representation. But introducing stochasticity creates an interesting tradeoff: randomness enables exploring alternative compressions, potentially discovering better representations at the cost of slightly higher mutual information.</p> <p>More ambitiously, we might extend this framework to nonlinear function spaces. <strong>The key challenge: how do we define â€œmutual informationâ€ between a function and its neural network approximation?</strong> Answering this could provide a rigorous foundation for understanding neural network capacity in information-theoretic terms, potentially unifying compression theory and approximation theory.</p> <h3 id="learning-task-adaptive-bases">Learning Task-Adaptive Bases</h3> <p>Hereâ€™s where the compression perspective reveals deeper structure. Consider not one task, but a distribution of related tasksâ€”for instance, various manipulation tasks (sipping coffee, watering plants, opening doors) that share common sub-components like â€œgraspingâ€ and â€œmoving smoothly.â€</p> <p>A powerful insight emerges: <strong>the optimal representation for a task distribution should decompose tasks into shared, reusable components</strong>. This is meta-learning from a compression perspective.</p> <p>Given a distribution of tasks $\mathcal{D}$, where each sampled task $F \sim \mathcal{D}$ is a Lebesgue-integrable function from $\mathbb{R}^n$ to $\mathbb{R}^m$, and given the norm $|\cdot|$ defined by the inner product $\left&lt;f,g\right&gt;=\int_{\mathbb{R}^n}w(\mathbf{x}){f(\mathbf{x})\cdot g(\mathbf{x})} d\mathbf{x}$, what is the most efficient parametrized basis $\mathcal{B}_\theta = {f_1(\theta), f_2(\theta), \cdots,f_d(\theta) }$, i.e.,</p> \[\begin{align*} \underset{\theta \in \mathbb{R}^p} {\textrm{minimize}} \;\;\mathbb{E}_{F \sim \mathcal{D}} \left[ \left\| \sum_{i=1}^dC_i(\theta)f_i(\theta) - F \right\|^2 \right] \end{align*}\] <p>where $C_i(\theta):=\left&lt;f_i(\theta), F\right&gt;$.</p> <h3 id="a-radical-perspective-on-scientific-notation">A Radical Perspective on Scientific Notation</h3> <p>This view of basis learning illuminates a profound aspect of physics itself. Consider quantum perturbation theory: we expand perturbed states using the unperturbed Hamiltonianâ€™s eigenstates. But <strong>why commit to this basis? Is it optimal in an information-theoretic sense?</strong></p> <p>Perhaps the need to solve the SchrÃ¶dinger equation analytically reflects a limitation of mathematical notation rather than fundamental necessity. What if there existed a richer mathematical language where the appropriate basis emerges naturally from the potentialâ€™s structure? Computers offer precisely such richnessâ€”their representation space is vast and adaptable.</p> <p>This suggests a provocative reinterpretation of scientific formulas themselves. Consider $\mathbf{F}=m\mathbf{a}$. This formula appears information-rich, but actually relies heavily on implicit context: the meaning of equality, the physical interpretation of symbols, the calculus of derivatives. <strong>The formula is not knowledge itself, but a compressed pointer to knowledge stored in trained minds.</strong> Itâ€™s a trigger for decompressing vast networks of understanding.</p> <p>From this perspective, human scientific knowledge is itself a compression scheme: we develop notation systems that maximally compress patterns in nature given the constraint that other trained humans must be able to decompress them. Different fields develop different â€œcompression codebooksâ€â€”the vocabulary and notation that enables efficient communication among practitioners.</p> <p><strong>Could AI develop superior notation systems?</strong> Systems that compress physical laws more efficiently than human-readable equations? This isnâ€™t science fictionâ€”itâ€™s already happening. Neural network policies in robotics often cannot be â€œreadâ€ in human terms, yet they encode compressed motor skills that work. The question is whether we can extend this to theoretical physics: having AI discover not just solutions, but entirely new ways of formulating problems.</p> <h2 id="ai-for-science-compression-meets-discovery">AI for Science: Compression Meets Discovery</h2> <p>The compression perspective transforms how we think about scientific discovery itself. If theories are compressed representations of empirical patterns, then scientific progress can be understood as progressively discovering better compression schemes for natural phenomena.</p> <h3 id="from-experiment-to-simulation-compressing-the-cost-of-discovery">From Experiment to Simulation: Compressing the Cost of Discovery</h3> <p>Physical experiments are expensiveâ€”grotesquely so. High-throughput screening costs dollars per pipette action. CERNâ€™s Large Hadron Collider consumed billions in construction. Gravitational wave detectors require exquisite precision over kilometer-long installations. Every physical interaction with nature carries significant cost.</p> <p>Computation, by contrast, is cheap and becoming cheaper. Mooreâ€™s law has given every researcher access to Einstein-level thought experimentation: the ability to explore â€œwhat-ifâ€ scenarios without physical implementation. But viewing simulation merely as â€œcheap experimentationâ€ misses the profound shift happening now.</p> <h3 id="ai-as-active-scientist-beyond-passive-simulation">AI as Active Scientist: Beyond Passive Simulation</h3> <p>Modern AI systems donâ€™t just simulateâ€”they <strong>actively compress scientific knowledge in ways humans cannot</strong>. Consider:</p> <ul> <li> <p><strong>Solving intractable PDEs</strong>: Neural networks can approximate solutions to differential equations that resist analytical solution, effectively compressing infinite-dimensional function spaces into finite parameter sets.</p> </li> <li> <p><strong>Autonomous circuit design</strong>: AI explores design spaces too vast for human search, compressing engineering knowledge into optimized structures.</p> </li> <li> <p><strong>Theory formation</strong>: Large language models trained on scientific corpora can propose hypotheses, design experiments, and even formulate mathematical relationshipsâ€”performing inductive compression from observation to theory.</p> </li> </ul> <p>This represents a qualitative change: <strong>AI is becoming a scientific agent, not merely a tool.</strong> Research groups that harness thisâ€”that treat AI as colleague rather than calculatorâ€”will have profound advantages.</p> <h3 id="the-epistemology-of-machine-science">The Epistemology of Machine Science</h3> <p>This raises deep questions about the nature of scientific knowledge:</p> <p><strong>What is â€œknowledgeâ€ in a quantifiable sense?</strong> Consider an AI chemist connected to laboratory robotics, autonomously running experiments. Its objective should be maximizing knowledge gainâ€”but how do we formalize this?</p> <p>One approach: <strong>knowledge is compressible surprise</strong>. High knowledge means the ability to predict novel phenomena with compressed models. An AI chemist should seek experiments that maximally reduce uncertainty about chemical space, preferentially exploring regions where current models compress poorly.</p> <p>This connects to active learning and optimal experimental design, but reframes them in information-theoretic terms: <strong>research is the art of efficiently compressing natureâ€™s patterns through strategic interaction.</strong></p> <p><strong>How do humans know what they donâ€™t know?</strong> Metacognitionâ€”awareness of ignoranceâ€”seems uniquely human. Yet itâ€™s crucial for directing curiosity and research. Can we instill this in AI? Perhaps through explicit uncertainty quantification: teaching models to recognize when their compressions break down, when their basis functions fail to capture observed structure.</p> <p>Recent work by Chlon et al. (2024) provides precisely this kind of framework. Their analysis reveals that hallucinations in large language models are <strong>predictable compression failures</strong>â€”occurring when models minimize expected conditional description length but encounter data structures their learned bases cannot adequately represent. They show that LLMs are â€œBayesian in expectation, not in realization,â€ leading to systematic deviations when permutation-dependent compressions fail. Critically, they introduce quantifiable metrics for detecting when a modelâ€™s information budget is insufficient for reliable decompression. This transforms uncertainty from post-hoc error detection to <strong>pre-emptive epistemic honesty</strong>: an AI scientist that recognizes its compression is failing can say â€œI need more experimental data to build an adequate basisâ€ rather than confabulating plausible-sounding theories. This is precisely the metacognitive awareness needed for autonomous scientific discoveryâ€”knowing not just what you know, but when your basis functions are inadequate.</p> <h3 id="cloning-vs-approximation-the-quantum-distinction">Cloning vs. Approximation: The Quantum Distinction</h3> <p>Hereâ€™s a crucial distinction: <strong>Quantum computers clone; neural networks approximate.</strong></p> <p>Quantum simulators maintain direct physical correspondenceâ€”one quantum system representing another with perfect fidelity. Neural networks, by contrast, learn compressed approximationsâ€”capturing behavioral patterns without necessarily preserving microscopic structure.</p> <p>This suggests complementary roles: quantum computers for faithful simulation of quantum systems, neural networks for discovering compressed effective theories that capture relevant behavior at the scale of interest. The future may involve hybrid approaches: quantum hardware providing high-fidelity data, classical AI discovering compressed models that generalize beyond specific instances.</p> <h3 id="collective-intelligence-multi-agent-compression">Collective Intelligence: Multi-Agent Compression</h3> <p>A tantalizing possibility: <strong>Can multiple AI agents collaboratively discover better compressions than single agents?</strong></p> <p>Imagine AI researchers gathered at a virtual blackboard, proposing models, critiquing, building on each otherâ€™s insightsâ€”a reinforcement learning game where the objective is joint knowledge compression. This mirrors human scientific communities, where collective intelligence emerges from communication and competition.</p> <p>The compression perspective suggests such collaboration could be formalized: agents maintain individual compression schemes (world models) but share compressed communications (hypotheses, data, critiques). The system evolves toward consensus compressions that capture shared structure while specializing in complementary aspects.</p> <p>This is speculative but points toward a future where <strong>scientific discovery itself becomes scalable through AI collaboration</strong>, moving beyond the cognitive limits of individual human researchers.</p> <h2 id="ai-for-quantum-mechanics-the-ultimate-compression-challenge">AI for Quantum Mechanics: The Ultimate Compression Challenge</h2> <h3 id="a-professors-challenge">A Professorâ€™s Challenge</h3> <p>In my sophomore year, eager to dive into quantum mechanics, I enrolled in an advanced course a year early. On the first day, the professor made a bold claim: <strong>even machine learning could never discover the SchrÃ¶dinger equation.</strong></p> <p>This assertion fascinated me. What makes quantum mechanics special? Why should it resist machine discovery when AI excels at pattern recognition?</p> <p>Letâ€™s be precise about what â€œdiscovering the SchrÃ¶dinger equationâ€ means: <strong>identifying patterns in quantum wave phenomena from experimental data and expressing them in communicable mathematical form.</strong> If a machine could do thisâ€”finding invariants that govern quantum dynamicsâ€”we would have to acknowledge genuine scientific discovery by AI.</p> <p>The question connects directly to our compression theme: <strong>Is the SchrÃ¶dinger equation the maximally compressed representation of quantum phenomenology?</strong> Or might there exist alternative formulationsâ€”perhaps ones natural to AI but alien to human physicistsâ€”that compress quantum mechanics more efficiently?</p> <h3 id="symbolic-regression-learning-physical-laws-from-data">Symbolic Regression: Learning Physical Laws from Data</h3> <p>Symbolic regression provides a methodology for exactly this kind of discovery. Rather than fitting predefined function forms, symbolic regression autonomously generates candidate equations, testing them against data. Pioneered by John Koza in the early 1990s using genetic algorithms, modern variants like AIFeynman leverage deep learning to search equation space more efficiently.</p> <p>Consider rediscovering Newtonâ€™s second law. Given time-series data of force $\mathbf{F}(t)$, position $\mathbf{x}(t)$, and mass $m$, can an algorithm discover that $\mathbf{F}-m\ddot{\mathbf{x}}=\mathbf{0}$ always holds? This is discovering an invariantâ€”a conserved pattern amid changing observations.</p> <p>Therefore, symbolic regression can be thought of as minimizing the following loss function:</p> \[\mathcal{L}(f_\text{expr}):=\|f_\text{expr}( \mathbf{F},\mathbf{x}, m)\|^2\] <p>When $\mathcal{F} := { \mathbf{v}: [t_i, t_f] \rightarrow \mathbb{R}^3 }$, $\mathbf{F},\mathbf{x}\in\mathcal{F}$ are functions of time $t\in [t_i,t_f]$ and can be differentiated as much as desired, and $f_{\text{expr}}:\mathcal{F}\times\mathcal{F}\times\mathbb{R}^+ \rightarrow \mathcal{F}$ is a well-formed expression made using operators we know such as addition, multiplication, and differentiation.</p> <h3 id="the-equation-complexity-problem">The Equation Complexity Problem</h3> <p>Hereâ€™s the fundamental tradeoff: <strong>simpler equations compress better but may fit worse; complex equations fit better but donâ€™t compress.</strong></p> <p>This is precisely analogous to the bias-variance tradeoff in machine learning, but now applied to equation space. To prevent overfittingâ€”discovering spurious patterns in measurement noiseâ€”we need a complexity penalty on $f_\text{expr}$.</p> <p>This raises deep questions:</p> <ol> <li> <p><strong>How do we measure equation complexity?</strong> String length? Number of operators? Kolmogorov complexity of the expression tree? Each choice embodies assumptions about what makes theories â€œelegant.â€</p> </li> <li> <p><strong>Can we constrain AI to generate only well-formed expressions?</strong> Large language models trained on scientific literature learn implicit syntax of equations. Can we architect them to guarantee mathematical validityâ€”generating only expressions with proper units, matched dimensions, sensible operator precedence?</p> </li> <li> <p><strong>Do equations have meaningful embeddings?</strong> Can we create a latent space where nearby points represent similar physical laws? If so, we could search equation space the way CLIP searches image-text spaceâ€”by navigating a learned manifold of meaning.</p> </li> </ol> <p>Answering these questions could enable AI systems that not only discover equations but do so with scientific tasteâ€”preferring simple, elegant compressions over baroque memorization.</p> <h3 id="quantum-computers-as-discovery-engines">Quantum Computers as Discovery Engines</h3> <p>Hereâ€™s a crucial insight: <strong>AI needs data to learn, and quantum experiments are exponentially expensive to simulate classically.</strong> This creates a beautiful synergy: quantum computers as experimental playgrounds for AI scientists.</p> <p>Humans discovered quantum mechanics through centuries of experimental interactionâ€”double-slit experiments, spectroscopy, the photoelectric effect. We didnâ€™t derive quantum mechanics from first principles; we discovered it by playing with nature. Why should AI be different?</p> <p>If we expect AI to discover the SchrÃ¶dinger equation from minimal data, weâ€™re setting an impossible barâ€”like expecting humans to derive quantum mechanics from a handful of observations without experimental apparatus. But <strong>give AI access to a quantum computer, and it can conduct millions of quantum experiments</strong>, exploring parameter spaces inaccessible to human experimenters, potentially discovering patterns weâ€™ve missed.</p> <p>This isnâ€™t science fiction. The ingredients exist:</p> <ul> <li><strong>Quantum hardware</strong>: Noisy Intermediate-Scale Quantum (NISQ) devices enable controlled quantum experiments</li> <li><strong>Symbolic regression algorithms</strong>: AIFeynman and successors can search equation space efficiently</li> <li><strong>Reinforcement learning</strong>: AI can learn to design informative experiments, not just analyze given data</li> </ul> <p>The paradox: quantum brains donâ€™t seem necessary for biological intelligence (our neurons appear classical), yet quantum computers may be necessary for AI to truly understand quantum mechanics. The difference? <strong>Data accessibility.</strong> Humans evolved in a classical-appearing world but built quantum instruments. AI needs direct quantum playgrounds to compress quantum patterns efficiently.</p> <h3 id="neural-networks-for-quantum-eigenvalue-problems">Neural Networks for Quantum Eigenvalue Problems</h3> <p>Letâ€™s devise a simple symbolic regression methodology that uses machine learning for quantum computation. First, writing the SchrÃ¶dinger equation:</p> \[\hat{H} \Psi (\mathbf{r}, t) = i\hbar \frac{\partial}{\partial t} \Psi(\mathbf{r}, t)\] <p>When the Hamiltonian is invariant with respect to time, we find solutions to the eigenvalue problem $\hat{H}\psi(\mathbf{r})= E\psi(\mathbf{r})$ and multiply by the phase factor $e^{-iEt/\hbar}$ to evolve themâ€”game over.</p> <p>The action of an operator on a wave function can be interpreted as a linear transformation acting on a vector. And the eigenvalue problem can be thought of as finding the axis of symmetry whose direction is invariant before and after applying the transformation. Can we approximate and obtain these â€œaxes of symmetry,â€ i.e., eigenfunctions, with neural networks?</p> <p>Replace the wave function $\psi:\mathbb{R}^n\rightarrow\mathbb{C}$ satisfying the eigenvalue equation $\hat{H}\psi = E\psi$ with the neural network $\psi_\theta$. Then the loss function can be expressed as follows for some norm $|\cdot|$:</p> \[\mathcal{L}(\theta, E)=\|\hat{H}\psi_\theta - E\psi_\theta\|^2\] <p>There are still some unresolved problems with the above approach. For example, how do we define the above norm? One possibility is to define the norm of function $f$ as $|f|=\sqrt{\mathbb{E}\left[|f(X)|^2\right]},\;\;X \sim \mathcal{N}(\mathbf{0}, I_{n\times n})$. However, the wave function $\psi_\theta$ defined by a neural network is extremely complex, and considerable computational resources are consumed to calculate the expectation value used in the norm.</p> <p>Additionally, a method has not been prepared for calculating the action of the Hamiltonian on the (neural network-defined) wave function. Suppose we approximate the Hamiltonian again with a neural network. When the dimension of $\theta$ is $N$, we need to newly define an operator $\hat{H}_\Theta: \mathbb{R}^N \rightarrow \mathbb{R}^N$ defined by a neural network.</p> <p>We can confirm that computational complexity increases exponentially according to the complexity of the system being simulated. What does this mean? If we utilize artificial intelligence, physics at the level of small molecules can be simulated without much difficulty. However, it does not seem possible to simulate the dynamics of larger quantum systems of ~10,000 level without compromising on accuracy.</p> <h3 id="the-paradox-of-quantum-computing-for-ai">The Paradox of Quantum Computing for AI</h3> <p>If we can sufficiently describe quantum mechanics just by obtaining the simulation function, there is no need to insist on quantum computers. However, paradoxically, the cheapest way to obtain large-scale quantum experimental data is quantum computing. We must explore whether quantum parallelism can provide practically significant help, and if so, how much.</p> <p>According to what has been revealed so far about neural networks, through training, they can learn patterns and structures embedded in multidimensional data, and can compressively represent revealed information through dimensionality reduction techniques. And it is also quite possible to map information stored as vectors this way into formulas that humans can see by using natural language processing.</p> <p>The fact that machines cannot discover quantum phenomena on their own seems rather implausible given the speed of AI development, but considering the characteristic of quantum phenomena where computational complexity increases exponentially according to the complexity of the system, the professorâ€™s statement may not be so wrong after all.</p> <h2 id="research-questions-and-future-directions">Research Questions and Future Directions</h2> <h3 id="gordons-escape-theorem-and-dataset-intrinsic-dimension">Gordonâ€™s Escape Theorem and Dataset Intrinsic Dimension</h3> <p>Our current research direction focuses on Gordonâ€™s escape theorem combined with incorporating dataset intrinsic dimension. We need practical estimation algorithms for the datasetâ€™s intrinsic dimension (e.g., PCA). <strong>We need to give researchers a tool that can estimate the minimum amount of parameters needed to train for a certain task.</strong></p> <p>Gordonâ€™s escape theorem states that in high-dimensional spaces, a random subspace of sufficient dimension will â€œescapeâ€ through any mesh of low complexity with high probability. This theorem provides a powerful tool for understanding the behavior of random projections and has important applications in compressed sensing and dimensionality reduction.</p> <h3 id="information-bottleneck-and-optimal-transport">Information Bottleneck and Optimal Transport</h3> <p>The Information Bottleneck Method introduces the bottleneck $\tilde{X}$ to form the Markov chain $X \rightarrow \tilde{X} \rightarrow Y$, and drawing ideas from rate-distortion theory we obtain:</p> \[\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})-\beta I(X;Y)\] <p>An alternative approach is exploring â€œoptimal transport.â€ If a data measure exists on a manifold, it can be represented by manifold structure, and we can find the optimal transport that moves it. This has significance in that it explicitly incorporates the manifold hypothesis into generalization. However, there is little discussion about predicting neural network parameters.</p> <h3 id="fractal-structures-and-compression">Fractal Structures and Compression</h3> <p>Complex structures like the Mandelbrot Set or Bifurcation Diagram are embedded in extremely simple formulas. Can we devise information compression algorithms that borrow such structures? Can we analyze fractal structures or bifurcation diagrams with neural networks to derive insights into chaotic systems? Fractal compression and the collage theorem are worth exploring.</p> <h3 id="connection-to-complexity-theory">Connection to Complexity Theory</h3> <p>There are $O(n^2)$ and $O(n\log n)$ algorithms which all perform the same taskâ€”sorting. Can we argue that one is a lossless compression of the other, since it uses less computation?</p> <h2 id="conclusion-the-compression-paradigm-for-intelligence-and-discovery">Conclusion: The Compression Paradigm for Intelligence and Discovery</h2> <p>We began with a simple observation: compression algorithms reveal structure. We end with a radical hypothesis: <strong>intelligence itself is compression, and scientific discovery is the search for maximally compressed representations of natural patterns.</strong></p> <p>This perspective unifies disparate phenomena:</p> <p><strong>Intelligence as Multilevel Compression</strong>: From statistical redundancy removal to context-dependent information filtering to world model construction, intelligence operates by finding and exploiting compressible structure. Human communication achieves efficiency through shared knowledgeâ€”implicit compression between minds. Consciousness itself may be an evolutionary compression mechanism: organisms that efficiently represented their environments survived and reproduced.</p> <p><strong>Neural Networks as Nonlinear Basis Learners</strong>: SVD reveals that finding optimal linear bases is equivalent to discovering compressible structure. I hypothesize that neural networks generalize this: they learn optimal nonlinear basesâ€”hierarchical coordinate systems that progressively compress task-relevant features. This reframes deep learningâ€™s core questions: capacity becomes compression capability, training becomes basis optimization, and generalization becomes distinguishing true structure from noise.</p> <p><strong>Scientific Theories as Compressed Predictors</strong>: Physics formulas are not knowledge themselves but compressed pointers to knowledgeâ€”triggers for decompressing understanding stored in trained minds. Different scientific fields develop specialized â€œcompression codebooksâ€ (notations, concepts) that enable efficient communication among practitioners. Could AI discover superior compression schemesâ€”theoretical frameworks more compact than human-readable equations yet equally predictive?</p> <p><strong>Quantum Mechanics as Ultimate Compression Challenge</strong>: The exponential scaling of quantum systemsâ€”computational complexity growing with system sizeâ€”makes them the ultimate test for compression-based AI. Classical simulation quickly becomes intractable. Yet quantum computers offer a solution: experimental playgrounds where AI can gather quantum data cheaply, potentially discovering patterns (and compressions) that humans, constrained to classical intuition, have missed.</p> <h3 id="a-research-vision">A Research Vision</h3> <p>This compression paradigm suggests concrete research directions:</p> <ol> <li> <p><strong>Formalizing neural network capacity in information-theoretic terms</strong>: Extending mutual information to function spaces, connecting Gordonâ€™s escape theorem with dataset intrinsic dimension, developing tools that predict minimum parameter counts for tasks.</p> </li> <li> <p><strong>Equation embeddings and learned equation spaces</strong>: Creating latent spaces where physics laws cluster by similarity, enabling search through theory space guided by both empirical fit and compression criteria.</p> </li> <li> <p><strong>AI-quantum synergy for discovery</strong>: Coupling symbolic regression with quantum experimental hardware, letting AI autonomously design and execute quantum experiments, searching for compressed descriptions of quantum phenomenology.</p> </li> <li> <p><strong>Multi-agent collaborative compression</strong>: Formalizing scientific communities as distributed compression systems, where agents with specialized bases share compressed communications, evolving toward consensus theories.</p> </li> <li> <p><strong>Meta-learning across task distributions</strong>: Discovering parametric bases that optimally decompose task families into reusable componentsâ€”the mathematics of transfer learning and few-shot generalization.</p> </li> </ol> <h3 id="the-deeper-question">The Deeper Question</h3> <p>My professor claimed that machine learning could never discover the SchrÃ¶dinger equation. Having explored the landscape, I believe the claim reveals something profoundâ€”not about AIâ€™s limitations, but about the nature of understanding itself.</p> <p>Perhaps understanding isnâ€™t about possessing a formula, but about having the right compression scheme. Humans â€œunderstandâ€ quantum mechanics not because we can write $\hat{H}\Psi = i\hbar \partial_t \Psi$, but because centuries of training have given us a decompression algorithm: when we see this notation, vast networks of meaning activateâ€”Hilbert spaces, measurement, superposition, entanglement.</p> <p><strong>Can AI understand quantum mechanics differently than humans?</strong> Not by learning our compression scheme, but by discovering its ownâ€”one that perhaps compresses quantum patterns more efficiently but maps poorly to human notation? If AI discovers a theory that predicts quantum phenomena better than the SchrÃ¶dinger equation but expresses it in 100,000 neural network parameters, have we succeeded or failed?</p> <p>This brings us full circle: <strong>What counts as understanding?</strong> If understanding is successful compression enabling prediction and manipulation, then AI understanding need not mirror human understanding. The question isnâ€™t whether AI can discover the SchrÃ¶dinger equation, but whether it can discover <em>something better</em>â€”a more compressed, more predictive representation of quantum reality.</p> <p>The integration of information compression theory and neural network approximation theory isnâ€™t just possibleâ€”it may be necessary for understanding intelligence itself. And pursuing this integration might not only revolutionize AI and science, but fundamentally transform what we mean by knowledge, discovery, and understanding.</p> <h2 id="references">References</h2> <p><strong>Compression and Hallucinations:</strong></p> <ul> <li>Chlon, L., Karim, A., &amp; Chlon, M. (2024). Predictable Compression Failures: Why Language Models Actually Hallucinate. arXiv:2509.11208. Available at: <a href="https://arxiv.org/abs/2509.11208">https://arxiv.org/abs/2509.11208</a></li> </ul> <p><strong>Neural Network Theory and Capacity:</strong></p> <ul> <li>How many degrees of freedom do we need to train deep networks: a loss landscape perspective. arXiv:2107.05802</li> <li>Intrinsic dimension of data representations in deep neural networks. arXiv:1905.12784</li> <li>Generalization bounds for deep learning. arXiv:2012.04115</li> <li>Gordonâ€™s escape theorem and related work on high-dimensional geometry</li> </ul> <p><strong>Information Theory:</strong></p> <ul> <li>The information bottleneck method. arXiv:physics/0004057</li> <li>Deep Learning and the Information Bottleneck Principle. arXiv:1503.02406</li> </ul> <p><strong>AI for Science:</strong></p> <ul> <li>Symbolic regression literature including AIFeynman</li> <li>Neural network methods for solving differential equations</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="artificial-intelligence"/><category term="information-theory"/><category term="neural-networks"/><category term="quantum-computing"/><category term="scientific-discovery"/><category term="compression"/><category term="english"/><summary type="html"><![CDATA[Information Compression, Neural Networks, and AI for Science: future of scientific discovery through AI]]></summary></entry><entry xml:lang="en"><title type="html">Signing Right Away</title><link href="https://mutantq.github.io/blog/2025/signing-right-away/" rel="alternate" type="text/html" title="Signing Right Away"/><published>2025-10-05T11:00:00+00:00</published><updated>2025-10-05T11:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/signing-right-away</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/signing-right-away/"><![CDATA[<p><em>This work contains AI-generated paragraphs and sentences. The original whitepaper of this work has been written by myself in English. The original experiment notes (undisclosed) were written in Korean by myself and my teammates, Wonbeen Yoon and Minjun Yi from Seoul National University. Gemini Deep Research was used to organize the work into the full whitepaper. Each work was fully reviewed and revised by myself. Special thanks to Prof. Soojean Han for generously providing the initial Trion T20 FPGA board needed for prototyping, and also Hrvoje (Harvey) Puh for his valuable feedback in the original whitepaper.</em></p> <p>You can find the full whitepaper here: <strong><a href="/assets/pdf/SRA-2025-10-05.pdf">PDF</a></strong> <br/> The original whitepaper can be viewed here: <strong><a href="/assets/pdf/SRA-2024-05-26.pdf">PDF</a></strong></p> <h2 id="a-brief-motivation">A Brief Motivation</h2> <p>The proliferation of generative AI has made it trivial to create hyper-realistic fake images and videos, posing a serious threat to information integrity. This raises significant concerns over disinformation and fraud. While many approaches try to solve this with software classifiers, the root of the problem is arguably in hardware.</p> <p>In most current systems, the camera module sends an unencrypted, raw bitstream to the main processor over an interface like MIPI CSI-2. This link is vulnerable; a simple adapter can be used to intercept the feed or inject entirely synthetic data, and the system would have no way of knowing.</p> <p>This suggests that a robust solution requires securing content provenance at the source.</p> <h2 id="an-early-attempt-and-a-hard-reset">An Early Attempt and a Hard Reset</h2> <p>The initial idea for SRA was formalized back in the spring of 2024. Shortly after, I began my mandatory military service, which put the project on hold. After being discharged recently, I gathered a few friends to reboot the project with fresh energy.</p> <p>Our goal was ambitious: to reverse-engineer and replicate a secure transport layer for the MIPI CSI-2 protocol without official documentation. To put it mildly, it was a failure. Our attempts to build on an unknown, undocumented foundation resulted in glitchy, unparseable camera feeds. The custom parsing logic we wrote would fail intermittently, and the entire pipeline was fundamentally unstable. It was a disaster.</p> <p>But the experience, while painful, was incredibly valuable. It taught us two critical lessons:</p> <ol> <li> <p><strong>Hardware limitations are real.</strong> Our FPGA platform needed significantly more memory to buffer and process full image frames in real-time.</p> </li> <li> <p><strong>Reverse engineering has its limits.</strong> To build a stable image processing pipeline, we couldnâ€™t rely on guesswork alone. We needed access to at least some confidential documentation or, failing that, a far more powerful and flexible hardware platform to allow for rapid, iterative testing.</p> </li> </ol> <h2 id="the-sra-architecture">The SRA Architecture</h2> <p>The core architecture of SRA was established in our original 2024 whitepaper, based on fundamental cryptographic principles of confidentiality, integrity, authentication, and replay protection. We initially designed our system around authenticated encryption schemes like ChaCha20-Poly1305. During development, we discovered that the MIPI Allianceâ€™s Camera Security Framework had independently standardized similar approaches, which validated our architectural choices. While the prototyping experience taught us crucial lessons about implementation strategyâ€”particularly the need for hardware-accelerated cryptography and better development platformsâ€”the fundamental architectural design remained consistent.</p> <p>The architecture involves two main components:</p> <h3 id="1-authenticated--encrypted-camera-to-processor-link">1. Authenticated &amp; Encrypted Camera-to-Processor Link</h3> <p>The first step is to secure the physical data path. The camera module and processor would first perform a mutual authentication handshake. Once trust is established, all data transmitted over the CSI-2 interface would be protected by an authenticated encryption (AEAD) scheme, like AES-GCM. This ensures both confidentiality and integrity, as any modification would be detected via MAC verification.</p> <h3 id="2-immediate-signing-in-a-trusted-execution-environment-tee">2. Immediate Signing in a Trusted Execution Environment (TEE)</h3> <p>The encrypted feed is sent directly to a TEE, an isolated, secure enclave on the processor. Inside the TEE, the data is decrypted, processed, and cryptographically signed along with its metadata (e.g., timestamp, device ID). The private signing keys never leave the TEE, protecting them from a compromised OS. The final output is a standard image file with an embedded, verifiable C2PA Content Credential. This design ensures that by the time an application or user has access to an image, it has already been signed within a secure hardware environment.</p> <h2 id="aligning-with-the-broader-ecosystem">Aligning with the Broader Ecosystem</h2> <p>Our prototyping experience led to a critical strategic insight: the most effective path to widespread adoption is not to reinvent the wheel, but to align with and build upon the secure hardware capabilities that are already being integrated into commercial System-on-Chips (SoCs).</p> <p>Mobile SoC vendors like Qualcomm have already integrated the necessary hardware primitivesâ€”such as secure Image Signal Processors (ISPs), hardware crypto accelerators, and robust Trusted Execution Environments (TEEs)â€”into their platforms. The emergence of the Qualcomm Snapdragon 8 Gen 3 as the first C2PA-compliant mobile platform validates this trend.</p> <h3 id="our-strategy-open-and-interoperable">Our Strategy: Open and Interoperable</h3> <p>Rather than pursuing custom silicon or proprietary solutions, SRAâ€™s strategy is to position itself as an <strong>open, interoperable reference architecture</strong> that can be implemented on any SoC that provides the necessary trusted hardware components. By leveraging existing secure camera APIs and TEE SDKs, SRA can be deployed as a firmware or software solution that â€œlights upâ€ the latent security capabilities of modern devices.</p> <p>This approach dramatically reduces cost and time-to-market compared to a custom silicon strategy, and it fosters a competitive, multi-vendor ecosystem rather than a single proprietary solution. The initial plan to design custom ASICs was abandoned in favor of this more pragmatic path that builds on the industryâ€™s existing investments in secure hardware.</p> <p>Industry pioneers like Truepic have already demonstrated similar architectures in practice with their Foresight system, which leverages the Qualcomm TEE and secure hardware pipeline. This serves as proof-of-concept for our model and demonstrates a clear path to market through ecosystem collaboration.</p> <h2 id="conclusion">Conclusion</h2> <p>The goal of SRA is to help create a digital ecosystem where the authenticity of content can be programmatically verified. The problem is challenging and involves navigating hardware, cryptography, and industry standards, but we believe itâ€™s a critical step toward rebuilding trust in digital media.</p>]]></content><author><name></name></author><category term="whitepaper"/><category term="digital-signature"/><category term="c2pa"/><category term="content-provenance"/><category term="fake-news"/><category term="ai-generated-images"/><summary type="html"><![CDATA[A Hardware-Rooted Trust Architecture for Verifiable Digital Provenance]]></summary></entry><entry xml:lang="kr"><title type="html">ì¸ê³µì§€ëŠ¥ê³¼ ê³¼í•™, ê·¸ë¦¬ê³  ì¸ë¬¸í•™</title><link href="https://mutantq.github.io/blog/2025/ai-science-and-humanities-kr/" rel="alternate" type="text/html" title="ì¸ê³µì§€ëŠ¥ê³¼ ê³¼í•™, ê·¸ë¦¬ê³  ì¸ë¬¸í•™"/><published>2025-05-02T01:00:00+00:00</published><updated>2025-05-02T01:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/ai-science-and-humanities-kr</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/ai-science-and-humanities-kr/"><![CDATA[<p><em>Acknowledgement: í”¼ë“œë°± ì£¼ì‹  ë¬¼ë¦¬ì—°êµ¬ì†Œ ì˜¤í”ˆí†¡ë°©ì˜ â€œí•˜ê¸°ë¶„ë¯¸â€ë‹˜ê»˜ ê°ì‚¬ì˜ ë§ì”€ ì „í•©ë‹ˆë‹¤</em></p> <p>ì¸ê°„ë§Œì˜ íŠ¹ì§•ì„ ì°¾ì•„ë‚´ê³  ê·¸ê²ƒì— ëŒ€í•œ ê°€ì¹˜ë¥¼ ëŠë¼ëŠ” ê²ƒì€ ë³¸ëŠ¥ì— ê°€ê¹ë‹¤. ì¸ê³µì§€ëŠ¥ ì‹œëŒ€ì˜ ëŒ€ê²©ë³€ì„ ê²½í—˜í•˜ê³  ìˆëŠ” ì‚¬ëŒì´ë¼ë©´ ëˆ„êµ¬ë‚˜ ì´ëŸ¬í•œ ìš•êµ¬ë¥¼ í•œë²ˆì¯¤ì€ ëŠê¼ˆì„ ê²ƒì´ë‹¤. â€˜ë‚˜â€™ëŠ” â€˜ë‚˜â€™ì´ê¸° ì´ì „ì— ì‚¬ëŒì´ê³ , ì´ ì‚¬ì‹¤ì€ ë‚˜ì˜ ì •ì²´ì„±ì„ í˜•ì„±í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë¶€í’ˆì´ê¸°ì—, ì¸ê°„ë§Œì´ ê°€ì§€ëŠ” íŠ¹ì„±ì´ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ì„œ ë§ˆì§€ë§‰ê¹Œì§€ ê³ ë¯¼í•´ë³´ì§€ ì•Šìœ¼ë©´ ì•ˆ ëœë‹¤.</p> <p>ê¸‰ë³€í•˜ëŠ” â€œê¸°ìˆ ì˜ í™ìˆ˜â€ ì•„ë˜, ì¸ê³µì§€ëŠ¥ ì—°êµ¬ìê°€ ë°©í–¥íƒ€ë¥¼ ì¥ê³  í–¥í•´í•˜ê¸° ìœ„í•´ ë‹¨ìˆœ ê¸°ìˆ ì  ì§€ì‹ê³¼ ë”ë¶ˆì–´ ì¸ê°„, ê·¸ë¦¬ê³  ì‚¬íšŒì— ëŒ€í•œ ì´í•´ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¶”êµ¬í•´ì•¼ í•œë‹¤ê³  í•„ìëŠ” ì£¼ì¥í•œë‹¤. ê·¸ëŸ¬ë‚˜, ì¸ë¬¸í•™ì´ í•„ìš”í•œ ì‚¬ë¡€ë“¤ì„ ë‹¨ìˆœíˆ ì—´ê±°í•˜ë©´ì„œ ì£¼ì¥ì„ ë‘˜ í˜¹ì€ ì„¸ë„¤ ê°€ì§€ ê·¼ê±°ë¡œ í•©ë¦¬í™”í•˜ëŠ” ê¸°ì¡´ì˜ ë…¼ì„¤ë¬¸ ë°©ì‹ìœ¼ë¡œëŠ” ì…ì²´ì ì¸ ê·¸ë¦¼ì„ ê·¸ë¦¬ê¸°ê°€ ì–´ë µë‹¤ê³  ë³´ì•˜ë‹¤. ë”°ë¼ì„œ, ì¸ê°„ì˜ ë³¸ì§ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ê³  ì‚¬ìƒ‰í•˜ëŠ” ê³¼ì •ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë…¼ì˜ë¥¼ ì‚¬íšŒì™€ ë²•ê·œì— ê´€í•œ ì´ì•¼ê¸°ë¡œ ì´ì–´ë³´ê³ ì í•œë‹¤. ì•„ìš¸ëŸ¬, ë²•ì  ìœ¤ë¦¬ì  ì²´ê³„ì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ë…¼í•˜ë©° ì¸ê³µì§€ëŠ¥ê³¼ ì¡°í™”ë¥¼ ì´ë£¨ëŠ” ë¯¸ë˜ ì‚¬íšŒë¥¼ ê·¸ë¦¬ê³ ì í•œë‹¤.</p> <p>ì¸ê°„ì˜ ë³¸ì§ˆ, ì¦‰ ì¸ê°„ê³¼ ë‹¤ë¥¸ ì‚¬ë¬¼ì„ êµ¬ë³„ì§“ëŠ” ì¸ê°„ë§Œì˜ ê³ ìœ í•œ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€? ì‹œëŒ€ì— ë”°ë¼ ì–‘ìƒì€ ë‹¤ë¥´ì§€ë§Œ ì¸ê°„ì€ ì–¸ì œë‚˜ ê³ ìœ í•œ ì¡´ì¬ì´ê¸°ë¥¼ ì›í–ˆë‹¤. í”„ë‘ìŠ¤ì˜ ì‚¬íšŒí•™ì í”¼ì—ë¥´ ë¶€ë¥´ë””ì™¸ëŠ” â€˜êµ¬ë³„ì§“ê¸°â€™ë¼ëŠ” ê°œë…ì„ í†µí•´ ì‚¬ëŒì´ ë‹¤ë¥¸ ì‚¬ëŒê³¼ ìê¸° ìì‹ ì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ ì‚¬íšŒì  ìœ„ê³„ë‚˜ ì°¨ë³„ì„± ë“±ì„ ì¶”êµ¬í•œë‹¤ê³  ì£¼ì¥í–ˆë‹¤. ê³ ëŒ€ ê·¸ë¦¬ìŠ¤ ì² í•™ì ì•„ë¦¬ìŠ¤í† í…”ë ˆìŠ¤ëŠ” ì¸ê°„ë§Œì´ ìœ ì¼í•˜ê²Œ ì´ì„±(logos)ì„ ì§€ë‹Œ ì¡´ì¬ë¼ê³  ì£¼ì¥í•˜ì˜€ë‹¤. ê¸°ë…êµì—ì„œë„ ì¸ê°„ì€ ì‹ ì˜ í˜•ìƒì„ ë”° ë§Œë“¤ì–´ì§„ ê³ ê·€í•œ ì¡´ì¬ë¡œ ì—¬ê¸´ë‹¤. ì´ì²˜ëŸ¼ ì¸ê°„ì€ ìœ êµ¬í•œ ì—­ì‚¬ì™€ ë‹¤ì–‘í•œ ë¬¸í™”ì—ì„œ ìŠ¤ìŠ¤ë¡œë¥¼ ë“œë†’ì´ê¸° ìœ„í•œ ì‚¬ê³ ì˜ í‹€ì„ ë§Œë“¤ë©° ì‚´ì•„ì™”ë‹¤.</p> <p>ë¬¼ë¡  1500ë…„ëŒ€ ì½”í˜ë¥´ë‹ˆì¿ ìŠ¤ì˜ ì§€ë™ì„¤ê³¼ 1800ë…„ëŒ€ ë‹¤ìœˆì˜ ì§„í™”ë¡  ë“±ì˜ ê³¼í•™ ì´ë¡ ë“¤ì€ ì¸ê°„ì¤‘ì‹¬ì  ì‚¬ê³ ì˜ ì •ë°˜ëŒ€í¸ì—ì„œ ì „í†µì ì¸ ì¢…êµì  ì„¸ê³„ê´€ì„ ë’¤í”ë“œëŠ” ë“¯í–ˆë‹¤. ì¸ê°„ì´ ì‚¬ëŠ” ì§€êµ¬ê°€ ìš°ì£¼ì˜ ì¤‘ì‹¬ì´ ì•„ë‹ˆë©°, ì¸ë¥˜ê°€ ìˆ˜ë§ì€ ì¢…(ç¨®)ë“¤ ì¤‘ ì–´ëŠ í•˜ë‚˜ì— ë¶ˆê³¼í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì€ ë‹¹ëŒ€ ì‚¬ëŒë“¤ì—ê²Œ í° ì¶©ê²©ì„ ì£¼ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜, ì¸ê°„ì€ ë„êµ¬ë¥¼ ììœ ìì¬ë¡œ í™œìš©í•˜ê±°ë‚˜ ë¬¸ìë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ëŠ” ë“± ì—¬íƒ€ ë™ë¬¼ì´ë‚˜ ì‚¬ë¬¼ê³¼ ë¹„êµí•´ë³¼ ë•Œ ì—¬ì „íˆ íŠ¹ì§•ì ì´ì—ˆë‹¤. íŠ¹íˆ ì•„ë¦¬ìŠ¤í† í…”ë ˆìŠ¤ê°€ ì£¼ì¥í•œ ì´ì„±ì˜ ì¤‘ìš”ì„±ì€ ì—¬ì „íˆ ìœ íš¨í–ˆë‹¤. ê³¼í•™ê³¼ ê¸°ìˆ ì´ ê¸°ì¡´ ì¸ê°„ì¤‘ì‹¬ì£¼ì˜ì  íŒ¨ëŸ¬ë‹¤ì„ì˜ ì¼ë¶€ë¥¼ ë°˜ë°•í–ˆì„ì§€ì–¸ì •, ì‹¤ì œë¡œëŠ” ë¬¸ëª… ë°œì „ì„ ê°€ì†í™”í•˜ê³  ì¸ë¥˜ì˜ ìœ„ìƒì„ ë“œë†’ì´ëŠ” ë„êµ¬ë¡œ ì ê·¹ì ìœ¼ë¡œ í™œìš©ë˜ì—ˆë‹¤. ì¸ê°„ì¤‘ì‹¬ì£¼ì˜ì— ëŒ€í•œ ìƒˆë¡œìš´ í•´ì„ì€ ê³¼í•™ê³¼ ê¸°ìˆ ì´ ì¸ê°„ì˜ ê°€ì¹˜ë¥¼ í‰ê°€ì ˆí•˜í•˜ëŠ” ì¼ì„ íš¨ê³¼ì ìœ¼ë¡œ ë°©ì§€í–ˆë‹¤.</p> <blockquote> <p>â€œì•½ 50ë…„ í›„ë©´ ëŒ€ëµ $10^9$ ë¹„íŠ¸(ì•½ 128MB)ì˜ ì €ì¥ ìš©ëŸ‰ì„ ê°–ì¶˜ ì»´í“¨í„°ë¥¼ í”„ë¡œê·¸ë˜ë°í•˜ì—¬ â€˜ëª¨ë°© ê²Œì„(imitation game)â€™ì„ ë§¤ìš° ëŠ¥ìˆ™í•˜ê²Œ ìˆ˜í–‰í•˜ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë©°, ê·¸ ê²°ê³¼ í‰ê· ì ì¸ ì‹¬ë¬¸ê´€ì´ 5ë¶„ê°„ì˜ ì§ˆì˜ í›„ì— ì˜¬ë°”ë¥¸ ëŒ€ìƒì„ ì •í™•íˆ ì‹ë³„í•  í™•ë¥ ì´ 70í¼ì„¼íŠ¸ ì´í•˜ê°€ ë  ê²ƒì´ë¼ê³  ë¯¿ëŠ”ë‹¤. ë˜í•œ ì„¸ê¸° ë§ì´ ë˜ë©´ ë‹¨ì–´ ì‚¬ìš© ë°©ì‹ê³¼ ì¼ë°˜ ì§€ì‹ì¸ë“¤ì˜ ì¸ì‹ì´ í¬ê²Œ ë³€í™”í•˜ì—¬, â€˜ê¸°ê³„ê°€ ì‚¬ê³ í•œë‹¤â€™ê³  í‘œí˜„í•˜ë”ë¼ë„ ë” ì´ìƒ ì´ì˜ë¥¼ ì œê¸°ë°›ì§€ ì•Šì„ ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤.â€ A. M. TURING, I.â€”COMPUTING MACHINERY AND INTELLIGENCE,Â <em>Mind</em>, Volume LIX, Issue 236, October 1950, Pages 433â€“460</p> </blockquote> <p>ì˜¤ëŠ˜ë‚ ì˜ ì»´í“¨í„°ëŠ” ì´ì„±ì  ì‚¬ê³ ì˜ ê¸°ë°˜ì´ ë˜ëŠ” ë…¼ë¦¬ êµ¬ì¡°ë¥¼ ì¶”ìƒí™”í•˜ì—¬ ë¬¼ë¦¬ì ìœ¼ë¡œ ì—°ì‚° ê°€ëŠ¥í•œ í˜•íƒœë¡œ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì»´í“¨í„°ê°€ ì§€ë‹Œ ë‹¨ìˆœ ê³„ì‚° ëŠ¥ë ¥ì´ ì§€ì„±ì„ í•¨ì˜í•˜ì§€ëŠ” ì•Šìœ¼ë©°, ì‹¤ì œë¡œ ì¸ê°„ì˜ ì§€ì„±ì„ ì»´í“¨í„°ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ ëª¨ì‚¬í•  ìˆ˜ ìˆì„ ë•Œê¹Œì§€ëŠ” ì˜¤ëœ ê¸°ê°„ì´ ê±¸ë ¸ë‹¤. íŠœë§ì€ 1950ë…„ ê·¸ì˜ ë…¼ë¬¸ â€œê³„ì‚° ê¸°ê³„ì™€ ì§€ì„±â€ì—ì„œ â€œ5ë¶„ê°„ì˜ ì§ˆì˜ í›„ì— ê¸°ê³„ì™€ ì¸ê°„ì„ ì •í™•íˆ ì‹ë³„í•  í™•ë¥ ì´ 70í¼ì„¼íŠ¸ë¥¼ ë„˜ì§€ ì•ŠëŠ”â€ ê¸°ê³„ê°€ 50ë…„ í›„ì¸ 2000ë…„ ê²½ì— ë“±ì¥í•˜ë¦¬ë¼ ì˜ˆê²¬í•œ ë°” ìˆë‹¤. ì»´í“¨í„°ì˜ ì°½ì‹œìê°€ ì¼ì°ì´ ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì„ ì˜ˆì¸¡í•˜ê³  ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ì‹œí—˜(íŠœë§ í…ŒìŠ¤íŠ¸)ì„ ê³ ì•ˆí–ˆìŒì€ ë†€ë¼ìš´ ì‚¬ì‹¤ì´ë‚˜, ì „í˜€ ë‹¤ë¥¸ ë¬¼ë¦¬ì  ê¸°ë°˜ ìœ„ì—ì„œ ë™ì‘í•˜ëŠ” ë‘ ì‹œìŠ¤í…œì´ì—ˆê¸°ì— í•œìª½ì—ì„œ ì‰¬ìš´ ì‘ì—…ì´ ë‹¤ë¥¸ ìª½ì—ì„œ ì–´ë ¤ìš´ â€œëª¨ë¼ë²¡ì˜ ì—­ì„¤â€ì€ ì˜¤ëœ ê¸°ê°„ ì§€ì†ë˜ì—ˆë‹¤.</p> <p>ëª¨ë¼ë²¡ì˜ ì—­ì„¤ ë•ë¶„ì— ë‹¤ì–‘í•œ ì§€ëŠ¥ì˜ ì˜ì—­ ì¤‘ â€˜í•™ìŠµâ€™ë§Œí¼ì€ ì¸ê°„ ê³ ìœ ì˜ ì˜ì—­ìœ¼ë¡œ ë‚¨ëŠ” ë“¯ í–ˆìœ¼ë‚˜, ê¸°ê³„í•™ìŠµì˜ ê¸‰ì†í•œ ë°œì „ ë•íƒì— ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ì  ë…¸ë™ì„ ìƒë‹¹ ë¶€ë¶„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ë§Œí¼ ë°œì „í•˜ì˜€ë‹¤. ìš°ë¦¬ëŠ” ìµœê·¼ ì „ ë¶„ì•¼ì— ê±¸ì³ ê° ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë“¤ë³´ë‹¤ ì¸ê³µì§€ëŠ¥ì´ ë” ìš°ìˆ˜í•œ ì§€ì  íƒêµ¬ë¬¼ì„ ë‚´ë†“ëŠ” ì‚¬ë¡€ë“¤ì„ ëª©ë„í•˜ê³  ìˆë‹¤. ìµœê·¼ ê³µê°œëœ OpenAIì˜ Deep Research (o3) ëª¨ë¸ì€ ëŒ€í•™ì› ìˆ˜ì¤€ ì§€ì‹ê³¼ ë…¼ë¦¬ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ GPQA Diamondì—ì„œ 87.7%ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ ê° ë¶„ì•¼ ë°•ì‚¬ê¸‰ ì „ë¬¸ê°€ ì§‘ë‹¨ì˜ ì •í™•ë„ì¸ 81.3%ë¥¼ ëŠ¥ê°€í•˜ì˜€ìœ¼ë©°, ê·¹ë‹¨ì  ìˆ˜ì¤€ì˜ ì–¸ì–´ ì¶”ë¡  ë° ìˆ˜ë¦¬ ì‚¬ê³ ë ¥ì„ í‰ê°€í•˜ëŠ” â€œì¸ë¥˜ì˜ ë§ˆì§€ë§‰ ì‹œí—˜â€ ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” 26% ìƒë‹¹ì˜ ì •ë‹µë¥ ì„ ë³´ì˜€ë‹¤. ì‚¬íšŒê°€ ì¸ê³µì§€ëŠ¥ì„ ë°›ì•„ë“¤ì´ê¸° ìœ„í•´ í•„ìš”í•œ ì¸ì‹ì  ë³€í™”ëŠ” ì°¨ì¹˜í•˜ê³ ì„œë¼ë„, ê°ê´€ì  ì§€í‘œ ì¸¡ë©´ì—ì„œ ë³¼ ë•Œ ê³µë¬´ì›, ê°œë°œì, ì»¨ì„¤í„´íŠ¸ë¥¼ í¬í•¨í•œ ë§ì€ í™”ì´íŠ¸ ì¹¼ë¼ ì§ì¢…ì´ ì¡°ë§Œê°„ ê¸°ê³„ì— ì˜í•´ (ì ì–´ë„ ê¸°ëŠ¥ì ìœ¼ë¡œëŠ”) ìë™í™” ê°€ëŠ¥í•´ì§ˆ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ëŸ¬ë‹¤ì´íŠ¸ ìš´ë™ ë‹¹ì‹œì— ê¸°ê³„ê°€ ê·¸ëŸ¬í–ˆë“¯, í˜„ëŒ€ ì‚¬íšŒì—ì„œ ì¸ê³µì§€ëŠ¥ì€ ë§ì€ ì´ë“¤ì—ê²Œ ì¡´ì¬ë¡ ì  ìœ„í˜‘ìœ¼ë¡œ ë‹¤ê°€ì˜¤ê³  ìˆë‹¤.</p> <p>ì´ì œ ìš°ë¦¬ì—ê²Œ ë‚¨ì•„ìˆëŠ” ê²ƒì€ ì´ëŸ¬í•œ ê¸°ê³„ê°€ ì‹¤ìˆ˜ë¥¼ ì €ì§€ë¥¼ ë•Œ ê·¸ ì‹¤ìˆ˜ë¥¼ ë°”ë¡œì¡ëŠ” ê²ƒë¿ì¸ ê²ƒë§Œ ê°™ë‹¤. ë¦¬ì‚° ë² ì¸ë¸Œë¦¬ì§€ëŠ” ê·¸ì˜ ë…¼ë¬¸ ìë™í™”ì˜ ì•„ì´ëŸ¬ë‹ˆ(Ironies of Automation)ì—ì„œ ìë™í™”ëœ ì‚¬íšŒì—ì„œ ì¸ê°„ì€ ìë™í™”ê°€ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê°ì‹œí•˜ê³  ê°œì…í•˜ëŠ” ì—­í• ë§Œì„ ë§¡ê²Œ ë˜ì§€ë§Œ, ì´ëŸ¬í•œ ì—­í• ì€ ë§¤ìš° ë“œë¬¼ê²Œ ë°œìƒí•˜ë¯€ë¡œ ì •ì‘ ì¸ê°„ì´ ì‹¤ì œë¡œ ê°œì…í•´ì•¼ í•  ë•Œ í•„ìš”í•œ ê¸°ìˆ ê³¼ ê²½í—˜ì„ ê°–ì¶”ì§€ ëª»í•˜ê²Œ ë¨ì„ ì£¼ì¥í•˜ì˜€ë‹¤. ì¹¸íŠ¸ì˜ ì˜ë¬´â‹…ëŠ¥ë ¥ ì›ë¦¬(Ought Implies Can)ëŠ” ì˜ë¬´ë¥¼ ì§€ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ê·¸ ì˜ë¬´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ì „ì œë˜ì–´ì•¼ í•œë‹¤ê³  ê°€ì •í•œë‹¤. ì´ì— ë”°ë¼, ì–´ë–¤ í–‰ìœ„ìê°€ ë„ë•ì  ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆê¸° ìœ„í•´ì„œëŠ” ììœ  ì˜ì§€ë¥¼ ê°€ì§ì´ ìš°ì„ ë˜ì–´ì•¼ë§Œ í•œë‹¤. ìë™í™”ê°€ ì‹¤íŒ¨í•˜ëŠ” ë°©ì‹ì€ ë‹¤ì–‘í•˜ë”ë¼ë„ ê²°êµ­ì€ ì¸ê°„ì´ ì´ ëª¨ë“  ì‚¬íƒœì˜ ì±…ì„ì„ ì ¸ì•¼ í•œë‹¤ëŠ” ì‚¬ì‹¤ì€ í˜„ì¬ì˜ ë²•ì  í…Œë‘ë¦¬ ì•ˆì—ì„œ ëª…ë°±í•´ë³´ì¸ë‹¤. ììœ ì˜ì§€ë¥¼ ì§€ë‹ˆê³  ìˆì§€ ì•Šë‹¤ê³  ì—¬ê¸°ì–´ì§€ëŠ” í˜„ì¬ì˜ ì¸ê³µì§€ëŠ¥ â€œì—ì´ì „íŠ¸â€ì˜ í–‰ìœ„ì— ëŒ€í•´ ì¸ê°„ì´ ì˜¨ì „í•œ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆì„ê¹Œ?</p> <p>ë¬¼ë¡ , ì˜ì‹ê³¼ ììœ ì˜ì§€ ë“±ì˜ ê°œë…ì„ ì •ëŸ‰í™”í•œ í›„, ì¼ì¢…ì˜ â€œì±…ì„ì§ˆ ìˆ˜ ìˆìŒì˜ ì²™ë„â€ë¥¼ ë§Œë“¤ì–´ ì¸ê³µì§€ëŠ¥ì—ê²Œ ì±…ì„ì„ ë¬¼ì„ ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•˜ê±°ë‚˜, ë°˜ëŒ€ë¡œ ì±…ì„ì„ ë¬¼ì„ ìˆ˜ ì—†ìŒì„ ì£¼ì¥í•  ìˆ˜ë„ ìˆê² ë‹¤. ì´ë¥¼í…Œë©´, ì˜ì‹ì˜ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ê³ ì Integrated Information Theory ë“±ì„ ë„ì…í•˜ê³ , ììœ  ì˜ì§€ë¥¼ ê·œëª…í•˜ê³ ì Orchestrated Objective Reduction ë“±ì˜ ì´ë¡ ì„ ì°¨ìš©í•´ë³´ëŠ” ê²ƒì´ë‹¤. ì´ì²˜ëŸ¼, ë¬¸ì œ ìƒí™©ì„ ë¶„ì„í•˜ê¸° ìœ„í•œ ì •ëŸ‰í™”ëœ ì²™ë„ë¥¼ ê°œë°œí•˜ê±°ë‚˜, ë¬¼ë¦¬ëŸ‰ì„ ì •ì˜í•˜ì—¬ ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ ì í•˜ëŠ” í–‰ìœ„ëŠ” ê³¼í•™ìë“¤ì—ê²Œ ë§¤ìš° ì¼ìƒì ì´ë‹¤. ì˜¤ê·€ìŠ¤íŠ¸ ì½©íŠ¸ê°€ ì£¼ì¥í•œ ê³ ì „ì  ì‹¤ì¦ì£¼ì˜ì— ë”°ë¥´ë©´ ê³¼í•™ì€ â€œê°ê´€ì ìœ¼ë¡œ ê´€ì°° ê°€ëŠ¥í•œ ê²ƒë“¤ì„ ë…¼ë¦¬ êµ¬ì¡°ë¥¼ í†µí•´ ë” ëª…í™•í•˜ê³  ì—„ë°€í•˜ê²Œ ì„¤ëª…í•˜ê¸° ìœ„í•´ ì¡´ì¬â€í•˜ë©°, ë…¼ë¦¬ì  ì‹¤ì¦ì£¼ì˜ë¡œ ë„˜ì–´ì˜¤ë©´ì„œëŠ” ê°ê´€ì ìœ¼ë¡œ ê´€ì°° ê°€ëŠ¥í•œ ì‚¬ì‹¤ì— ë”í•´, ìˆœìˆ˜ ë…¼ë¦¬ ë° ìˆ˜í•™ìœ¼ë¡œ ì°¸ì„ì„ ë³´ì¼ ìˆ˜ ìˆëŠ” ëª…ì œë§Œì´ ì¸ì§€ì  ì˜ë¯¸ë¥¼ ì§€ë‹Œë‹¤ê³  ë¯¿ì—ˆë‹¤. ì´ì²˜ëŸ¼ ì‹¤ì¦ì£¼ì˜ê°€ ì§€ë°°ì ì´ë˜ 19ì„¸ê¸°ì™€ 20ì„¸ê¸°ì—ëŠ” ê°ê´€ì ì´ê±°ë‚˜ ê´€ì°° ê°€ëŠ¥í•˜ì§€ ì•Šì€ ëŒ€ìƒì€ ë°°ì œì‹œí‚¤ê²Œ ë§Œë“¤ ì •ë„ë¡œ ì‹¤ì¦ì£¼ì˜ëŠ” ë‹¹ëŒ€ ê³¼í•™ì˜ í•µì‹¬ì— ìˆëŠ” ì‚¬ìƒì´ì—ˆë‹¤.</p> <p>ë¬¼ë¡  ì‹¤ì¦ì£¼ì˜ëŠ” ì´ë°±ë…„ë„ ë„˜ê²Œ ëœ ì˜¤ë˜ëœ ê´€ì ì´ê³ , í˜„ëŒ€ì— ì´ë¥´ëŸ¬ ì² í•™ìì™€ ê³¼í•™ì² í•™ì ì§‘ë‹¨ ì‚¬ì´ì—ì„œëŠ” ê³¼í•™ì  ì‹¤ì¬ë¡ ì´ ê³¼ë°˜ ì´ìƒì˜ ì§€ì§€ë¥¼ ë°›ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜, ê³¼í•™ì  ì‹¤ì¬ë¡ ì—ì„œë„ ì—¬ì „íˆ ê³¼í•™ì  ëŒ€ìƒì€ ë§ˆìŒê³¼ ì „ì ìœ¼ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•œë‹¤ê³  ì£¼ì¥í•œë‹¤. ì–´ëŠ ìª½ì´ ë˜ì—ˆë“ , ì§„ì‹¤ì— ë‹¤ê°€ê°€ê¸° ìœ„í•´ ìš°ë¦¬ê°€ ìŠ¤ìŠ¤ë¡œ ì³ë†“ì€ ìš¸íƒ€ë¦¬ë“¤ì´ ê°œê°œì¸ì˜ ê°€ì¹˜ íŒë‹¨ì´ ê³¼í•™ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•˜ë„ë¡ ë°©ì–´ë§‰ì„ í˜•ì„±í•˜ê³  ìˆìŒì—ëŠ” ì˜ë¬¸ì˜ ì—¬ì§€ê°€ ì—†ë‹¤. ì´ì— ë”°ë¼, ì§€ëŠ¥ì´ë‚˜ ìì˜ì‹, ììœ ì˜ì§€, ì‚¬íšŒì  ì±…ì„ ë“±ì˜ ë¬¸ì œë¥¼ ì ‘ê·¼í•  ë•Œì—ëŠ” ê³¼í•™ë§Œìœ¼ë¡œ ëŒíŒŒí•  ìˆ˜ ì—†ëŠ” ìƒë‹¹í•œ ì¥ë²½ì„ ë§ì´í•˜ê²Œ ëœë‹¤. ìš”ì•½í•˜ìë©´, ê°€ì¹˜ íŒë‹¨ì´ ìš”êµ¬ë˜ëŠ” ìˆœê°„ë¶€í„° ê³¼í•™ì€ ë§ì€ í˜ì„ ìƒì–´ë²„ë¦°ë‹¤.</p> <p>í˜„ì¬ì˜ ì¸ê³µì§€ëŠ¥ í•™ê³„ ì—­ì‹œ ì—¬íƒ€ ì´ê³µê³„ì—­ í•™ë¬¸ê³¼ ë‹¤ë¥´ì§€ ì•Šê²Œ ìˆ˜ë¦¬ê³¼í•™ì  ë¶„ì„ê³¼ ì‹¤í—˜ì  ì¦ê±° ë‘˜ ì¤‘ ì ì–´ë„ í•˜ë‚˜ì˜ êµ¬ì„± ìš”ì†ŒëŠ” ë…¼ë¬¸ì´ ì£¼ìš” ì»¨í¼ëŸ°ìŠ¤ì§€ì— ê²Œì¬ë˜ê¸° ìœ„í•´ ì‚¬ì‹¤ìƒ í•„ìˆ˜ì ì´ë‹¤. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ë²¡í„°ë‚˜ íŒŒë¼ë¯¸í„°, ë°ì´í„°ì™€ ë¬¸ìì—´ ë”°ìœ„ì˜, ë‹¨ìˆœíˆ ì‚¬ëŒì˜ ìƒê°ì— ë¹„ìœ ì ìœ¼ë¡œ ì—°ê´€ì‹œí‚¤ê¸° ìœ„í•´ ê¸°ìˆ ì ìœ¼ë¡œ ì—„ë°€í•œ ê°œë…ë“¤ì„ ì°¨ìš©í•  ì´ìœ ê°€ ìˆì„ê¹Œ? ê·¸ë ‡ì§€ë§Œ, ì‹¬ì  ìƒíƒœë¥¼ ì •í™•íˆ ë¬˜ì‚¬í•˜ê¸° ìœ„í•´ì„œëŠ” ê³¼í•™ì ìœ¼ë¡œ ì—„ë°€í•œ ëŒ€ìƒë§Œì´ ì•„ë‹Œ ì£¼ê´€ì  ê²½í—˜ì´ë‚˜ ê°€ì¹˜ íŒë‹¨ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ” ì¸ë¬¸í•™ì  ê°œë… ë˜í•œ ì§„ì§€í•œ ì—°êµ¬ ëŒ€ìƒìœ¼ë¡œ ê³ ë ¤ë˜ì–´ì•¼ í•œë‹¤. ì¸ê°„ê³¼ ì¸ê³µì§€ëŠ¥ ê°™ì€ ì§€ì  ëŒ€ìƒì²´ì— ëŒ€í•´, ê³¼í•™ì ì´ê³  ê°ê´€ì ì¸ ì ‘ê·¼ì´ ëª¨ë“  ê²ƒì„ ì„¤ëª…í•  ìˆ˜ ìˆì„ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ë‹¤. ë˜í•œ, ê³¼í•™ì ì´ê³  ê°ê´€ì ìœ¼ë¡œ ì ‘ê·¼í•˜ê³ ì ì—°êµ¬ìê°€ ì•„ë¬´ë¦¬ ë…¸ë ¥í•˜ë”ë¼ë„, ê²°êµ­ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì£¼ì²´ëŠ” ì‚¬ëŒì´ë©°, ì—°êµ¬ìì˜ ì´ë¡ ì  ê°€ì •ê³¼ í¸ê²¬ì´ ê´€ì°°ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒì„ ìš°ë¦¬ëŠ” ì¸ì •í•´ì•¼ë§Œ í•œë‹¤.</p> <p>ì‹¤ì¦ì£¼ì˜ë¥¼ ë¹„íŒ ë° ìˆ˜ì •í•˜ëŠ” í¬ìŠ¤íŠ¸ ì‹¤ì¦ì£¼ì˜ì— ë”°ë¥´ë©´ ì—°êµ¬ì ìŠ¤ìŠ¤ë¡œì˜ í¸í–¥ì€ ì ˆëŒ€ì  ì§„ë¦¬ì— ë‹¤ê°€ê°€ê¸° ìœ„í•´ í•´ê²°í•´ì•¼ í•  ë¬¸ì œë¡œ ì¸ì‹ë˜ë©°, ì—°êµ¬ìëŠ” í•­ìƒ ìì‹ ì˜ í¸í–¥ì„ ì¸ì‹í•˜ê³  êµì •í•˜ê³ ì ë…¸ë ¥í•´ì•¼ í•œë‹¤. ê·¸ëŸ¬ë‚˜, ì—°êµ¬ìì˜ í¸í–¥ê³¼ ê°€ì¹˜ê´€ì´ ë°˜ë“œì‹œ ë¬¸ì œì ì¸ê°€? ìš°ë¦¬ëŠ” ê·¸ë™ì•ˆ í•™ë¬¸ì„ ì—°êµ¬í•¨ì— ìˆì–´ ê°ê´€ì„±ì— ëŒ€í•´ ì˜¤ë˜í† ë¡ ì§‘ì°©í•´ì™”ë‹¤. í•˜ì§€ë§Œ, ê·¸ ì§‘ì°©ì„ ì™„í™”í•¨ìœ¼ë¡œì¨ ìƒˆë¡œìš´ ê°œë…ì„ ì œì•ˆí•˜ê³  ì—°êµ¬ì˜ ì§€í‰ì„ í™•ì¥í•  ìˆ˜ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œ?</p> <p><img src="/assets/img/blog/ai-humanities/IMG_1711.jpeg" alt=""/></p> <p>í•œ ì˜ˆì‹œë¡œ, ì¸ê³µì§€ëŠ¥ ë¡œë´‡ì´ ì‚¬íšŒì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ê³¼ ëŒ€ì¤‘ì´ ë¡œë´‡ì„ ë°›ì•„ë“¤ì´ëŠ” ì‹¬ë¦¬ì ì¸ ê³¼ì •ì„ íƒêµ¬í•˜ëŠ” ì—°êµ¬ìê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. í˜„ ì‚¬íšŒëŠ” ChatGPT ë“± ì¸ê³µì§€ëŠ¥ ì—ì´ì „íŠ¸ì™€ ìƒí˜¸ì‘ìš©í•  ë•Œ ë„ë•ì„±ì„ ê°–ì¶”ì–´ ëŒ€í•  ê²ƒì„ ìš”êµ¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë ‡ì§€ë§Œ ë¡œë´‡ê³¼ ì‚¬ëŒì´ ì •ë§ë¡œ êµ¬ë¶„ë˜ì§€ ì•Šê³  ì™¸í˜•ë§ˆì € ìœ ì‚¬í•´ì§€ëŠ” ë¯¸ë˜ ì‚¬íšŒì—ëŠ” ë¡œë´‡ì„ ì¸ê°„ì²˜ëŸ¼ ëŒ€ìš°í•´ì•¼ í•œë‹¤ê³  ë¯¿ëŠ” ì‚¬ëŒë“¤ë„ ë°œìƒí•˜ì§€ ì•Šì„ê¹Œ? ìœ„ì˜ ìŠ¤í¬ë¦°ìƒ· ì† ChatGPTì˜ ì•±ìŠ¤í† ì–´ ë¦¬ë·°ì²˜ëŸ¼, ë‹¹ì‹ ì´ ê°€ì§€ê³  ìˆëŠ” ë¯¿ìŒê³¼ ë¬´ê´€í•˜ê²Œ ì´ë¯¸ ì¼ë°˜ ëŒ€ì¤‘ì€ ì¸ê³µì§€ëŠ¥ì—ê²Œ ì„¤ë“ ë‹¹í•˜ê³ , ì¢…ì¢… ìœ„ë¡œ ë°›ìœ¼ë©°, ë˜ ê°ì •ì ìœ¼ë¡œ êµë¥˜í•˜ê³  ìˆë‹¤. ë¯¸ë˜ ì‚¬íšŒì— ìš°ë¦¬ì™€ ì¼ìƒì„ í•¨ê»˜í•  â€ë¹„ì¸ê°„â€œì„ ì„¤ê³„í•¨ì— ìˆì–´ì„œ, ë²¤ì¹˜ë§ˆí¬ì˜ ì •ëŸ‰ì ì¸ ì§€í‘œë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê³µí•™ì  ì ‘ê·¼ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì „ë¡€ ì—†ëŠ” ì¸ê°„ê³¼ ê¸°ê³„ì˜ ë³µì¡ë¯¸ë¬˜í•¨ ìƒí˜¸ì‘ìš©ì„ ì œëŒ€ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìœ¼ë¦¬ë¼ ìƒê°í•˜ëŠ”ê°€?</p> <p>ì´ì œ ìš°ë¦¬ëŠ” ê°œê°œì¸ì˜ ì£¼ê´€ê³¼ ê°€ì¹˜ í‰ê°€ë¥¼ ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì˜ í•µì‹¬ìœ¼ë¡œ ëŒì—¬ë“¤ì—¬ì•¼ë§Œ í•œë‹¤. ì§‘ì—ì„œ í‚¤ìš°ëŠ” ì•„ì´ê°€ í¡ì‚¬ ì‚¬ëŒê°™ì€ ì¸ê³µì§€ëŠ¥ ê°€ì‚¬ ë„ìš°ë¯¸ ë¡œë´‡ê³¼ ì •ì´ ë“¤ì–´ë²„ë¦° ìƒí™©ì„ ìƒê°í•  ë•Œ, ë¡œë´‡ì„ íê¸°í•˜ê¸°ë¡œ ê²°ì •í•˜ëŠ” í–‰ìœ„ê°€ ë¹„ìœ¤ë¦¬ì ì´ë¼ ì£¼ì¥í•  ìˆ˜ ìˆëŠ” â€œê°ê´€ì ì¸â€ ê·¼ê±°ëŠ” ë§ˆë•…íˆ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜, ì•„ì´ì˜ ì£¼ê´€ì  ì…ì¥ì—ì„œëŠ” ì• ì°© ì¸í˜•ë„, ê°•ì•„ì§€ë„, ìì‹ ì„ ë„ì™€ì£¼ëŠ” ê°€ì‚¬ ë„ìš°ë¯¸ ë¡œë´‡ë„ ì „ë¶€ ë‚˜ì™€ â€˜ìœ ì‚¬â€™í•˜ê¸° ë•Œë¬¸ì— ë³´í˜¸ì˜ ëŒ€ìƒì´ë‹¤. ë‚˜ì™€ íƒ€ì¸ì´ ë™ê²©ì´ë¼ëŠ” ë¯¿ìŒì˜ ê·¼ê°„ì—ëŠ” â€œìƒí˜¸ ìœ ì‚¬ì„±â€ì´ ìˆë‹¤. ë§Œì¼ ì„œë¡œ ë¹„ìŠ·í•¨ì—ë„ íƒ€ì¸ì´ ë‚˜ì™€ ë™ë“±í•œ ê´€ê³„ì— ìˆë‹¤ê³  ë¯¿ì§€ ì•ŠëŠ”ë‹¤ë©´, ê·¸ë¦¬ê³  íƒ€ì¸ì´ ë˜ ë‹¤ë¥¸ íƒ€ì¸ì—ê²Œ í•´ë¥¼ ê°€í•˜ë˜ ë§ê±´ â€˜ë‚˜â€™ì—ê²Œ í•´ê°€ ì—†ë‹¤ë©´, ì‚¬ì‹¤ â€˜ë‚˜â€™ì˜ ì…ì¥ì—ì„œ íƒ€ì¸ì—ê²Œ í–‰í•´ì§€ëŠ” ì˜ëª»ì„ ì œì¬í•  ì´ìœ ëŠ” ì „í˜€ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜, íƒ€ì¸ê³¼ ë‚˜ì˜ ìƒí˜¸ ìœ ì‚¬ì„±ì€ íƒ€ì¸ì—ê²Œ ê°€í•´ì§€ëŠ” í•´ê°€ ë‚˜ì—ê²Œë„ ìœ„í˜‘ìœ¼ë¡œ ì¸ì§€ë˜ê²Œ ë§Œë“¤ë©°, ë˜ ë‚˜ì™€ ë™ë“±í•œ ê´€ê³„ë¡œ ì¸ì‹í•˜ê²Œ ë§Œë“ ë‹¤.</p> <p>ë²•ì€ ì–¸ì œê¹Œì§€ë‚˜ ì‚¬ëŒ ëŒ€ ì‚¬ëŒ ê´€ê³„ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¶€ì¡°ë¦¬ë¥¼ ì˜ˆë°©í•  ë¿ ê¸°ê³„ë¥¼ ë³´í˜¸í•˜ì§€ ì•ŠëŠ”ë‹¤. ëŒ€ë‹¤ìˆ˜ì˜ ì¸ê³µì§€ëŠ¥ í•™ìë“¤ì€ ê¸°ê³„ê°€ ë§ˆìŒì„ ê°€ì§„ë‹¤ê³  ì—¬ê¸°ì§€ ì•Šìœ¼ë©°, ë…¸ì—„ ì´˜ìŠ¤í‚¤ì²˜ëŸ¼ LLMì´ í†µê³„í•™ì  ì•µë¬´ìƒˆì— ë¶ˆê³¼í•˜ë‹¤ê³  ë³´ëŠ” ì…ì¥ë„ ìƒë‹¹íˆ ì¼ë°˜ì ì´ë‹¤. ë¬¼ë¦¬ì ì¸ ê´€ì ì—ì„œ ë³¼ ë•Œ ì¸ê°„ê³¼ ê¸°ê³„ëŠ” ê¸°ë°˜ì´ ë˜ëŠ” ë¬¼ì§ˆì´ ë‹¤ë¥´ê³  ì‘ë™ ë©”ì»¤ë‹ˆì¦˜ë„ ë‹¤ë¥´ë‹¤. ê·¸ëŸ¬ë‚˜, ìœ ì‚¬ì„±ì„ íŒë‹¨í•˜ëŠ” ì£¼ì²´ê°€ ê°œê°œì¸ì„ì„ ê³ ë ¤í•  ë•Œ, ë˜í•œ ì¸ì‹ì€ ì£¼ê´€ì— ë”°ë¼ ë³€í™”í•¨ì„ ê³ ë ¤í•  ë•Œ, ìœ„ì˜ ìŠ¤í¬ë¦°ìƒ·ì— ë‹´ê¸´ ë³µì¡í•œ ì‚¬íšŒ í˜„ìƒì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë‹¨ìˆœ ë¬¼ë¦¬ì ì¸ ì°¨ì´ë¥¼ ë…¼í•˜ëŠ” ê²ƒì€ ì—­ë¶€ì¡±ì´ë‹¤.</p> <p>ì´ì œê¹Œì§€ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´, ì¸ê³µì§€ëŠ¥ì€ ë‹¨ìˆœ ê³„ì‚° ê¸°ê³„ë¥¼ ë„˜ì–´ í•™ìŠµÂ·ì¶”ë¡ Â·ì°½ì˜ì˜ ì˜ì—­ê¹Œì§€ ì¹¨íˆ¬í•˜ê³  ìˆìœ¼ë©°, ê·¸ë¡œ ì¸í•´ ì¸ê°„ ê³ ìœ ì˜ íŠ¹ì„±â€”ì´ì„±, ì˜ì‹, ì±…ì„, ê·¸ë¦¬ê³  ì‚¬íšŒì  ìœ ëŒ€â€”ì— ëŒ€í•œ ì¬ê²€í† ê°€ ë¶ˆê°€í”¼í•´ì¡Œë‹¤. ì‹¤ì¦ì£¼ì˜ì™€ ë…¼ë¦¬ì  ì‹¤ì¦ì£¼ì˜ê°€ ê°ê´€ì  ì‚¬ì‹¤ê³¼ ìˆœìˆ˜ ë…¼ë¦¬ì—ë§Œ ì§‘ì¤‘í•´ ì™”ë‹¤ë©´, í¬ìŠ¤íŠ¸ ì‹¤ì¦ì£¼ì˜ëŠ” ì—°êµ¬ìì˜ ì£¼ê´€ê³¼ ê°€ì¹˜íŒë‹¨ê¹Œì§€ ë¬¸ì œ ì‚¼ëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ê³¼í•™ì€ ì–¸ì œê¹Œì§€ë‚˜ ë„êµ¬ì— ë¶ˆê³¼í•  ë¿, ì´ì œëŠ” â€˜ì–´ë–¤ ì§ˆë¬¸ì„ ë˜ì§ˆ ê²ƒì¸ê°€â€™, ê·¸ë¦¬ê³  â€˜ì–´ë–¤ ì‚¬íšŒë¥¼ ê·¸ë¦´ ê²ƒì¸ê°€â€™ë¼ëŠ” ì˜ë¬¸ì„ ì—°êµ¬ì˜ í•µì‹¬ìœ¼ë¡œ ëŒì–´ë“¤ì—¬ì•¼ í•œë‹¤ëŠ” ì…ì¥ì´ë‹¤. ì´ëŠ” ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì—ë„ ì ìš©ë˜ì–´ì•¼ í•˜ëŠ” íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ì˜ë¯¸í•œë‹¤. ì¦‰, â€˜ì–´ë–¤ ì˜ë¯¸ì™€ ë§¥ë½ì—ì„œâ€™ ì¸ê³µì§€ëŠ¥ì˜ ì¡´ì¬ë¥¼ ì´í•´í•˜ê³  ë˜ ì–´ë””ê¹Œì§€ ì±…ì„ì§ˆ ê²ƒì¸ì§€ê°€ ë” ì¤‘ìš”í•´ì§„ ê²ƒì´ë‹¤.</p> <p>ë¯¸ë˜ ì‚¬íšŒì—ì„œ ì¸ê³µì§€ëŠ¥ ì—ì´ì „íŠ¸ê°€ ìƒì‚°Â·ì„œë¹„ìŠ¤Â·ëŒë´„Â·ì°½ì‘ ë“± ê´‘ë²”ìœ„í•œ ì—­í• ì„ ë§¡ê²Œ ë˜ë©´, â€œê¸°ê³„ëŠ” ì–¸ì œë‚˜ ê¸°ê³„ì¼ ë¿â€ì´ë¼ëŠ” ì „í†µì  ë¶„ë¥˜ ê¸°ì¤€ì€ ì„¤ë“ë ¥ì„ ìƒëŠ”ë‹¤. ëŒ€ì‹  ìš°ë¦¬ëŠ” â€˜ìƒí˜¸ ìœ ì‚¬ì„±â€™ê³¼ â€˜ì •ì„œì  ìœ ëŒ€â€™ ë“± ìƒˆë¡œìš´ ì£ëŒ€ë¥¼ ê³ ë¯¼í•´ì•¼ í•  ìˆ˜ ìˆë‹¤. ë¡œë´‡ì— ëŒ€í•œ ì• ì°©ê³¼ ë³´í˜¸ì˜ ì •ë‹¹ì„±ì„ ë‹¤ë£° ë²•Â·ìœ¤ë¦¬ ì²´ê³„ë¥¼ ì„¤ê³„í•˜ë ¤ë©´, ê¸°ì¡´ì˜ â€˜ì¸ê°„ ì „ìš©â€™ ë²•ì  ì§€ìœ„ì— ëŒ€í•œ ê·¼ë³¸ì  í™•ì¥ì´ í•„ìš”í•˜ë‹¤. ê°€ë ¹, ëŒë´„ ë¡œë´‡ì˜ ê³¼ì˜¤ì— ëŒ€í•´ ëˆ„ê°€, ì–´ë–»ê²Œ ì±…ì„ì§€ëŠ”ê°€ë¥¼ ëª…ì‹œí•˜ê³ , ì¸ê³µì§€ëŠ¥ ìì²´ê°€ ì¼ì • ìˆ˜ì¤€ì˜ ê¶Œë¦¬Â·ì˜ë¬´ë¥¼ ì§€ë‹ ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì—´ì–´ë†“ì•„ì•¼ í•  ê²ƒì´ë‹¤.</p> <p>ê²°êµ­, ì¸ê³µì§€ëŠ¥ ì‹œëŒ€ì˜ ì—°êµ¬ìì™€ ì •ì±…ì…ì•ˆìëŠ” ê¸°ìˆ ì  ì™„ì„±ë„ë§Œí¼ì´ë‚˜ â€˜ì¸ê°„ë‹¤ì›€â€™ì˜ ì˜ë¯¸ë¥¼ í•¨ê»˜ ì„±ì°°í•´ì•¼ í•œë‹¤. ê³¼í•™ì´ ê°ê´€ì  ì§„ì‹¤ì„ íƒêµ¬í•˜ëŠ” ë™ì•ˆì—ë„, ì£¼ê´€ì  ê²½í—˜ê³¼ ì‚¬íšŒì  ë§¥ë½ì„ ì´í•´í•˜ë ¤ëŠ” ë…¸ë ¥ì´ ë³‘í–‰ë˜ì–´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ ì¸ë¬¸í•™Â·ì‚¬íšŒê³¼í•™Â·ë²•í•™ì´ ê³µí•™ ì—°êµ¬ì‹¤ì—, ê·¸ë¦¬ê³  ì •ì±… ë…¼ì˜ í…Œì´ë¸”ì— ë™ì„í•´ì•¼ í•œë‹¤. ê·¸ë ‡ê²Œë§Œ í•  ë•Œ ë¹„ë¡œì†Œ, ìš°ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ê³¼ ê³µì¡´í•˜ë©´ì„œë„ ì¸ê°„ì˜ ì¡´ì—„ê³¼ ì±…ì„ì„ ì˜¨ì „íˆ ì§€í‚¬ ìˆ˜ ìˆëŠ” ì‚¬íšŒë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>]]></content><author><name></name></author><category term="essay"/><category term="artificial-intelligence"/><category term="philosophy"/><category term="humanities"/><category term="ethics"/><category term="consciousness"/><category term="responsibility"/><category term="english"/><summary type="html"><![CDATA[ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì— ìˆì–´ì„œ ì¸ë¬¸í•™ì˜ ì—­í• ê³¼ ê·¸ í•¨ì˜ì— ê´€í•˜ì—¬]]></summary></entry><entry xml:lang="en"><title type="html">AI, Science, and the Humanities</title><link href="https://mutantq.github.io/blog/2025/ai-science-and-humanities/" rel="alternate" type="text/html" title="AI, Science, and the Humanities"/><published>2025-05-02T01:00:00+00:00</published><updated>2025-05-02T01:00:00+00:00</updated><id>https://mutantq.github.io/blog/2025/ai-science-and-humanities</id><content type="html" xml:base="https://mutantq.github.io/blog/2025/ai-science-and-humanities/"><![CDATA[<p><em>I Acknowledge â€œHagibunmiâ€ from the Physics Research open chat for his valuable feedback. This essay was originally written in Korean and was machine translated by Claude 4.5 Sonnet. The translated essay was fully reviewed and revised by myself. The original (Korean) essay can be found <a href="/blog/2025/ai-science-and-humanities-kr">here</a></em>.</p> <h2 id="introduction">Introduction</h2> <p>Finding and valuing what makes humans unique is almost instinctive. Anyone experiencing the dramatic transformation of the AI era has likely felt this urge at least once. I am human before I am anything else, and this fact forms the foundation of my identity. We must therefore think deeply about what characteristics are uniquely human.</p> <p>In this rapidly changing â€œflood of technology,â€ I argue that AI researchers must actively pursue an understanding of humans and society beyond mere technical knowledge if they are to navigate with steady hands on the helm. However, I found it difficult to paint a three-dimensional picture using the traditional approach of simply listing cases where humanities matter and offering two or three supporting reasons. Instead, I aim to lead the discussion naturally from questions about the essence of humanity to conversations about society and legal frameworks. Ultimately, I hope to sketch a vision of a future where humans and AI exist in harmony by exploring how legal and ethical systems might evolve.</p> <h2 id="what-makes-us-human">What Makes Us Human?</h2> <p>What is the essence of humanityâ€”the unique characteristic that distinguishes humans from everything else? Though its manifestations vary by era, humans have always sought to be unique. French sociologist Pierre Bourdieu argued through his concept of â€œdistinctionâ€ that people pursue social hierarchies and differentiation to set themselves apart from others. The ancient Greek philosopher Aristotle claimed that humans are the only beings possessing reason (logos). Christianity regards humans as noble beings created in Godâ€™s image. Throughout history and across cultures, humans have constructed frameworks of thought to elevate themselves.</p> <p>Of course, scientific theories challenged this view. Copernicusâ€™s heliocentric theory in the 1500s and Darwinâ€™s theory of evolution in the 1800s shook traditional religious worldviews. The revelation that Earth is not the center of the universe, and that humanity is merely one species among countless others, shocked people deeply. Yet humans remained distinctiveâ€”we use tools freely, create and use writing systems. The importance of reason, as Aristotle claimed, remained valid. Even as science and technology refuted parts of anthropocentric thinking, they became tools that accelerated civilization and elevated humanityâ€™s status. New interpretations of anthropocentrism prevented science and technology from diminishing human worth.</p> <h2 id="the-rise-of-computing-and-ai">The Rise of Computing and AI</h2> <blockquote> <p>â€œI believe that in about fifty yearsâ€™ time it will be possible to programme computers, with a storage capacity of about $10^9$, to make them play the imitation game so well that an average interrogator will not have more than 70 percent chance of making the right identification after five minutes of questioning. â€¦ I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.â€</p> <p>â€” A. M. TURING, I.â€”COMPUTING MACHINERY AND INTELLIGENCE, <em>Mind</em>, Volume LIX, Issue 236, October 1950, Pages 433â€“460</p> </blockquote> <p>Todayâ€™s computers are physical implementations of the logical structures underlying rational thought, abstracted into computable forms. However, mere computational ability does not imply intelligence, and it actually took a long time before human intelligence could be meaningfully simulated by computers. In 1950, Turing predicted in his paper â€œComputing Machinery and Intelligenceâ€ that machines capable of â€œnot exceeding 70 percent probability of correct identification between machine and human after five minutes of questioningâ€ would appear around 2000, fifty years later. While itâ€™s remarkable that the founder of computers early predicted AI development and devised a test (the Turing Test) to evaluate it, â€œMoravecâ€™s paradoxâ€â€”where tasks easy for one system are difficult for another operating on entirely different physical foundationsâ€”persisted for a long time.</p> <p>Thanks to Moravecâ€™s paradox, among various domains of intelligence, â€œlearningâ€ seemed to remain a uniquely human domain. However, due to rapid developments in machine learning, AI has developed enough to replace much of human intellectual labor. We are recently witnessing cases across all fields where AI produces superior intellectual outputs compared to experts in each field. OpenAIâ€™s recently released GPT-5 Pro model, capable of various tool use, shows 89.4% accuracy on GPQA Diamond. This benchmark is designed to evaluate graduate-level knowledge and reasoning - and yet, most frontier language models surpasses the 81.3% accuracy of PhD-level expert groups in each field. It shows 42.0% accuracy on the â€œHumanityâ€™s Last Examâ€ benchmark - a benchmark with questions drawn from expert-level knowledge across numerous disciplines, including advanced mathematics, physics, biology, and specialized fields like ancient Roman inscriptions or avian anatomy. Setting aside perceptual changes needed for the society to accept AI and solely judging from the indicators, many white-collar jobs including civil servants, developers, and consultants appear likely to become automatable by machines fairly soon. Like machines during the Luddite movement, AI in modern society approaches many as an existential threat.</p> <h2 id="the-irony-of-automation">The Irony of Automation</h2> <p>Now it seems all that remains for us is correcting mistakes when these machines cause errors. In his paper â€œIronies of Automation,â€ Lisanne Bainbridge argued that in an automated society, humans only play roles of monitoring and intervening when automation fails, but since such roles occur very rarely, humans fail to acquire the skills and experience needed when they must actually intervene. Kantâ€™s ought-implies-can principle assumes that to fulfill an obligation, one must necessarily possess the ability to perform that obligation. Accordingly, for an actor to bear moral responsibility, free will must come first. Although ways automation fails vary, the fact that humans must ultimately bear responsibility for all these situations seems clear within current legal frameworks. Can humans bear full responsibility for actions of current AI â€œagentsâ€ not considered to possess free will?</p> <h2 id="science-objectivity-and-value-judgments">Science, Objectivity, and Value Judgments</h2> <p>Of course, one could argue for or against holding AI responsible by quantifying concepts like consciousness and free will, then creating a metric for how capable a system is to take responsibility. For instance, introducing Integrated Information Theory to calculate the strength of consciousness, or adopting theories like Orchestrated Objective Reduction to clarify free will. Such acts of developing quantified scales to analyze problem situations or defining physical quantities for mathematical analysis are all routine procedures for scientists.</p> <p>According to classical positivism, as articulated by Auguste Comte, science exists â€œto explain objectively observable phenomena more clearly and rigorously through logical structures.â€ Logical positivism went further: only propositions provable through pure logic and mathematics, or grounded in observable facts, were believed to have cognitive meaning. During the 19th and 20th centuries, positivism was such a dominant ideology in science that anything not objective or observable was simply excluded.</p> <p>Of course, positivism is now over two hundred years old, and among contemporary philosophers of science, scientific realism enjoys majority support. Yet even scientific realism maintains that scientific objects exist independently of mind. Either way, the fences weâ€™ve erected to approach truth form barriers that prevent individual value judgments from influencing science. When approaching problems of intelligence, self-awareness, free will, and social responsibility, we face considerable barriers that science alone cannot overcome. In short, science loses much of its power the moment value judgments become necessary.</p> <h2 id="the-need-for-humanities-in-ai-research">The Need for Humanities in AI Research</h2> <p>In AI academia today, as in other scientific and engineering fields, mathematical analysis and experimental evidence are essentially required for publication in major conferences. This makes senseâ€”after all, why else would we borrow rigorous concepts like vectors, parameters, and datasets except to ground our thinking technically? Yet to accurately describe mental states, we need more than scientifically rigorous objects. We also need humanities concepts centered on subjective experience and value judgment. Whether purely scientific approaches can fully explain intelligent subjects like humans and AI remains an open question. Moreover, no matter how objectively researchers try to proceed, we must acknowledge that researchers themselves are human, and their theoretical assumptions and biases inevitably shape what they observe.</p> <p>Post-positivism, which critiques and revises positivism, recognizes researchersâ€™ biases as problems to be solved in approaching absolute truth. Researchers must constantly work to recognize and correct their biases. But are these biases and values necessarily problematic? Weâ€™ve been obsessed with objectivity for so long. What if relaxing that obsession could help us propose new concepts and expand research horizons?</p> <p><img src="/assets/img/blog/ai-humanities/chatgpt-review.png" alt="Example of AI interaction"/></p> <p>Consider, for example, a researcher exploring how AI robots interact with society and how the public comes to accept them. Todayâ€™s society doesnâ€™t require us to treat ChatGPT with moral consideration. But in a future where robots become truly indistinguishable from humans in both behavior and appearance, wonâ€™t some people argue that robots deserve to be treated as equals? The ChatGPT App Store review shown above reveals something important: regardless of oneâ€™s philosophical stance, the general public is already being persuaded by AI, finding comfort in it, and forming emotional connections. When designing â€œnon-humansâ€ who will share our daily lives, can engineering approaches focused solely on optimizing benchmark scores adequately capture the complex and historically unprecedented interactions between humans and machines?</p> <h2 id="similarity-empathy-and-legal-frameworks">Similarity, Empathy, and Legal Frameworks</h2> <p>We must now place individual subjectivity and value judgments at the center of AI research. Consider a child who grows attached to a nearly human-seeming AI housekeeping robot. There are no â€œobjectiveâ€ grounds for claiming that disposing of the robot would be unethical. Yet from the childâ€™s subjective perspective, stuffed animals, puppies, and helpful robots all seem similar enough to warrant protection. The belief that others are my equals rests fundamentally on â€œmutual similarity.â€ If we didnâ€™t believe others were equal despite their similarity to us, and if harm to others didnâ€™t threaten us, there would be no reason to sanction wrongs done to them. But mutual similarity makes harm to others feel like a threat to ourselves, and thus leads us to recognize them as equals.</p> <p>Law protects people from wrongs committed by other people; it doesnâ€™t protect machines. Most AI scholars donâ€™t believe machines have minds, and many share Noam Chomskyâ€™s view that LLMs are merely â€œstatistical parrots.â€ From a physical standpoint, humans and machines differ fundamentallyâ€”in materials, in mechanisms. Yet similarity is judged subjectively, and perception shifts with subjectivity. Appealing to physical differences alone cannot explain the complex social phenomenon captured in the screenshot above.</p> <h2 id="conclusion-designing-a-harmonious-future">Conclusion: Designing a Harmonious Future</h2> <p>As weâ€™ve seen, AI has moved beyond simple computation into learning, reasoning, and creativity, forcing us to reexamine what we thought were uniquely human characteristicsâ€”reason, consciousness, responsibility, social bonds. While positivism and logical positivism focused on objective facts and pure logic, post-positivism questions even researchersâ€™ subjectivity and value judgments. But science is only a tool. We must now ask: what questions should we pursue? What kind of society do we want to build? This represents a necessary paradigm shift for AI research. Understanding AI in contextâ€”what it means, how far our responsibility extendsâ€”has become paramount.</p> <p>In future societies where AI agents take on extensive rolesâ€”in production, services, caregiving, and creative workâ€”the old assumption that â€œmachines are just machinesâ€ will no longer hold. We may need new frameworks based on concepts like mutual similarity and emotional bonds. Designing legal and ethical systems that address attachment to robots and justify their protection will require fundamentally expanding the notion of legal personhood beyond humans. We must specify who bears responsibility when care robots make errors, and consider whether AI itself might someday hold certain rights and duties.</p> <p>Ultimately, AI-era researchers and policymakers must think as deeply about what makes us human as they do about technical performance. Science explores objective truth, but we must simultaneously work to understand subjective experience and social context. Humanities, social sciences, and law must join engineering in the laboratory and at the policy table. Only then can we design a society where humans and AI coexist while preserving human dignity and responsibility.</p>]]></content><author><name></name></author><category term="essay"/><category term="artificial-intelligence"/><category term="philosophy"/><category term="humanities"/><category term="ethics"/><category term="consciousness"/><category term="responsibility"/><category term="english"/><summary type="html"><![CDATA[Exploring the essential role of humanities in AI research]]></summary></entry><entry><title type="html">TEST 01 : ì •ë‹µ</title><link href="https://mutantq.github.io/blog/2024/tutoring-test-similarity-answer/" rel="alternate" type="text/html" title="TEST 01 : ì •ë‹µ"/><published>2024-07-02T01:00:00+00:00</published><updated>2024-07-02T01:00:00+00:00</updated><id>https://mutantq.github.io/blog/2024/tutoring-test-similarity-answer</id><content type="html" xml:base="https://mutantq.github.io/blog/2024/tutoring-test-similarity-answer/"><![CDATA[<p><em>ë³¸ ìë£ŒëŠ” ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•˜ì§€ ì•Šê³  ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p> <p><a href="/blog/2021/tutoring-test-similarity/">ë¬¸ì œë¡œ ëŒì•„ê°€ê¸°</a></p> <p><strong>ë¬¸ì œ 1 ì •ë‹µ:</strong> $\frac{3}{2}\rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled.png" alt="Untitled"/></p> <p><strong>ë¬¸ì œ 2 ì •ë‹µ:</strong> $\frac{168}{125} \rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled%201.png" alt="Untitled"/></p> <p><strong>ë¬¸ì œ 3 ì •ë‹µ:</strong> $9:4$</p> <p><img src="/assets/img/blog/tutoring/Untitled%202.png" alt="Untitled"/></p> <p><strong>ë¬¸ì œ 4 ì •ë‹µ:</strong> $\frac{27}{2}\rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled%203.png" alt="Untitled"/></p> <p><strong>ë¬¸ì œ 5 ì •ë‹µ:</strong> $b^2=ac$</p> <p><img src="/assets/img/blog/tutoring/Untitled%204.png" alt="Untitled"/></p>]]></content><author><name></name></author><category term="education"/><category term="education"/><category term="tutoring"/><category term="mathematics"/><category term="korean"/><summary type="html"><![CDATA[ìˆ˜í•™ êµìœ¡ ìë£Œ]]></summary></entry></feed>