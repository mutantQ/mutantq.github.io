<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://codingjang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://codingjang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-05T15:04:06+00:00</updated><id>https://codingjang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yejun Jang is an AI researcher specializing in reinforcement learning and deep learning at Seoul National University. </subtitle><entry><title type="html">TEST 01 정답</title><link href="https://codingjang.github.io/blog/2024/tutoring-test-similarity-answer/" rel="alternate" type="text/html" title="TEST 01 정답"/><published>2024-07-02T10:00:00+00:00</published><updated>2024-07-02T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/tutoring-test-similarity-answer</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/tutoring-test-similarity-answer/"><![CDATA[<p><a href="/blog/2021/tutoring-test-similarity/">문제로 돌아가기</a></p> <p><strong>문제 1 정답:</strong> $\frac{3}{2}\rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled.png" alt="Untitled"/></p> <p><strong>문제 2 정답:</strong> $\frac{168}{125} \rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled%201.png" alt="Untitled"/></p> <p><strong>문제 3 정답:</strong> $9:4$</p> <p><img src="/assets/img/blog/tutoring/Untitled%202.png" alt="Untitled"/></p> <p><strong>문제 4 정답:</strong> $\frac{27}{2}\rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled%203.png" alt="Untitled"/></p> <p><strong>문제 5 정답:</strong> $b^2=ac$</p> <p><img src="/assets/img/blog/tutoring/Untitled%204.png" alt="Untitled"/></p>]]></content><author><name></name></author><category term="education"/><category term="education"/><category term="tutoring"/><category term="mathematics"/><category term="korean"/><summary type="html"><![CDATA[수학 교육 자료]]></summary></entry><entry xml:lang="en"><title type="html">Introduction to PettingZoo</title><link href="https://codingjang.github.io/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466/" rel="alternate" type="text/html" title="Introduction to PettingZoo"/><published>2023-10-12T10:00:00+00:00</published><updated>2023-10-12T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466/"><![CDATA[<h2 id="documentation-link">Documentation link</h2> <p><a href="https://pettingzoo.farama.org/">PettingZoo Documentation</a></p> <h2 id="brief-introduction-to-pettingzoo">Brief Introduction to PettingZoo</h2> <p>PettingZoo is a Python library that facilitates General Multi-Agent Reinforcement Learning (MARL) simulations.</p> <p>PettingZoo consists of two main APIs:</p> <ul> <li><strong>AEC API</strong>: Helps implement environments with turns between agents, similar to board games. <ul> <li>Environments with turns are called Agent Environment Cycle (AEC) environments.</li> </ul> </li> <li><strong>Parallel API</strong>: All agents act simultaneously within a single time step. <ul> <li>These are called Parallel environments, referring to parallel/concurrent execution.</li> </ul> </li> </ul> <p>There exist AEC-to-Parallel and Parallel-to-AEC converters that allow transformation between the two APIs. However, when developing for the first time, it’s recommended to focus on one API without worrying too much about conversion.</p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="MARL"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Multi-agent reinforcement learning library for Python]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 7. Function Approximation</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13/" rel="alternate" type="text/html" title="DeepMind X UCL | 7. Function Approximation"/><published>2023-08-23T11:00:00+00:00</published><updated>2023-08-23T11:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13/"><![CDATA[<h2 id="why-function-approximations">Why function approximations?</h2> <p>It is hard to build lookup tables if the state space $S$ or the action space $A$ is large. For example, the game of go has $10^{170}$ states, which is more than the number of atoms in the universe squared. Therefore, creating the lookup table $v(s)$ is simply infeasible. Not only that, the universe itself is continuous - and we cannot give up on RL just because it is.</p> <p>Therefore, we will estimate the value function using function approximation. In many cases, the function approximation will involve some parameter vector $\mathbf{w}$:</p> \[\begin{aligned} v_{\mathbf{w}}(s) &amp;\approx v^\pi(s) \;(\text{or} \; v^\star(s)), \\ q_{\mathbf{w}}(s,a)&amp;\approx q^\pi(s,a) \;(\text{or} \; q^\star(s,a)). \end{aligned}\] <p>The parameter vector $\mathbf{w}$ will usually be some high dimensional vector that we can adjust to approximate the the value function. In a deep RL setup, we can also imagine $v_\mathbf{w}(s)$ and $q_\mathbf{w} (s, a)$ as a neural network, a rich class of nonlinear function that maps the state space to a range of values $(v_\mathbf{w}(s):S \rightarrow \mathbb{R})$ or the state-action space to a range of values $(q_\mathbf{w}(s, a) : S \times A \rightarrow \mathbb{R})$.</p> <h2 id="properties-of-rl">Properties of RL</h2> <p>There are certain properties of RL we should consider when constructing function approximations for value functions. The most important of all is that the regression targets may be non-stationary because of either:</p> <ul> <li>Changing policies of the agent <ul> <li>The agent’s policy $\pi$ change $v^\pi(s)$, which then changes the target,</li> </ul> </li> <li>Bootstrapping <ul> <li>The target involves $v^\pi(s)$ itself, which is constantly being updated,</li> </ul> </li> <li>Non-stationary dynamics <ul> <li>There might be other learning agents in the environment,</li> </ul> </li> <li>Previously unobserved information <ul> <li>Previously unobserved facts might influence the value function.</li> </ul> </li> </ul> <h2 id="tabular-vs-linear-vs-non-linear">Tabular vs. Linear vs. Non-linear</h2> <p>When we use <strong>tabular function approximations</strong>, we have a good theory on how agents learn (such as the Bellman Equation). But it does not scale or generalize well to larger or continuous state-action spaces.</p> <p>With <strong>linear function approximations</strong>, we have a reasonably good theory on the learning process. But the actual performance is dependent on the fixed feature mapping $\mathbf{x}:S\rightarrow \mathbb{R}^n$ (which will be discussed below).</p> <p>With <strong>nonlinear function approximations (deep learning)</strong>, we do not yet have a good theory at all - but we know that it works well experimentally, and that it works even better when we do not hand-engineer the feature mapping and let the neural network learn the mapping by itself.</p> <p>The fact that we do not have to hand engineer features when using deep learning is quite useful, because it means that we can use it to study less well-known problems without having to worry about feature mappings.</p> <h2 id="linear-function-approximations">Linear Function Approximations</h2> <h3 id="approximating-state-value-functions">Approximating state-value functions</h3> <p>Since we have already explored the tabular case, let’s now explore the linear case. In a linear function approximation setup, we represent the state (or the observation) at time $t$ as a vector $\mathbf{x}(S_t)$ which we will also call $\mathbf{x}_t$. This mapping $\mathbf{x}:S\rightarrow \mathbb{R}^n$ is called the feature mapping, and is considered to be fixed:</p> \[\mathbf{x}(s)=\begin{pmatrix} x_1(s) \\ \vdots \\ x_n(s) \end{pmatrix},\; \forall s \in S.\] <p>Then we approximate the value function by a linear combination of the weights and features:</p> \[v_\mathbf{w}(s)=\mathbf{w}^\top \mathbf{x}(s)=\sum_{j=1}^{n} \mathbf{w}_j\mathbf{x}_j(s)\] <h3 id="approximating-action-value-functions">Approximating action-value functions</h3> <p>But what about action-value functions? We can use two different techniques to approximate $q(s,a)$. The first one is similar to the previous case. With the feature mapping $\mathbf{x}:S\times A \rightarrow \mathbb{R}^n$:</p> \[\mathbf{x}(s,a)=\begin{pmatrix} x_1(s,a) \\ \vdots \\ x_n(s,a) \end{pmatrix},\; \forall s \in S.\] <p>The approximation of the value function is almost identical as before:</p> \[q_\mathbf{w}(s, a)=\mathbf{w}^\top\mathbf{x}(s,a)=\mathbf{w}_j\mathbf{x}_j(s,a)\] <p>This is called the <strong>action-in approximation</strong> because we take the action $a$ as an input of the value function. Here, we reuse the same weights $\mathbf{w}$ for different actions $a$. We also have the <strong>action-out approximation</strong>:</p> \[\mathbf{q}_\mathbf{w}(s)=\mathbf{W}\mathbf{x}(s),\; \text{ where }\;q_\mathbf{w}(s, a)= \mathbf{q}_\mathbf{w}(s)[a]\] <p>Note that $\mathbf{W}$ is a matrix. Therefore, we have different set of weights for different actions. For the action-out case we do not reuse the weights. Instead, we share the feature vector $\mathbf{x}(s)$ for all action $a$’s.</p> <p><strong>Action-in</strong> approximation is easier if the actions space is <strong>large or continuous</strong>. But for <strong>(small) discrete action spaces</strong>, <strong>action out</strong> is common. An example of this is DQN.</p> <h2 id="objective-minimize-loss">Objective: Minimize Loss</h2> <p>Now, we will construct a quadratic loss $J(\mathbf{w})$ and minimize it by optimizing our weight vector:</p> \[J(\mathbf{w}) = \mathbb{E}_{S \sim d} \left[(v^\pi(S)-\mathbf{w}^\top \mathbf{x}(S))^2 \right]\] <p>The distribution $d$ is some distribution of states in which we can sample from. Suppose we already know the value function $v^\pi(s)$. Then we can apply stochastic gradient descent (SGD) with decaying step size $\alpha_t$ to find some local minimum / saddle point of a smooth function:</p> \[\nabla_\mathbf{w} v_\mathbf{w}(S_t)=\mathbf{x}(S_t) = \mathbf{x}_t, \\ \Delta \mathbf{w}_t = \alpha_t(v^\pi(S_t)-v_\mathbf{w}(S_t)) \mathbf{x}_t.\] <p>Note that the gradient of our value function, $\nabla_\mathbf{w} v_\mathbf{w}(S_t)$, is simply $\mathbf{x}(S_t)$. Luckily, $J(\mathbf{w})$ is quadratic in $\mathbf{w}$, and it’s Hessian is positive semi-definite everywhere in $\mathbb{R}^n$ which makes it a semi-convex(?) function. Hence, every local minimum is a global minimum. SGD with decaying step size will then ensure $J(\mathbf{w})$’s convergence to the global minimum.</p> <h2 id="mc-and-td-with-linear-approximation">MC and TD with Linear Approximation</h2> <p>However, we do not “know” the value function $v^\pi(S_t)$, which means we can’t use it to update our weight vector $\mathbf{w}$. Therefore, we substitute $v^\pi(s)$ with a stochastic target $G_t$:</p> \[\Delta \mathbf{w}_t = \alpha_t(G_t-v_\mathbf{w}(S_t)) \mathbf{x}_t.\] <p>This is <strong>MC with Linear Approximation</strong>. What is cool is that we can apply a supervised learning setup to the online training data</p> \[\{(S_0, G_0), \cdots, (S_T, G_T)\}\;\;T=\text{terminal time-step},\] <p>since $G_t$ is unbiased. If the variance of $G_t$ is too large, we can replace the target with the TD target $R_{t+1} + \gamma v_\mathbf{w}(S_{t+1})$ to get:</p> \[\Delta \mathbf{w}_t = \alpha_t(R_{t+1}+\gamma v_\mathbf{w} (S_{t+1})-v_\mathbf{w}(S_t)) \mathbf{x}_t.\] <p>The above update is called <strong>TD with Linear Approximation</strong>.</p> <h2 id="convergence-of-mc-with-linear-approximation">Convergence of MC with Linear Approximation</h2> <p>With linear value function approximation and suitably decaying step size $\alpha_t \rightarrow 0$, it is known that MC converges to:</p> \[\mathbf{w}_\text{MC} =\operatorname{argmin}_\mathbf{w} {\mathbb{E}^\pi[(G_t-v_\mathbf{w}(S_t))^2]}=\mathbb{E}^\pi [\mathbf{x}_t \mathbf{x}_t^\top]^{-1} \mathbb{E}^\pi[G_t\mathbf{x}_t]\] <p>We can verify this by setting the gradient of ${\mathbb{E}^\pi[(G_t-v_\mathbf{w}(S_t))^2]}$ with respect to $\mathbf{w}$ to zero:</p> \[\begin{align} \nabla_\mathbf{w} \mathbb{E}^\pi[(G_t - v_\mathbf{w}(S_t))^2]=\nabla_\mathbf{w} \mathbb{E}^\pi[(G_t - \mathbf{w}^\top\mathbf{x}_t)^2]&amp;=0\\\mathbb{E}^\pi[(G_t - \mathbf{w}^\top\mathbf{x}(S_t))\nabla_\mathbf{w}(\mathbf{w}^\top\mathbf{x}_t)]&amp;=0 \\=\mathbb{E}^\pi[(G_t - \mathbf{w}^\top\mathbf{x}_t)\mathbf{x}_t]&amp;=0 \\= \mathbb{E}^\pi[G_t\mathbf{x}_t - \mathbf{x}_t^\top\mathbf{x}_t\mathbf{w}]&amp;=0 \\ \mathbb{E}^\pi[\mathbf{x}_t\mathbf{x}_t^\top]\mathbf{w}=\mathbb{E}^\pi[G_t\mathbf{x}_t] \\ \mathbf{w}=\mathbf{w}_\text{MC}=\mathbb{E}^\pi[\mathbf{x}_t \mathbf{x}_t^\top]^{-1}\mathbb{E}^\pi[G_t\mathbf{x}_t]\end{align}\] <h2 id="convergence-of-td-with-linear-approximation">Convergence of TD with Linear Approximation</h2> <p>With linear value function approximation and suitably decaying step size $\alpha_t \rightarrow 0$, it is known that TD converges to:</p> \[\mathbf{w}_\text{TD} =\mathbb{E}^\pi [\mathbf{x}_t (\mathbf{x}_t-\gamma \mathbf{x}_{t+1})^\top]^{-1} \mathbb{E}^\pi[R_{t+1}\mathbf{x}_t]\] <p>We can verify this by setting the expected value of $\Delta\mathbf{w}$ to zero. Assuming $\alpha_t$ does not correlate with $R_{t+1},\mathbf{x}<em>t,\mathbf{x}</em>{t+1}$:</p> \[\begin{align} \mathbb{E}^\pi[\Delta \mathbf{w}]=0&amp;=\mathbb{E}^\pi[\alpha_t(R_{t+1} + \gamma\mathbf{x}_{t+1}^\top\mathbf{w}-\mathbf{x}_t^\top\mathbf{w})\mathbf{x}_t] \\ 0 &amp;= \mathbb{E}^\pi[\alpha_tR_{t+1} \mathbf{x}_t] + \mathbb{E}^\pi[\alpha_t\mathbf{x}_t(\gamma\mathbf{x}_{t+1}^\top-\mathbf{x}_t^\top)\mathbf{w}] \\ \mathbb{E}^\pi[\alpha_t\mathbf{x}_t(\mathbf{x}_t^\top-\gamma\mathbf{x}_{t+1}^\top)]\mathbf{w} &amp;= \mathbb{E}^\pi[\alpha_tR_{t+1} \mathbf{x}_t] \\ \mathbf{w} = \mathbf{w}_\text{TD}&amp;= \mathbb{E}^\pi[\alpha_t\mathbf{x}_t(\mathbf{x}_t^\top-\gamma\mathbf{x}_{t+1}^\top)]^{-1} \mathbb{E}^\pi[R_{t+1} \mathbf{x}_t]\end{align}\] <p>This differs from the MC solution. Remember, TD updates have less variance but may be biased. But since they have less variance, they tend to converge faster.</p> <h2 id="residual-bellman-updates">Residual Bellman updates</h2> <p>Note that the TD update is not a gradient update, since it ignores the dependence of $v_\mathbf{w}(S_{t+1})$ on $\mathbf{w}$.</p> \[\Delta \mathbf{w}_t = \alpha \delta \nabla_\mathbf{w} v_\mathbf{w}(S_t)\;\text{ where }\; \delta_t= R_{t+1} + \gamma v_\mathbf{w}(S_{t+1})-v_\mathbf{w}(S_t)\] <p>To remedy this, we can use the Bellman residual gradient update, where the Bellman loss is given as $\mathbb{E}^\pi[\delta_t^2]$ and we take the gradient of it to update $\mathbf{w}$:</p> \[\nabla_\mathbf{w}\mathbb{E}^\pi[\delta_t^2]=\mathbb{E}^\pi[\nabla_\mathbf{w}(\delta_t^2)]=2\mathbb{E}^\pi[\delta_t \nabla_\mathbf{w}\delta_t]\] \[\Delta \mathbf{w}_t = \alpha\delta_t \nabla_\mathbf{w}\delta_t = \alpha \delta_t \nabla_\mathbf{w} (v_\mathbf{w}(S_t)-\gamma v_\mathbf{w}(s_{t+1}))\] <p>However, residual Bellman updates tend to work worse in practice.</p> <h2 id="the-deadly-triad">The Deadly Triad</h2> <p>Algorithms that combine:</p> <ul> <li><strong>Bootstrapping</strong></li> <li><strong>Off-policy learning</strong>, and</li> <li><strong>Function approximation</strong></li> </ul> <p>may diverge. This is called <strong>the deadly triad</strong>. However, just because an algorithm combines the three methods does not mean that it is divergent - rather, we <strong>cannot guarantee</strong> the convergence of an algorithm if is has combined the above three.</p> <h2 id="summary">Summary</h2> <p><img src="/assets/img/blog/reinforcement-learning/untitled.png" alt="Untitled"/></p> <p>In addition to the deadly triad, we cannot guarantee the convergence of MC or TD when we combine on-policy learning with bootstrapping and non-linear function approximation. In summary, the deadly triad is the theoretical risk due to combinations of different methods in reinforcement learning, but is rarely seen in practice.</p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 6. Model-free Control</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c/" rel="alternate" type="text/html" title="DeepMind X UCL | 6. Model-free Control"/><published>2023-08-23T10:00:00+00:00</published><updated>2023-08-23T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c/"><![CDATA[<h2 id="glie">GLIE</h2> <p>GLIE stands for <strong>Greedy in the Limit with Infinite Exploration</strong>. It is used to describe a set of desirable properties of a policy. GLIE is a combination of the following two properties:</p> <ol> <li> <p><strong>Greedy in the Limit</strong> means that the policy eventually converges to a greedy policy, i.e.</p> \[\lim_{t \rightarrow \infty} {\pi_t(a|s)}=I(a=\operatorname{argmax}_{a' \in A}{q_t(s,a')})\] </li> <li> <p><strong>Infinite Exploration</strong> means that all state-action pairs are explored infinitely many times, i.e.</p> </li> </ol> \[\forall s, a \;\;\lim_{t\rightarrow \infty} {N_t (s,a)=\infty}\] <p>If we have the Infinite Exploration property, samples for each state-action pairs will accumulate enough, allowing for accurate value prediction. The Greedy in the Limit property then ensures that the policy will converge to the optimal greedy policy.</p> <p>Greedy policy alone will not explore enough, and the $\epsilon$-greedy policy with fixed $\epsilon \in (0,1]$ will never fully exploit. By choosing $\epsilon$-greedy policy with $\epsilon_t=1/t$, where $t$ is the number of time-steps elapsed, we have a GLIE policy that will both explore and exploit sufficiently in the limit.</p> <h2 id="analogy-between-dp-and-model-free-algorithms">Analogy Between DP and Model-free Algorithms</h2> <p>In lecture 04, we covered different types of Bellman operators:</p> \[\begin{aligned} (T_V^\star f)(s)&amp;=\max_{a \in A} \biggl[ {r(s, a) + \gamma \mathbb{E} \left[f(s')|s, a\right]} \bigg], \;\forall f \in V \\ (T_V^\pi f)(s)&amp;=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s') \bigg| s, a \bigg], \;\forall f \in V \\(T_Q^\star f)(s,a)&amp;=\mathbb{E} \bigg[r(s, a) + \gamma \max_{a'\in A} f(s',a') \bigg|s, a\bigg], \;\forall f \in Q \\ (T_Q^\pi f)(s, a)&amp;=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s',a') \bigg| s, a \bigg], \;\forall f \in Q \end{aligned}\] <p>To apply a Bellman operator we need exact knowledge of the transition dynamics of the system. We can avoid this problem using a sampled version of the operator. It turns out that the sampled versions of the above Bellman operators correspond to different model-free algorithms, except for $(T_V^\star f)(s)$:</p> \[\begin{aligned} &amp;(T_V^\star f)(s) \leftrightarrow \text{(None)} \\ &amp;(T_V^\pi f)(s)\leftrightarrow \text{(TD)} \\ &amp;\leftrightarrow v_{t+1}(S_t)=v_t(S_t)+\alpha_t\bigg(R_{t+1}+\gamma v_t(S_{t+1})-v_t(S_t)\bigg)\\&amp;(T_Q^\star f)(s,a)\leftrightarrow \text{(Q-learning)} \\&amp; \leftrightarrow q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \alpha_t \bigg(R_{t+1} + \gamma \max_{a' \in A}{q_t(S_{t+1}, a')-q_t(S_t, A_t)\bigg)}\\ &amp;(T_Q^\pi f)(s, a) \leftrightarrow \text{(SARSA)} \\ &amp;\leftrightarrow q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \alpha_t \bigg(R_{t+1} + \gamma q_t(S_{t+1}, A_{t+1})-q_t(S_t, A_t)\bigg) \end{aligned}\] <p>It is evident that we cannot build a sampled version of the operator $(T_V^\star f)(s)$ - Since the $\max_{a \in A}$ and the $\mathbb{E}$ operator do not commute, $(T_V^\star f)(s)$ cannot be expressed as an expectation from which we can sample upon.</p> <p>SARSA is relatively simple - it’s simply the $q$-version of TD. However, Q-learning has some interesting properties that deserves attention of its own.</p> <h2 id="on--off-policy-learning">On &amp; Off-Policy Learning</h2> <p>As humans, we learn from our experience. But we can also learn from the experience of others. In on-policy learning, the agent learns about the <strong>behavior policy $\pi$</strong> from experience sampled from the same policy $\pi$. On the other hand, in off-policy learning, the agent learns about the <strong>target policy $\pi$</strong> from experience sampled from a separate behavior policy $\mu$.</p> <p>Using off-policy learning, we can:</p> <ul> <li>learn from observing humans or other agents</li> <li>re-use experience from old policies</li> <li>learn about multiple policies while following one policy</li> <li><strong>learn about greedy policy while following exploratory policy</strong></li> </ul> <h2 id="q-learning">Q-Learning</h2> <p>Q-learning can learn the greedy policy while following any (exploratory) policy. We can see this from the update equation:</p> \[q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \alpha_t \bigg(R_{t+1} + \gamma \max_{a' \in A}{q_t(S_{t+1}, a')-q_t(S_t, A_t)\bigg)}\] <p>Here, there is no policy $\pi$ involved - you can use any behavior policy $\mu$ to converge to the optimal value function $q^\star$, as long as it is a infinite exploration policy. Once $q^\star$ is learned, we can use the (optimal) greedy policy for exploitation:</p> \[{\pi^\star(a|s)}=I(a=\operatorname{argmax}_{a'\in A}{q^\star(s,a')}).\] <h3 id="theorem">Theorem</h3> <p>Q-learning converges to the optimal $q$-value function, $q\rightarrow q^\star$, as long as we take each action in each state indefinitely often AND decay the step sizes in such a way that $\sum_t\alpha_t=\infty$ and $\sum_t \alpha_t^2&lt;\infty$.</p> <p>For example,<br/> $\alpha_t= 1/t^\omega, \omega \in (0.5, 1)$.</p> <h2 id="overestimation-in-q-learning">Overestimation in Q-Learning</h2> <p>In the Q-learning update equation, let’s take a look at the maximization:</p> \[\max_{a' \in A}{q_t(S_{t+1}, a')}\] <p>To write things differently:</p> \[\max_{a' \in A}{q_t(S_{t+1}, a')}=q_t\left(S_{t+1}, \operatorname{argmax}_{a' \in A} q_t(S_{t+1}, a')\right)\] <p>Suppose that the value function is currently inaccurate and has high noise. For simplicity, assume that the optimal q-value function $q^\star(S_{t+1},a’)$ stays constant regardless of the action $a’$ taken. For some of the $a’$s, the noise will add up to increase $q$. Therefore, the $\operatorname{argmax}_{a’ \in A}$ will choose the $a’$ with the highest noise value then update $q(S_{t}, A_{t})$ towards the noise-added value. Similar logic applies to the case where $q^\star(S_{t+1},a’)$ is not constant with respect to $a’$. Hence, Q-learning tends to overestimate the optimal $q$-value function.</p> <h3 id="double-q-learning">Double Q-Learning</h3> <p>How can we solve this problem? We can store two action value functions, $q$ and $q’$, and alternate between the two targets below:</p> \[\text{(target for }q \text{):}\;\;R_{t+1} + \gamma q'\left(S_{t+1}, \operatorname{argmax}_{a' \in A} q(S_{t+1},a')\right) \\\\ \text{(target for }q' \text{):}\;\;R_{t+1} + \gamma q\left(S_{t+1}, \operatorname{argmax}_{a' \in A} q'(S_{t+1},a')\right)\] <p>This eliminates the influence of noise by decoupling the selection ($\operatorname{argmax}$) step and the evaluation step.</p> <p><img src="/assets/img/blog/reinforcement-learning/screenshot_2023-08-30_at_3.44.02_pm.png" alt="Q-learning overestimates, whereas double Q-learning does not. (Source: DeepMind X UCL Deep RL lectures)"/></p> <p>Q-learning overestimates, whereas double Q-learning does not. (Source: DeepMind X UCL Deep RL lectures)</p> <p>The above plot shows how decoupling indeed eliminates the overestimation in Q-learning. We can also apply this method to SARSA whenever the behavior policy is (soft) greedy and has correlation with $q$ (we call this double SARSA).</p> <h2 id="importance-sampling">Importance Sampling</h2> <p>Suppose you want to evaluate</p> \[\mathbb{E}_{X \sim d}[f(X)]\] <p>for some distribution $d$. If we sample $X$ to yield the estimate as follows,</p> \[\mathbb{E}_{X \sim d}[f(X)] \simeq \hat{X} :=\frac{1}{N} \sum_{i=1}^{N} f(X_i), \;\text{for each}\;X_i \sim d,\] <p>It could be problematic if $f(X)$ deviates significantly from $\mathbb{E}_{X \sim d}[f(X)]$ for some rare events, since it will overestimate or underestimate whenever the rare event is not sufficiently sampled.</p> <p>Therefore, we can seek to sample from a different distribution $d’$ so that the rare events are sampled more. Now, suppose that we have samples $f(X_i)$ with $X_i \sim d’$. How can we evaluate the original expectation using these samples? We can first modify the original expectation as follows:</p> \[\begin{aligned}\mathbb{E}_{X \sim d}[f(X)]&amp;=\sum_x d(x)f(x) \\ &amp;= \sum_x d'(x)\frac{d(x)}{d'(x)}f(x) \\ &amp;= \mathbb{E}_{X \sim d'} \left[\frac{d(x)}{d'(x)}f(x)\right] \end{aligned}\] <p>Now, we have a new expectation that can be sampled from $d’$ instead. Note that $d’$ has to be positive for all $x$ for this to work. Sampling from $d’$ gives:</p> \[\mathbb{E}_{X \sim d}[f(X)] \simeq \hat{X}' := \frac{1}{N} \sum_{i=1}^{N} \frac{d(X_i)}{d'(X_i)}f(X_i), \;\text{for each}\;X_i \sim d'.\] <p>This technique of sampling from a new distribution $d’$ to yield an estimate for the original expectation $\mathbb{E}_{X \sim d}[f(X)]$ is called Importance Sampling.</p> <h3 id="importance-sampling-for-off-policy-mc">Importance Sampling for Off-Policy MC</h3> <p>Suppose you want to estimate the $v$-value function $v^\pi$ for some policy $\pi$ using MC, and that the trajectory $\tau_t={S_t, A_t, R_{t+1} , \cdots }$ is generated with some behavior policy $\mu$. We can get an importance sample for $G_t=G(\tau_t)=R_{t+1}+\gamma R_{t+2} + \cdots$ by reweighing the target with $\frac{p(\tau_t|\pi)}{p(\tau_t|\mu)}$ (Suppose $N=1$):</p> \[\frac{p(\tau_t|\pi)}{p(\tau_t|\mu)} G_t = \frac{p(A_t|S_t,\pi)p(R_{t+1},S_{t+1}|S_t,A_t)p(A_{t+1}|S_{t+1},\pi) \cdots}{p(A_t|S_t,\mu)p(R_{t+1},S_{t+1}|S_t,A_t)p(A_{t+1}|S_{t+1},\mu) \cdots} G_t\] <p>Luckily, the transition probability (in which most cases we do not know) cancels out and we are left with:</p> \[\begin{aligned}\frac{p(\tau_t|\pi)}{p(\tau_t|\mu)} G_t &amp;= \frac{p(A_t|S_t,\pi)p(A_{t+1}|S_{t+1},\pi) \cdots}{p(A_t|S_t,\mu)p(A_{t+1}|S_{t+1},\mu) \cdots} G_t \\ &amp;= \frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1}) \cdots}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1}) \cdots} G_t \end{aligned}\] <p>We can then update $v^\pi$ towards the importance sampled target to get:</p> \[v(S_t) \leftarrow v(S_t) + \alpha\left({\frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1}) \cdots}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1}) \cdots} G_t - v(S_t)} \right)\] <h3 id="importance-sampling-for-off-policy-td">Importance Sampling for Off-Policy TD</h3> <p>Now, suppose you want to go through the same procedure with MC. In this case, you only need a single correction:</p> \[v(S_t) \leftarrow v(S_t) + \alpha\left(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} (R_{t+1} +\gamma v(S_{t+1})) - v(S_t) \right)\] <p>The proof for this can be found in page 44 of the lecture material (<a href="https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%206%20-%20Model-free%20control.pdf">link</a>).</p> <h2 id="expected-sarsa-generalized-q-learning">Expected SARSA (Generalized Q-learning)</h2> <p>We can also attempt to apply importance sampling to SARSA. However, we quickly realize that we don’t actually need IS because the $q$-value function conditions on selecting some action $a$. Therefore, we can simply take the expectation for the next $q$-values conditioned on policy $\pi$, while creating the trajectory according to some other policy $\mu$:</p> \[q(S_t, A_t) \leftarrow q(S_t, A_t) + \alpha \left(R_{t+1}+ \gamma \sum_{a \in A} \pi(a |S_{t+1})q(S_{t+1}, a)-q(S_t, A_t) \right)\] <p>Expected SARSA is also called Generalized Q-learning because it reduces to Q-learning when the policy chosen to be $\pi=\pi_q$, where $\pi_q$ is the greedy policy generated from $q$.</p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 5. Model-free Prediction</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/" rel="alternate" type="text/html" title="DeepMind X UCL | 5. Model-free Prediction"/><published>2023-08-16T10:00:00+00:00</published><updated>2023-08-16T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/"><![CDATA[<p><strong>Note.</strong> The notation used here might be confusing. We use $S$ to denote the state space and $S_t$ to represent the state at time $t$. Similarly, $A$ represents the action space, and $A_t$ denotes the action at time $t$.</p> <h2 id="dp">DP</h2> \[v_{n+1}(S_t) = \mathbb{E}^\pi \left[R_{t+1}+\gamma v_n(S_{t+1})\,|\,S_t\right]\] <h3 id="what-does-this-mean">What does this mean?</h3> <p>DP stands for Dynamic Programming. There are several different versions of DP. In this version, we improve the value function by directly evaluating the expectation on the right-hand side, i.e. the Bellman Expectation Operator. Evaluating this operator can easily become computationally infeasible when the state-action space is large. Moreover, it’s impossible to evaluate without knowledge of the environment’s dynamics. This is why we need model-free algorithms such as MC and TD.</p> <h2 id="mc">MC</h2> \[\begin{aligned} G_t&amp;=R_{t+1}+\gamma G_{t+1}=\cdots=\sum_{k=0}^{T} {\gamma^k R_{t+k+1}} \text{ (target)} \\ v_{n+1}(S_t)&amp;=v_n(S_t)+\alpha (G_t-v_n(S_t)) \text{ (update)} \end{aligned}\] <h3 id="what-does-this-mean-1">What does this mean?</h3> <p>MC stands for <strong>Monte Carlo</strong>. In a Monte Carlo update, the sampled return $G_t$ is determined by processing the entire $n$-th episode up to the terminal time step $T$. Then, $v_n(S_t)$ is updated towards $G_t$ with a step-size $\alpha$. Unlike DP, MC updates can be performed even without the knowledge of the rules underlying the environment, as the updates are based on samples. However, since $G_t$ can have a large variance, we use a small step-size $\alpha$ to reduce noise during updates.</p> <h2 id="td">TD</h2> \[\begin{aligned} H_t&amp;=R_{t+1}+\gamma v_t(S_{t+1})\text{ (target)} \\ v_{t+1}(S_t)&amp;=v_t(S_t)+\alpha(H_t-v_t(S_t))\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-2">What does this mean?</h3> <p>TD stands for <strong>Temporal Difference</strong>. In a Temporal Difference update, for each time step $t$ of the episode we update the value $v_t(S_t)$ to $v_{t+1}(S_t)$ by updating it towards the bootstrapped return $H_t$. We can think of TD as the sampled version of DP. TD is a bootstrapping method in the sense that it uses the estimate $v_t(S_{t+1})$ itself to create the target $H_t$. Therefore, it does not calculate the full cumulative return $G_t$. Note that the step-size $\alpha$ is also used since $H_t$ is a random variable.</p> <h2 id="comparing-mc-and-td">Comparing MC and TD</h2> <p>Although the equations look similar, MC and TD differ substantially in the below aspects:</p> <ol> <li> <p><strong>computation</strong></p> <p>For a MC update, the episode needs to conclude before updating the value function, as it requires the calculation of $G_t$, which involves future terms. In contrast, TD can be updated as we go, since the target $H_t$ can be calculated for each time step.</p> </li> <li> <p><strong>bootstrapping</strong></p> <p>In a MC update, we do not bootstrap from the value function estimate $v_n(S_t)$ to calculate the the target $G_t$, since it is independently calculated from the rewards in the time range $[t+1, T]$. However, we do bootstrap in a TD update as can be seen from the definition of the target $H_t$ - it involves the value function itself, $v_t(S_{t+1})$.</p> </li> <li> <p><strong>bias and variance</strong></p> <p>In a MC update, the target $G_t$ is the unbiased estimator of the true value function value $v^{\pi}(S_t)$. However $G_t$ has large variance as it is the weighted sum of multiple rewards $R_k\;(k=t+1,\cdots,T)$, which are all random variables. The circumstances are different for TD updates, because the target $H_t$ only involves two random variables, $R_{t+1}$ and $v_t(S_{t+1})$. Since there are less random “components” in the target, the variance is kept low - however unbiasedness is sacrificed due to bias-variance tradeoff.</p> </li> </ol> <h2 id="n-step-td">$n$-step TD</h2> \[\begin{aligned} G_t^{(n)}&amp;=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n v_k(S_{t+n}) \text{ (target)} \\ v_{k+1}(S_t)&amp;=v_k(S_t)+\alpha\left(G_t^{(n)}-v_k(S_t)\right)\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-3">What does this mean?</h3> <p>Suppose you want to cut off the later terms in $G_t$, and bootstrap at some point instead. If we do so, we get the $n$-step TD, with $n$ reward terms ($R_{t+1},\cdots,R_{t+n}$) and one bootstrapping term ($v_k(S_{t+n})$). As expected, the $n$-step TD has intermediate bias and intermediate variance, and interpolates between MC and TD.</p> <h2 id="lambda-td">$\lambda$-TD</h2> \[\begin{aligned} G_t^{\lambda}&amp;=R_{t+1}+\gamma \left((1-\lambda)v_k(S_{t+1}) + \lambda G_{t+1}^\lambda\right) \text{ (target)} \\ v_{k+1}(S_t)&amp;=v_k(S_t)+\alpha\left(G_t^{\lambda}-v_k(S_t)\right)\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-4">What does this mean?</h3> <p>In MC, we continue sampling the rewards until the end of the episode, whereas in TD, we stop and bootstrap. We can also do something in the middle - that is, we can take a linear combination of rewards and bootstrapped value functions. This gives the $\lambda$-TD, which is another way of interpolating between MC ($\lambda=1)$ and TD ($\lambda=0$). The $\lambda$-TD can be represented as a weighted average of $n$-step returns: $G_t^\lambda=\sum_{n=1}^{T}(1-\lambda)\lambda^{n-1} G_t^{(n)}$.</p> <h2 id="n-step-td-vs-lambda-td">$n$-step TD vs. $\lambda$-TD</h2> <p><img src="/assets/img/blog/reinforcement-learning/untitled_deepmind_x_ucl_5_model-free_pr.png" alt="Comparing the $n$-step TD and $\lambda$-TD. The plots obtained from the $\lambda$-TD algorithm is similar to the $n$-step TD algorithm, especially when $n \approx 1/(1-\lambda)$. (Source: Deepmind X UCL Deep Reinforcement Learning Lecture 5, given by Prof. Hado van Hasselt.)"/></p> <p>Comparing the $n$-step TD and $\lambda$-TD. The plots obtained from the $\lambda$-TD algorithm is similar to the $n$-step TD algorithm, especially when $n \approx 1/(1-\lambda)$. (Source: Deepmind X UCL Deep Reinforcement Learning Lecture 5, given by Prof. Hado van Hasselt.)</p> <p>The $n$-step TD and the $\lambda$-TD are both interpolations between MC and TD, and they share commonalities. In fact, you can think of the value $1/(1-\lambda)$ as the “horizon” of the $\lambda$-TD in the sense that the $n$-step TD and $\lambda$-TD yield similar results when $n \approx 1/(1-\lambda)$. The $n$-step TD and the $\lambda$-TD both have intermediate bias and intermediate variance. Typically, intermediate values of $n$ and $\lambda$ are good as they trade off bias and variance in an appropriate way, e.g. $n=10$, $\lambda=0.9$. This gives a good starting point for training RL algorithms.</p> <h2 id="eligibility-traces-advanced">Eligibility Traces (Advanced)</h2> <h3 id="motivation-independence-of-temporal-span">Motivation: Independence of Temporal Span</h3> <p>In MC updates and $n$-step TD / $\lambda$-TD updates, the update depends on the temporal span of each episode. Having to wait until the end of the episode is problematic, because it prevents us from online learning (i.e., learning as new data becomes available). Can we implement MC in an online learning setup?</p> <h3 id="prerequisite-linear-function-approximation">Prerequisite: Linear Function Approximation</h3> <p>The tabular value function can be written as an inner product between the one-hot feature vector $\mathbf{x}(s)$ and some weight vector $\mathbf{w}$, for any state $s$:</p> \[v_\mathbf{w} (s)= \mathbf{w}^{T}\mathbf{x}(s)\] <p>If we want to update the values for a state $s=S_t$ using MC, we update the weight acoording to the following:</p> \[\Delta \mathbf{w} = \alpha(G_t-v(S_t))\mathbf{x}(S_t)\] <p>Normally, we cannot update the values of states in the middle of the $k$-th episode. Instead, we have to update it later, simultaneously:</p> \[\Delta \mathbf{w}_{k+1} = \sum_{t=0}^{T-1} {\alpha (G_t-v(S_t))\mathbf{x}(S_t)}\] <p>But what’s interesting is that we can split the MC error $G_t-v(S_t)$ into two parts:</p> <ol> <li>the TD error term, $\delta_t := R_{t+1}+\gamma v(S_{t+1})-v(S_t)$,</li> <li>and the non-TD error term, $\gamma (G_{t+1}-v(S_{t+1}))$.</li> </ol> \[\begin{aligned} G_t-v(S_t) &amp;= R_{t+1}+\gamma G_{t+1}-v(S_t)\\ &amp;= R_{t+1}+\gamma G_{t+1}-v(S_t) + \gamma v(S_{t+1}) - \gamma v(S_{t+1})\\ &amp;= (R_{t+1}+\gamma v(S_{t+1})-v(S_t)) + \gamma (G_{t+1} - v(S_{t+1})) \\ &amp;= \delta_t+\gamma(G_{t+1} -v(S_{t+1}))\end{aligned}\] <p>Let’s utilize this discovery to our advantage. Note that the non-TD error term turns out to be the discounted MC error term for the next time step. Continuing the recursion, we obtain the following:</p> \[\begin{aligned} G_t-v(S_t) &amp;= \delta_t+\gamma(G_{t+1} - v(S_{t+1})) \\&amp;=\delta_t +\gamma\delta_{t+1}+\gamma^2(G_{t+2}- v(S_{t+2})) \\&amp;= \cdots \\&amp;=\sum_{k=t}^{T-1} {\gamma^{k-t}\delta_k} \end{aligned}\] <p>Now, let’s plug this into the updating equation and change the order of summation:</p> \[\begin{aligned} \Delta \mathbf{w}_{k+1} &amp;= \sum_{t=0}^{T-1} {\alpha (G_t-v(S_t))\mathbf{x}(S_t)} \\ &amp;= \sum_{t=0}^{T-1} {\alpha \left(\sum_{k=t}^{T-1} {\gamma^{k-t}\delta_k}\right)\mathbf{x}(S_t)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\left(\sum_{t=0}^{k} {\gamma^{k-t}}\mathbf{x}(S_t)\right)} \end{aligned}\] <p>Defining the eligibility trace $\mathbf{e}_{k} := \sum_{t=0}^{k} {\gamma^{k-t}}\mathbf{x}(S_t)$ and renaming the summation index $k$ to $t$, we have:</p> \[\begin{aligned} \Delta \mathbf{w}_{k+1} &amp;= \sum_{k=0}^{T-1} {\alpha\delta_k\mathbf{e}_k} \\ &amp;= \sum_{t=0}^{T-1} {\alpha\delta_t\mathbf{e}_t}, \end{aligned}\] <p>plus the recursion relation of the eligibility trace:</p> \[\mathbf{e}_t=\gamma\mathbf{e}_{t-1}+\mathbf{x}_t\] <p>What’s “magical”, as Hado mentions in the lecture, is that the term $\alpha\delta_t\mathbf{e}_t$ now does not involve future terms at all! Therefore, we can now update the weights online, and obtain (almost) the same results as the original MC.</p> <p>Even if we choose not to update the values online and instead accumulate the summation until the end of the episode, the required memory remains independent of the episode’s duration. In this case, the result will exactly equal the result of the original MC.</p> <p>By altering the recursion relation as follows, we can generalize this method to the $\lambda$-TD case:</p> \[\tilde{\mathbf{e}}_t=\gamma\lambda\tilde{\mathbf{e}}_{t-1}+\mathbf{x}_t.\] <p>The derivation is similar:</p> \[\begin{aligned} G_t^\lambda-v(S_t) &amp;= R_{t+1}+\gamma((1-\lambda)v(S_{t+1})+\lambda G_{t+1}^\lambda)-v(S_t)\\ &amp;= R_{t+1}+\gamma((1-\lambda)v(S_{t+1})+\lambda G_{t+1}^\lambda)-v(S_t)+ \gamma\lambda v(S_{t+1}) - \gamma\lambda v(S_{t+1})\\ &amp;= (R_{t+1}+\gamma v(S_{t+1})-v(S_t)) + \gamma\lambda (G_{t+1}^\lambda - v(S_{t+1})) \\ &amp;= \delta_t+\gamma\lambda(G_{t+1}^\lambda - v(S_{t+1}))\\&amp;=\delta_t +\gamma\lambda\delta_{t+1}+\gamma^2\lambda^2(G_{t+2}^\lambda - v(S_{t+2})) \\&amp;= \cdots \\&amp;=\sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k},\\\Delta \mathbf{w}_{k+1} &amp;= \sum_{t=0}^{T-1} {\alpha (G_t^\lambda-v(S_t))\mathbf{x}(S_t)} \\ &amp;= \sum_{t=0}^{T-1} {\alpha \left(\sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k}\right)\mathbf{x}(S_t)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\left(\sum_{t=0}^{k} {(\gamma\lambda)^{k-t}}\mathbf{x}(S_t)\right)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\tilde{\mathbf{e}}_t} \\\text{where}\\ \tilde{\mathbf{e}}_t &amp;:= \sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k}. \end{aligned}\]]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 4. Theoretical Fundamentals of Dynamic Programming</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam/" rel="alternate" type="text/html" title="DeepMind X UCL | 4. Theoretical Fundamentals of Dynamic Programming"/><published>2023-08-09T10:00:00+00:00</published><updated>2023-08-09T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam/"><![CDATA[<h2 id="the-banach-fixed-point-theorem">The Banach Fixed Point Theorem</h2> <p>Let $X$ be a complete normed vector space, equipped with a norm $|\cdot|$ and $T:X \rightarrow X$ a $\gamma$-contraction mapping, then:</p> <ol> <li>$T$ has a unique fixed point $x^\star \in X$ s.t. $T x^\star=x^\star$</li> <li> <p>$\forall x_0 \in X$, the sequence $x_{n+1}=Tx_n$ converges to $x^\star$ in a geometric fashion:</p> \[\|x_n-x^\star\| \le \gamma^n\|x_0-x^\star\|\] <p>Thus, $\lim_{n\rightarrow\infty}|x_n-x^\star|\le \lim_{n\rightarrow\infty}\gamma^n|x_0-x^\star|=0.$</p> </li> </ol> <h2 id="what-does-this-mean">What does this mean?</h2> <p>It means that if the distance between two points after applying some operator $T$ is no greater than the original distance multiplied by $\gamma \in [0, 1)$, then applying the operator repeatedly to any point $x_0$ to yield $x_n$ is a great way to search for the unique fixed point of the operator $T$.</p> <h2 id="definitions-of-the-bellman-operators">Definitions of the Bellman Operators</h2> <h3 id="definition-bellman-optimality-operator-t_vstar">Definition: Bellman Optimality Operator $T_V^\star$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $V=V_S$ be the space of bounded real-valued functions over $S$. We define, point-wise, the Bellman Optimality Operator $T_V^\star:V\rightarrow V$ as:</p> \[(T_V^\star f)(s)=\max_{a \in A} \biggl[ {r(s, a) + \gamma \mathbb{E} \left[f(s')|s, a\right]} \bigg], \;\forall f \in V\] <p>Sometimes we drop the index and use $T^\star=T_V^\star$.</p> <h3 id="what-is-this-operator">What is this operator?</h3> <p>This operator is a $\gamma$-contraction with the unique fixed point being $f=v^\star$, the optimal value function of the MDP. Therefore, if we apply this operator iteratively to some (value) function $v$, it will converge to the optimal value function $v^\star$. This is why we attempt to approximate this specific operator (possibly with a neural network).</p> <h3 id="definition-bellman-expectation-operator-tpi_v">Definition: Bellman Expectation Operator $T^\pi_V$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $V=V_S$ be the space of bounded real-valued functions over $S$. For any policy $\pi:S \times A \rightarrow [0, 1]$, we define, point-wise, the Bellman Expectation Operator $T_V^\pi:V\rightarrow V$ as:</p> \[(T_V^\pi f)(s)=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s') \bigg| s \bigg], \;\forall f \in V\] <p>Note: By the tower rule, this is equivalent to:</p> \[(T_V^\pi f)(s)=\mathbb{E}^\pi \biggl[ {r(s, a) + \gamma \mathbb{E} \left[f(s')|s, a\right]} \bigg| s \bigg], \;\forall f \in V\] <table> <tbody> <tr> <td>Which is the same as the Bellman Optimality Operator, except $\max_{a\in A}$ being replaced by $\mathbb{E}^\pi[\cdot</td> <td>s]$.</td> </tr> </tbody> </table> <p>Sometimes we drop the index and use $T^\pi=T_V^\pi$.</p> <h3 id="what-is-this-operator-1">What is this operator?</h3> <p>Same as the Bellman Optimality Operator, this operator is a $\gamma$-contraction except the unique fixed point being $f=v^\pi$, the value function for policy $\pi$ in a given MDP. Therefore, we can evaluate the policy $\pi$ by repeatedly applying this operator to the initial (value) function $v$. We then know whether the policy $\pi$ is doing well or not.</p> <h3 id="definition-bellman-optimality-operator-t_qstar">Definition: Bellman Optimality Operator $T_Q^\star$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $Q=Q_{S, A}$ be the space of bounded real-valued functions over $S\times A$. We define the Bellman Optimality Operator $T_Q^\star:Q\rightarrow Q$ as:</p> \[(T_Q^\star f)(s, a)=\mathbb{E} \bigg[ r(s, a) + \gamma \max_{a'\in A}f(s',a') \bigg| s, a \bigg], \;\forall f \in Q\] <p>Note: You can push the expectation inside to get</p> \[(T_Q^\star f)(s,a)={r(s, a) + \gamma \mathbb{E} \left[\max_{a'\in A} f(s',a')\bigg|s, a\right]}, \;\forall f \in Q\] <h3 id="what-is-this-operator-2">What is this operator?</h3> <p>This is the q-version of the previous Bellman Optimality Operator $T_V^\star$. Similarly, this operator is a $\gamma$-contraction with the unique fixed point being $f=q^\star$, the optimal q-value function of the MDP. Therefore, if we apply this operator iteratively to some (value) function $q$, it will converge to the optimal value function $q^\star$. We may attempt to approximate this operator too.</p> <h3 id="definition-bellman-expectation-operator-tpi_q">Definition: Bellman Expectation Operator $T^\pi_Q$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $Q=Q_{S, A}$ be the space of bounded real-valued functions over $S\times A$. For any policy $\pi:S \times A \rightarrow [0, 1]$, we define, point-wise, the Bellman Expectation Operator $T_Q^\pi:Q \rightarrow Q$ as:</p> \[(T_Q^\pi f)(s, a)=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s',a') \bigg| s, a \bigg], \;\forall f \in Q\] <p>Note: You can push the expectation inside to get</p> \[(T_Q^\pi f)(s, a) = r(s, a) + \gamma \mathbb{E^\pi} \bigg[ f(s',a')\bigg|s, a\bigg], \;\forall f \in Q\] <h3 id="what-is-this-operator-3">What is this operator?</h3> <p>This is the q-version of the previous Bellman Expectation Operator $T_V^\pi$. It is also a $\gamma$-contraction, with the unique fixed point being $f=q^\pi$. Therefore, we can evaluate the policy $\pi$ by repeatedly applying this operator to the initial value function $q$. We then know the performance of the policy $\pi$. Since this is a q-value function, we can also use it to greedify our policy $\pi$ by $\pi \leftarrow \operatorname{argmax}_{a\in A} q^\pi(s, a)$.</p> <h2 id="properties-of-the-bellman-operators">Properties of the Bellman Operators</h2> <h3 id="properties-bellman-optimality-operator-t_vstar--tstar-">Properties: Bellman Optimality Operator $T_V^\star \;(= T^\star )$</h3> <ol> <li>$T^\star$ has a unique fixed point $v^\star$.</li> <li> <p>$T^\star$ is a $\gamma$-contraction with respect to $|\cdot|_\infty$:</p> \[\|T^\star v-T^\star u\|_\infty \le \gamma \|v-u\|_\infty, \forall u,v \in V\] </li> <li>$T^\star$ is monotonic:</li> </ol> \[\forall u,v \in V \text{ s.t. } u \le v \text{ component-wise, then } T^\star u \le T^\star v\] <p><strong>The properties are similar for all other operators.</strong></p> <h2 id="approximate-dp">Approximate DP</h2> <p>So far, we have assumed perfect knowledge of the MDP &amp; perfect/exact representation of the value functions. However, we often encounter situations where we don’t know the underlying MDP or cannot represent the value function exactly after each update.</p> <p>Therefore, we will have to use approximate versions of the value functions / Bellman Operators. However, when the approximation is really bad, iteratively applying the approximated Bellman Operator to an initial function may not guarantee convergence.</p> <p>An example of divergence induced by some approximation is explored in the lecture. However, in most cases, divergence is not an issue. In the lecture, it is mentioned that “sample versions of these algorithms converge under mild conditions, and even for the function approximation case, the theoretical danger of divergence is rarely materialised in practice.”</p> <h2 id="theorem-value-of-a-greedy-policy">Theorem (Value of a greedy Policy)</h2> <p>Consider an MDP. Let $q:S\times A \rightarrow \mathbb{R}$ be an arbitrary function and let $\pi$ be the greedy policy associated with $q$, then:</p> \[\|q^\star - q^\pi \|_\infty \le \frac{2\gamma}{1-\gamma} \|q^\star-q\|_{\infty}\] <p>where $q^\star$ is the optimal value function associated with this MDP.</p> <p>We can gain insights from this theorem:</p> <ol> <li>Small values of $\gamma$ give a better (lower) upper bound for the potential loss of the performance. (Why?)</li> <li>If $\gamma=0$, then $q^\star=q^\pi$. Therefore, the greedy policy associated with any $q$ yields the optimal value function.</li> <li>If $q=q^\star$, it means that the value function, from which you are about to make the greedy policy out of, is the optimal value function. The greedy policy is the optimal policy, hence $q^\star=q^\pi$ in this case.</li> </ol>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">Reinforcement Learning Basics</title><link href="https://codingjang.github.io/blog/2023/reinforcement-learning-basics/" rel="alternate" type="text/html" title="Reinforcement Learning Basics"/><published>2023-02-01T10:00:00+00:00</published><updated>2023-02-01T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/reinforcement-learning-basics</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/reinforcement-learning-basics/"><![CDATA[<p>This series consists of lecture notes from the DeepMind X UCL reinforcement learning course and an introduction to the PettingZoo library.</p> <h2 id="deepmind-x-ucl-deep-rl-series">DeepMind X UCL Deep RL Series</h2> <ul> <li><a href="/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam/">DeepMind X UCL | 4. Theoretical Fundamentals of Dynamic Programming (8/9)</a></li> <li><a href="/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/">DeepMind X UCL | 5. Model-free Prediction (8/16)</a></li> <li><a href="/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c/">DeepMind X UCL | 6. Model-free Control (8/23)</a></li> <li><a href="/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13/">DeepMind X UCL | 7. Function Approximation (8/23)</a></li> </ul> <h2 id="other-resources">Other Resources</h2> <ul> <li><a href="/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466/">Introduction to PettingZoo</a></li> </ul>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="DeepMind"/><category term="tutorial"/><category term="english"/><summary type="html"><![CDATA[DeepMind X UCL reinforcement learning series notes and PettingZoo introduction]]></summary></entry><entry><title type="html">Day 8: Day 8 일변수 최적화 문제와 경사하강법</title><link href="https://codingjang.github.io/blog/2023/dl-day8-day-8-day-8/" rel="alternate" type="text/html" title="Day 8: Day 8 일변수 최적화 문제와 경사하강법"/><published>2023-01-29T10:00:00+00:00</published><updated>2023-01-29T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/dl-day8-day-8-day-8</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/dl-day8-day-8-day-8/"><![CDATA[<h3 id="일변수-최적화-문제">일변수 최적화 문제</h3> <p>앞서 살펴본 예시에서 중학생 철수의 뇌에는 수많은 시냅스 연결 상태가 하나의 벡터 $\theta$로 표현되었고, 우리는 이 벡터를 변경해가며 최적해 $\theta_0$를 찾는 중에 있다. 너무 복잡한 상황이므로, 먼저 간단하게 만들어서 최대한 쉽게 바라볼 필요가 있다. 따라서 가장 쉬운 경우인 $n=1$인 케이스부터 다뤄보자. 일변수함수 $f(\theta)$를 어떻게 최적화할 수 있을까? 아래의 표준형 최적화 문제를 살펴보자.</p> \[\begin{equation}\begin{aligned}&amp; \underset{\theta \in \mathbb{R}}{\text{minimize}}&amp; &amp; f(\theta)=\theta^2 \end{aligned}\end{equation}\] <p>$\text{subject to}$ 문구가 없으므로 제약 조건은 특별히 없는 상태다. 우리는 위의 함수를 최소화하는 문제를 이미 중학교 때 해결해 본 경험이 있다. 그때는 실수의 제곱 $x^2$이 항상 $0$ 이상이고, $x^2=0$이 되는 필요충분조건이 $x=0$이라는 사실을 받아들이고 사용했을 것이다. 이 두 사실을 알고 있다면 $x^2$의 최솟값이 $0$이라는 사실은 쉽게 도출된다.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> 고등학교 수준으로 올라가면 이계도함수 판정법과 같은 조금 더 고차원적인 도구들을 활용하여 위의 문제를 접근하는 것도 가능해진다. (아니면 그냥 저 함수의 최솟값이 $0$이라는 사실을 외워버렸을 수도 있겠다…)</p> <p>아무튼 위의 케이스는 너무 간단한 케이스라 최적해를 찾는 것이 그다지 어려운 일이 아니다. 하지만, 현실 문제는 이처럼 간단하지 않고 공식으로 외울 수 있는 최적해따윈 존재하지 않는 경우가 대부분이다. 미분해서 $0$이 되는 지점을 살펴보는 것은 그나마 일반적인 상황까지 적용할 수 있는 강력한 방법 중 하나이지만, 방정식 $f’(\theta)=0$이 일반해를 찾을 수 없는 방정식이라면 이또한 무용지물이지 않은가?<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> 그러니 위 사실을 모르는 상황에서도 최적화 문제를 해결할 수 있는 방법이 필요하다. 함수의 형태에 구애받지 않고 함수값을 최소화하려면 어떤 방법을 사용해야 할까?</p> <h3 id="일변수-경사하강법">일변수 경사하강법</h3> <p>$f’(\theta)=0$이 되는 지점을 전체 실수 구간에서 탐색하는 것은 어렵지만, 한 지점에서의 $f’(\theta)$을 계산하여 그 부호를 확인하는 것은 쉽다. 따라서 $f’(\theta)$가 $0$보다 크면 $\theta$를 조금 작게 하고, $f’(\theta)$가 $0$보다 작으면 $\theta$를 조금 키우는 과정을 계속 반복해보자. 이 과정을 계속하다보면 $f(\theta)$는 계속 작아지게 된다.</p> <p><img src="/assets/img/blog/deep-learning/untitled.jpeg" alt="Untitled"/></p> <p>이 과정을 수식으로 표현하면 아래와 같다:</p> \[\begin{equation}\theta^{(k+1)}=\theta^{(k)}-\alpha\:\text{sgn}\:f'(\theta^{(k)}) \end{equation}\] <p>여기서 $\alpha$는 충분히 작은 양수이고, $\text{sgn} \;\cdot$이라고 나타낸 것은 $\cdot$의 부호를 뜻한다.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> 또한, $\theta^{(k)}$라고 적은 것은 $k$번째 단계step에서의 $\theta$를 의미한다. 앞서 이야기한 직관이 틀리지 않았다면, 실변수 $\theta$는 가장 처음 단계인 $k=0$ 단계에서 초기값 $\theta^{(0)}$로 출발하여, 위의 재귀식recurrence relation에 의해 최소값을 향해 점차 다가갈 것이다. 구체적으로, $f(\theta)$가 이상한 함수가 아니라면 $\theta$는 $f(\theta)$의 골짜기 지점인 $\theta=0$로 다가가게 된다. $k$의 값이 증가함에 따라 $\theta^{(k)}$가 (일반적으로) 감소할 것임을 확인해보자.</p> <p>하지만, 위의 방식에는 한 가지 문제점이 있다. $\alpha\:\text{sgn}\:f’(\theta^{(k)})$의 크기</p> \[\left|\alpha\:\text{sgn}\:f'(\theta^{(k)})\right|=\left|\alpha \cdot(\pm1)\right|\] <p>는 항상 $\alpha$이기 때문에, 최소값에 다다를 때 즈음에도 계속 크기-$\alpha$의 step-size로만 $\theta^{(k)}$를 변경하게 된다. 즉, 최소값 근처에 다가갈 때는 서서히 감속하여 정확한 위치에 수렴할 수 있도록 해야 하는데, 부호만을 따지는 위의 상황에서는 최소값으로 수렴하지 못하고 크기-$\alpha$로 진동하게 되는 것이다!</p> <p>한편, 최소값 근처에 다가왔다는 것은 $\lvert f’(\theta)\rvert$의 크기가 작아졌다는 사실, 즉 $f’(\theta)$이 $0$에 가까워졌다는 것으로부터 짐작할 수 있다. 이 정보를 활용하여 위의 식을 개선할 수는 없을까? 사실 곰곰히 생각해보면 $f’(\theta^{(k)})$가 이미 부호를 가지는 실수이기 때문에 $\text{sgn}$ 함수를 반드시 사용할 필요는 없을 것 같다. 따라서 $\text{sgn}$ 함수를 제거하면 아래의 식을 얻는다:</p> \[\begin{equation} \theta^{(k+1)}=\theta^{(k)}-\alpha f'(\theta^{(k)}) \end{equation}\] <p>위의 식은 앞선 식 $(3)$에서 step-size가 $\alpha$로 고정되어 있는 문제를 해결한다. $f’(\theta^{(k)})$의 크기(절대값)가 감소하면 $\alpha f’(\theta^{(k)})$의 크기도 감소한다. 움직이는 방향은 이전 경우와 똑같이 $f$의 값을 감소시키는 방향이다. 따라서 좀 전의 진동 문제는 어느 정도 해결된 듯하다. 식 $(4)$와 같은 방식으로 함수를 최적화하는 것을 경사하강법Gradient Descent, GD이라고 부른다.</p> <p>$f(\theta)=\theta^2$와 같이 간단한 함수의 경우에는 (놀랍게도) 고등학교에서 배우는 수열과 미분의 개념만을 활용하여 $\theta^{(k)}$가 $f(\theta)$의 값이 가장 작아지는 $\theta=0$ 상태로 수렴할 조건을 구하고 증명할 수 있다. $\alpha=0.01$로 두고, $\theta^{(0)}=1$에서 출발한다고 가정하자. $f’(\theta)=2\theta$이므로 이를 기존 식에 대입하면</p> \[\begin{equation} \begin{aligned} \theta^{(k+1)}&amp;=\theta^{(k)}-0.02\:\theta^{(k)}=0.98\:\theta^{(k)}, \\ \theta^{(k)}&amp;=0.98^k, \\ \theta^{(0)}&amp;=0.98^k. \end{aligned} \end{equation}\] <p>$0&lt;0.98&lt;1$이므로, 실수열 $\theta^{(k)}$는 $k$가 $\infty$로 갈 때 $0$으로 수렴한다.</p> \[\theta^{(k)}=0.98^k \rightarrow 0 \;\;\text{as}\;\; k\rightarrow \infty.\] <p>하지만 $\alpha$의 크기가 너무 클 경우에는 $\theta^{(k)}$의 수열이 발산할 수도 있다! 예를 들어, $\alpha=2$이고 $\theta^{(0)}=1$로 동일한 경우에는</p> \[\begin{equation}\begin{aligned}\theta^{(k+1)}&amp;=\theta^{(k)}-4\:\theta^{(k)}=(-3)\,\theta^{(k)},\\ \theta^{(k)}&amp;=(-3)^k \theta^{(0)}=(-3)^k .\end{aligned}\end{equation}\] <p>위의 경우 실수열 $\theta^{(k)}$는 $k$가 $\infty$로 갈 때 진동하며 발산한다.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> \[\theta^{(k)}=(-3)^k \;\;\text{oscillates as}\;\; k\rightarrow \infty.\] <h3 id="문제-21">문제 2.1</h3> <p>$f(\theta)=\theta^2$에 경사하강법을 적용할 때, $\theta^{(k)}$가 최적해 $\theta=0$으로 수렴하기 위한 $\alpha, \theta^{(0)}$의 조건은 무엇인가? 단, $\alpha&gt;0$이다.</p> <ul> <li> <p>정답</p> <p>$0&lt;\alpha&lt;1$ 또는 $\theta^{(0)}=0.$</p> </li> </ul> <h3 id="문제-22">문제 2.2</h3> <p>$f(\theta)=(\theta+3)^2$에 경사하강법을 적용해보자. 구체적으로, $\alpha=0.7$로 두고 $\theta^{(0)}=-4$인 조건에서 출발하여 식 $(4)$에 의해 $\theta$의 값을 업데이트한다고 할 때, $\theta^{(0)}$부터 $\theta^{(30)}$까지의 $\theta$값을 리스트로 만든 다음 <code class="language-plaintext highlighter-rouge">print()</code>함수를 이용하여 출력하시오.</p> <ul> <li> <p>정답</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># 코드
</span>    
  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span>
  <span class="n">theta</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
  <span class="n">thetas</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># 비어있는 리스트 생성
</span>  <span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>      <span class="c1"># 초기 theta 값 저장하기
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
      <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 경사하강
</span>      <span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>             <span class="c1"># theta값 출력
</span></code></pre></div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># 출력값
</span>    
  <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.16</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.936</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.98976</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.004096</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9983616</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.00065536</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999737856</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0001048576000002</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999580569599997</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000016777216</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999932891136</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.00000268435456</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999998926258176</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000004294967297</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999998282013083</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000687194768</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999725122093</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000109951164</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999956019536</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000017592185</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999992963127</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000281475</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.99999999988741</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000045036</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999999819855</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000007206</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999999999997118</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000001153</span><span class="p">]</span>
</code></pre></div> </div> </li> </ul> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>함수의 최솟값을 찾았다는 것은 함수를 최소화하는 최적해 $\theta_0$가 존재하여 정의역 내의 임의의 $\theta$에 대해 $f(\theta)\ge f(\theta_0)$인 것이다. $\theta_0=0$로 두면 앞서 밝힌 두 사실에 의해 명제가 성립한다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>예를 들어, $f(\theta)=c_6\theta^6+c_5\theta^5+\cdots+c_1\theta+c_0$와 같은 $6$차 방정식일 경우, $c_6&gt;0$일 때 최솟값이 실수 집합 어딘가에 존재한다는 사실은 알지만, $f’(\theta)=6c_6\theta^5+5c_5\theta^4+\cdots+c_1=0$은 $5$차 방정식이다. 아벨과 갈루아는 $5$차 이상의 다항방정식은 근의 공식이 존재하지 않음을 밝혀냈다. 따라서, 특수한 경우를 제외하고는 $f’(\theta)=0$을 풀 수 없다. 굳이 5차 이상의 다항방정식이 아니더라도 보통 ‘미분해서 $0$되는 지점 찾기’ 전략은 방정식이 조금이라도 복잡해지면 잘 통하지 않는다. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>$\cdot$이 양수라면 $+1$, 음수라면 $-1$. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>GD가 ‘삐딱하게’ 구는 케이스… <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="education"/><category term="deep-learning"/><category term="tutorial"/><category term="korean"/><category term="education"/><category term="series"/><summary type="html"><![CDATA[딥러닝의 기초 - Day 8]]></summary></entry><entry xml:lang="en"><title type="html">Gradient Descent</title><link href="https://codingjang.github.io/blog/2023/gradient-descent/" rel="alternate" type="text/html" title="Gradient Descent"/><published>2023-01-29T10:00:00+00:00</published><updated>2023-01-29T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/gradient-descent</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/gradient-descent/"><![CDATA[<h1 id="gradient-descent">Gradient Descent</h1> <h2 id="what-is-an-optimization-problem">What is an Optimization Problem?</h2> <p>An <strong>optimization problem</strong> seeks the <strong>optimal solution</strong> from all possible solutions. For instance, a researcher at an IT company developing navigation software would aim to provide users with the “fastest traffic route”. Similarly, someone with a background in industrial engineering might look for a “business model that maximizes profitability”. Both are tackling optimization problems. There are multiple ways to travel from point $A$ to $B$ using public transportation. Similarly, countless business models can make a company profitable. However, people naturally desire the quickest route and the most profitable business model. In this context, optimization problems are very familiar to computer scientists, industrial engineers, and even business and economics professionals.</p> <p>To determine the best or <strong>optimal</strong> state, we need a standard for what it good and what is not. Optimization problems are usually represented by a function that measures “how good” some task is done, called the <strong>objective function</strong>. The goal in optimization problems is to maximize or minimize this objective function. For example, consider a middle school student, Cheolsu, taking his first math test. If we frame his situation as an optimization problem, the test score becomes the objective function, and the optimization problem becomes “maximizing the test score”. If he does exceptionally well, he can score $100$, making the optimal solution when the test score (objective function) equals $100$.</p> <p>But what exactly is this “objective function” a function of? In Cheolsu’s case, the synaptic connections in his brain, represented by an $n$-dimensional vector $\theta$, determine his test score $s$. Hence, the test score, which is also the objective function $s$, can be considered a function of $\theta$. Cheolsu probably has a vast number of synapses, so $n$ would be a very large number. If cheolsu is smart enough, he can adjust his synaptic connections through learning to maximize his test score $s(θ)$. If he scores $100$ on the test, then there exists a $θ_0$ such that $s(θ_0) = 100$, making $θ_0$ the optimal solution.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>In general, the objective function in optimization problems is usually a function from $\mathbb{R}^n$ to $\mathbb{R}$, where $\mathbb{R}^n$ represents an $n$-dimensional real vector space. Although there may be various ways to express optimization problems, <strong>the standard form (continuous) optimization problem</strong> is usually represented as follows:</p> \[\begin{equation}\begin{aligned}&amp; \underset{\theta \in \mathbb{R}^n}{\text{minimize}}&amp; &amp; f(\theta) \\&amp; \text{subject to}&amp; &amp; g_i(\theta) \leq 0, \; i = 1, \ldots, m.\\&amp;&amp;&amp; h_j(\theta) = 0, \; j = 1, \ldots, p.\end{aligned}\end{equation}\] <p>Here, the objective function is called $f$, and the constraint functions are called $g_i$ and $h_j$ for $i=1,\dots,m$ and $j=1,\dots,p$. All of these functions maps $\mathbb{R}^n$ to $\mathbb{R}$, which means that it takes in as input an $n$-dimensional vector and outputs a single number. Unlike what was mentioned earlier about maximizing the test score, it’s traditional in standard optimization problems to minimize the objective function $f$. In fact, maximizing and minimizing differ only by a sign. To convert a maximization problem into a minimization problem, simply put a minus sign in front of the objective function. Instead of maximizing Cheol-su’s test score $s(\theta)$, if we set $-s(\theta)$ as $f(\theta)$ and mimimize it, the test score maximization problem can be represented in the standard form.</p> <p>The functions $g_i$ and $h_j$ represent <strong>inequality and equality constraints</strong>, respectively. For instance, consider a case where a neuron can be damaged if the synaptic connection strength becomes too strong. In such a case, we can set $g_i(\theta):= \theta_i -M\;(M&gt;0)$. Alternatively, let’s say that studying something too difficult for an exam can worsen one’s quality of life (QoL). In that case, we can record the previous QoL as $T_i$ and obtain the current QoL as $t_i(\theta)$. Then, we can set $h_i(\theta):= t_i(\theta)-T_i$. If these examples are hard to understand, remember that both the objective function and constraints are chosen appropriately based on real-life situations, and there’s no fixed standard.</p> <h2 id="single-variable-optimization-problems"><strong>Single-variable Optimization Problems</strong></h2> <p>In the previous example, Cheolsu’s brain had numerous synaptic connections represented by a vector $θ$. We’re trying to find the optimal $θ_0$ by adjusting this vector. To simplify, let’s start with the simplest case where $n=1$ and gradually generalize. How can we optimize a single-variable function $f(θ)$? Consider the standard optimization problem:</p> \[\begin{equation}\begin{aligned}&amp; \underset{\theta \in \mathbb{R}}{\text{minimize}}&amp; &amp; f(\theta)=\theta^2 \end{aligned}\end{equation}\] <p>Since there’s no “subject to” phrase, there are no specific constraints. We’ve already tackled the problem of minimizing the above function back in middle school. At that time, we accepted and used the fact that the square of a real number, $x^2$, is always non-negative and that the necessary and sufficient condition for $x^2$ to be zero is when $x=0$. Knowing these two facts, it’s easy to deduce that the minimum value of $x^2$ is $0$.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> As we progress to high school level, we can approach this problem using more advanced tools like the second derivative test. (Or, we might have just memorized that the minimum value of that function is $0$…)</p> <p>In any case, the above scenario is a very simple one, so finding the optimal solution isn’t particularly challenging. However, real-world problems aren’t always this straightforward. Most of the time, there isn’t an optimal solution that can be memorized using a formula. And although it is true that examining the point where the derivative becomes zero is a powerful method applicable to many general situations, there are cases where the equation $f’(\theta)=0$ becomes unsolvable and doesn’t lead to the general solution.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> In that case, we need a way to solve optimization problems without having to solve the above equation. How can we minimize the function value without being constrained by the function’s form?</p> <h2 id="univariate-gradient-descent">Univariate Gradient Descent</h2> <p>It’s challenging to search for points where $f’(\theta) = 0$ across the entire range of real numbers, but it’s fairly easy to calculate $f’(\theta)$ at a particular point and check its sign. If so, it seems like it’s good idea to slightly decrease $\theta$ whenever $f’(\theta)$ is greater than $0$. Similarly, let’s slightly increase $\theta$ whenever $f’(\theta)$ is less than $0$. If we continue this process, $f(\theta)$ will keep decreasing.</p> <p><img src="/assets/img/blog/deep-learning/untitled.jpeg" alt="Untitled"/></p> <p>The process can be expressed with the following equation:</p> \[\begin{equation}\theta^{(k+1)}=\theta^{(k)}-\alpha\:\text{sgn}\:f'(\theta^{(k)})\end{equation}\] <p>In this, $\alpha$ is a sufficiently small positive number. The notation $\text{sgn} \;\cdot$ represents the sign of $\cdot$.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> Also, $\theta^{(k)}$ denotes the value of $\theta$ at the $k$-th step. If our intuition is correct, the real variable $\theta$ starts from its initial value $\theta^{(0)}$ at the very first step, $k=0$, and gradually approaches the minimum point according to the above recurrence relation. Specifically, unless $f(\theta)$ is an unusual function, $\theta$ will approach the global minimum point of $f(\theta)$, which is $\theta=0$. Let’s verify that as the value of $k$ increases, $\theta^{(k)}$ generally decreases.</p> <p>However, there’s a problem with the above approach. The magnitude of $\alpha\:\text{sgn}\:f’(\theta^{(k)})$,</p> \[\begin{equation} \left|\alpha\:\text{sgn}\:f'(\theta^{(k)})\right|=\left|\alpha \cdot(\pm1)\right| \end{equation}\] <p>is always $\alpha$. This means that even when approaching the minimum value, $\theta^{(k)}$ is updated only by a fixed step-size of $\alpha$. In other words, if we only consider the sign, we won’t converge to the minimum value and will instead be oscillating by a magnitude of $\alpha$! Therefore, we should gradually decelerate as we approach the exact minimum value.</p> <p>On the other hand, we can know when we’re close to the minimum value by noticing that the magnitude of $\lvert f’(\theta)\rvert$ has decreased, i.e., $f’(\theta)$ is close to $0$. Can we use this information to improve the equation? In fact, since $f’(\theta^{(k)})$ is already a real number with a sign, there’s no need to use the $\text{sgn}$ function. Removing the $\text{sgn}$ function gives:</p> \[\begin{equation} \theta^{(k+1)}=\theta^{(k)}-\alpha f'(\theta^{(k)})\end{equation}\] <p>This equation addresses the issue in the previous equation where the step-size was fixed at $\alpha$. As $\theta^{(k)}$ approaches the minimum value, the magnitude (absolute value) of $f’(\theta^{(k)})$ decreases, which means that we’ll be decelerating. The direction of movement will remains the same as before, reducing the value of $f$. Thus, it seems all the previous issues have been resolved. The method of optimizing a function as in the above equation is called Gradient Descent (GD).</p> <p>Surprisingly, for simple functions like $f(\theta)=\theta^2$ we can use concepts of sequences and differentiation learned in high school to determine and prove the conditions under which $\theta^{(k)}$ converges to the state $\theta=0$, the minimum point. Let’s assume $\alpha=0.01$ and start from $\theta^{(0)}=1$. Since $f’(\theta)=2\theta$, substituting this into the original equation gives:</p> \[\begin{align} \theta^{(k+1)}&amp;=\theta^{(k)}-0.02\:\theta^{(k)}=0.98\:\theta^{(k)},\\ \theta^{(k)}&amp;=0.98^k \theta^{(0)}=0.98^k. \end{align}\] <p>Since $0&lt;0.98&lt;1$, the sequence $\theta^{(k)}$ converges to $0$ as $k$ approaches infinity.</p> \[\begin{equation} \theta^{(k)}=0.98^k \rightarrow 0 \;\;\text{as}\;\; k\rightarrow \infty. \end{equation}\] <p>However, if the magnitude of $\alpha$ is too large, the sequence $\theta^{(k)}$ might diverge! For instance, if $\alpha=2$ and we start from $\theta^{(0)}=1$ as before:</p> \[\begin{equation} \begin{aligned} \theta^{(k+1)}&amp;=\theta^{(k)}-4\:\theta^{(k)}=(-3)\,\theta^{(k)}, \\ \theta^{(k)}&amp;=(-3)^k, \\ \theta^{(0)}&amp;=(-3)^k. \end{aligned} \end{equation}\] <p>In this case, the sequence $\theta^{(k)}$ oscillates and diverges as $k$ approaches infinity.</p> \[\begin{equation} \theta^{(k)}=(-3)^k \text{ oscillates as } k\rightarrow \infty. \end{equation}\] <p><strong>Check Problem 1.</strong></p> <p>When applying gradient descent to the function $f(\theta) = \theta^2$, what are the conditions for $\alpha$ and $\theta^{(0)}$ for $\theta^{(k)}$ to converge to the optimal solution $\theta = 0$? Note that $\alpha &gt; 0$.</p> <ul> <li>Answer $0&lt;\alpha&lt;1$ or $\theta^{(0)}=0.$</li> </ul> <p><strong>Coding Problem 1.</strong></p> <p>Let’s apply gradient descent to the function $f(\theta) = (\theta + 3)^2$. Specifically, with a learning rate of $\alpha = 0.7$ and starting from the condition $\theta^{(0)} = -4$, update the value of $\theta$ according to equation $(4)$. List the values of $\theta$ from $\theta^{(0)}$ to $\theta^{(30)}$ and use the <code class="language-plaintext highlighter-rouge">print()</code> function to display them.</p> <ul> <li>Answer <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code
</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">theta</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># Create an empty list
</span><span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>      <span class="c1"># Save initial theta
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># Apply gradient descent
</span>    <span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>             <span class="c1"># Print values for theta
</span></code></pre></div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Output
</span>
<span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.16</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.936</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.98976</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.004096</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9983616</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.00065536</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999737856</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0001048576000002</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999580569599997</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000016777216</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999932891136</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.00000268435456</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999998926258176</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000004294967297</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999998282013083</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000687194768</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999725122093</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000109951164</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999956019536</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000017592185</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999992963127</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000281475</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.99999999988741</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000045036</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999999819855</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000007206</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999999999997118</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000001153</span><span class="p">]</span>
</code></pre></div> </div> </li> </ul> <h2 id="local-minima-and-global-minima">Local Minima and Global Minima</h2> <p><img src="/assets/img/blog/deep-learning/untitled_day_9_161f0f24f931802a97f3e1b6.png" alt="Untitled"/></p> <p>The <strong>local minimum point</strong> of a function $f(\theta)$, denoted as $\theta_\text{local}$, is a point where the function value is less than or equal to the values in the neighborhood<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> of $\theta$, i.e., $f(\theta)\ge f(\theta_\text{local})$ for $\theta$ in the neighborhood of $\theta_\text{local}$. In the graph of the univariate real function $f$ above, with the horizontal axis as the $\theta$-axis, the points $\theta=0$ and $\theta=10$ are local minima points because their function values are always less than those of their neighboring points. The function value at a minimum point is called the minimum value. Thus, the function $f$ has a local minimum value of $f(0)=0$ at $\theta=0$ and a local minimum value of $f(10)=50$ at $\theta=10$.</p> <p>On the other hand, the <strong>global minimum point</strong> of the function $f(\theta)$, denoted as $\theta_\text{global}$, is a point where the function value is less than or equal to the values at all points in its domain, i.e., $f(\theta)\ge f(\theta_\text{global})$ for all $\theta$ within the domain of $f$. In the above case, $\theta=0$ is both a local and global minimum, but $\theta=10$ is only a local minimum. This is because the function value at $\theta=0$, which is $f(0)=0$, is less than the function value at $\theta=10$, which is $f(10)=50$.</p> <p>When we applied the gradient descent method to the function $f(\theta)=\theta^2$, we observed that if $\alpha$ is sufficiently small, the sequence $\theta^{(k)}$ converges well to the optimal solution $\theta=0$. However, for $f(\theta)=\theta^2$, as there’s only one local minimum (and because the function values shoot to $\infty$ as $\theta$ approaches $\pm \infty$), the local minimum is the same as the global minimum. But for functions like $f$ in the above graph with multiple local minima, it’s uncertain whether $\theta$ will converge to the global minimum, as it can get stuck in a minimum that is local but not global.</p> <p>For instance, in some cases, a company’s profit structure might be hard to improve with minor adjustments. If any slight modification to the current structure only worsens profitability, the company might need to completely overhaul its structure to expect significant profit growth. This can be interpreted as an effort to escape from a local minimum. Another example is misconceptions during learning. Even with misconceptions, one might still be able to explain phenomena adequately. However, as one encounters more information, it’s more effective to break away from the misconception and relearn, enhancing the ability to explain phenomena. In other words, it’s preferable to move away from the misconception (local minimum) and relearn a more accurate concept (global minimum).</p> <p>While gradient descent ensures convergence to a local minimum under appropriate assumptions, it doesn’t guarantee convergence to a global minimum.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> Hence, optimization algorithms like Momentum and Adam, which include inertial terms to escape shallow local minima and head towards deeper global minima with smaller function values, have been developed. If you’re interested in exploring various optimization techniques like Momentum and Adam, consider checking out the referenced post below after reading Part II of this series.</p> <p><a href="https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/">A Comprehensive Guide on Optimizers in Deep Learning</a></p> <p><strong>Check Problem 2.</strong></p> <p>For the quartic function $f(\theta) = \theta^2(\theta-1)(\theta-2)$:</p> <p>(a) Find the local minimum point.</p> <p>(b) Find the global minimum point.</p> <p>(c) Determine the minimum value at the point which is a local minimum but not a global minimum.</p> <ul> <li>Answer (a) local minimum point: $\theta=0,\;\theta=\frac{9+\sqrt{17}}{8}\approx1.640$ (b) global minimum point: $\theta =\frac{9+\sqrt{17}}{8}\approx1.640$ (c) $f(0)=0$</li> </ul> <p><strong>Coding Problem 2.</strong></p> <p>Let’s apply the gradient descent method to the quartic function $f(\theta) = \theta^2(\theta-1)(\theta-2)$.</p> <p>(a) With a learning rate of $\alpha = 0.05$ and starting from $\theta^{(0)} = 3$, update the value of $\theta$ using equation $(4)$. List the values of $\theta$ from $\theta^{(0)}$ to $\theta^{(30)}$ and print them using the <code class="language-plaintext highlighter-rouge">print()</code> function.</p> <p>(b) Check if $\theta$ converges to the global minimum. If it does, find the convergence value of $f(\theta^{(k)})$.</p> <p>(c) Identify one initial value of $\theta^{(0)}$ for which $\theta$ does not converge to the global minimum. Note that $\alpha = 0.05$.</p> <ul> <li>Answer (a) <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code
</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># Create empty list
</span><span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>      <span class="c1"># Save initial theta
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="c1"># Gradient descent
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">9</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
    <span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>             <span class="c1"># Print values of theta
</span></code></pre></div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 출력값
</span>
<span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.0499999999999998</span><span class="p">,</span> <span class="mf">1.1045999999999998</span><span class="p">,</span> <span class="mf">1.1631899369327998</span><span class="p">,</span> <span class="mf">1.2246451065084327</span><span class="p">,</span> <span class="mf">1.2872724414731267</span><span class="p">,</span> <span class="mf">1.3488794094514513</span><span class="p">,</span> <span class="mf">1.4070169240305987</span><span class="p">,</span> <span class="mf">1.4593836892273668</span><span class="p">,</span> <span class="mf">1.5042779940878397</span><span class="p">,</span> <span class="mf">1.540914144056721</span><span class="p">,</span> <span class="mf">1.5694643322257227</span><span class="p">,</span> <span class="mf">1.5908330465698923</span><span class="p">,</span> <span class="mf">1.6063037620026415</span><span class="p">,</span> <span class="mf">1.6172175322689708</span><span class="p">,</span> <span class="mf">1.6247689357696435</span><span class="p">,</span> <span class="mf">1.629921406541151</span><span class="p">,</span> <span class="mf">1.6334027143439558</span><span class="p">,</span> <span class="mf">1.6357390290643616</span><span class="p">,</span> <span class="mf">1.6372997348435627</span><span class="p">,</span> <span class="mf">1.6383390867159184</span><span class="p">,</span> <span class="mf">1.6390298045931315</span><span class="p">,</span> <span class="mf">1.6394881953333464</span><span class="p">,</span> <span class="mf">1.6397921226262633</span><span class="p">,</span> <span class="mf">1.6399935122140699</span><span class="p">,</span> <span class="mf">1.640126903506169</span><span class="p">,</span> <span class="mf">1.6402152319777135</span><span class="p">,</span> <span class="mf">1.6402737104854812</span><span class="p">,</span> <span class="mf">1.6403124220218963</span><span class="p">,</span> <span class="mf">1.6403380462312662</span><span class="p">,</span> <span class="mf">1.640355006705482</span><span class="p">]</span>
</code></pre></div> </div> <p>(b) We can observe that theta is converging to the global minimum point 1.640. To evaluate the function at this point:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code
</span>
<span class="n">theta_global</span> <span class="o">=</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">30</span><span class="p">]</span>  <span class="c1"># Assume the 30th theta is sufficiently close to the global minimum point
</span><span class="nf">print</span><span class="p">(</span><span class="n">theta_global</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_global</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_global</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
</code></pre></div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Output
</span>
<span class="o">-</span><span class="mf">0.6196843457001793</span>
</code></pre></div> </div> <p>(c) If we run the code with $\theta^{(0)}=-1$,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Output
</span>
<span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1499999999999999</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.10919999999999994</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.08173347786239996</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06227141782417553</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0480238616518109</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.037359106839121914</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.029248790740098913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.023009056880295777</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.018166571714116383</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01438354734163941</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.011413143825789467</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00907160079235181</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0072200990529043794</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0057525455421655915</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.004587107060241312</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0036601976461915</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.002922119638721227</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0023338482682754252</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.001864624990714408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0014901341241158653</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011911074126556803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0009522471605601266</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0007613895071587255</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0006088506461576267</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00048691365718682596</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00038942421445218494</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00031147111670195584</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0002491332309026941</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00019927865131451355</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00015940504907746946</span><span class="p">]</span>
</code></pre></div> </div> <p>We can observe that theta is converging to the point $\theta=0$ (which is a local minimum point, but not a global minimum point)</p> </li> </ul> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Let’s note that Cheolsu’s synaptic connection state $\theta_0$, which allows him to score $100$ points in the test, may not be unique. In other words, the solution to the optimization problem may not be unique. Moreover, if Cheolsu was not a middle school student but a lizard, the synaptic connection state $\theta_0$ that allows him to achieve a test score of $100$ points might physically be impossible and might not even exist. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>Finding the minimum value of a function means that there exists an optimal solution $\theta_0$ that minimizes the function, such that for any $\theta$ within its domain, $f(\theta)\ge f(\theta_0)$. If we set $\theta_0=0$, the proposition holds true based on the two facts mentioned earlier. More will be discussed towards the end of Part I. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>For example, consider a 6th-degree polynomial given by $f(\theta)=c_6\theta^6+c_5\theta^5+\cdots+c_1\theta+c_0$. We know that if $c_6&gt;0$, a minimum value exists somewhere in the set of real numbers. However, the derivative $f’(\theta)=6c_6\theta^5+5c_5\theta^4+\cdots+c_1=0$ is a $5$th-degree polynomial. Abel and Galois proved that polynomials of degree $5$ or higher do not have a general solution formula. Therefore, we cannot solve $f’(\theta)=0$except special cases. Even if it’s not a polynomial of degree $5$ or higher, the usual strategy of “finding the point where the derivative is zero” often doesn’t work well when the equation becomes even slightly complex. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>$+1$ if $\cdot$ is positive, $-1$ if negative. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>The ‘neighborhood’ of a point refers to a small open interval (or an open set) that includes that point. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6"> <p>Specifically, if a function $f$ is differentiable and its derivative $f’$ is $L$-Lipschitz continuous, then $f’$ is guaranteed to converge to $0$ only when $0 &lt; \alpha &lt; 2/L$. Additionally, for $f(\theta^{(k)})$ to converge to a local minimum, it is necessary that the function $f$ does not have a saddle point, among other conditions. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="education"/><category term="deep-learning"/><category term="optimization"/><category term="gradient-descent"/><category term="machine-learning"/><category term="tutorial"/><category term="english"/><summary type="html"><![CDATA[Understanding optimization problems, gradient descent algorithms, and local vs global minima]]></summary></entry><entry><title type="html">Day 7: Day 7 최적화 문제의 정의</title><link href="https://codingjang.github.io/blog/2023/dl-day7-day-7-day-7/" rel="alternate" type="text/html" title="Day 7: Day 7 최적화 문제의 정의"/><published>2023-01-28T10:00:00+00:00</published><updated>2023-01-28T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/dl-day7-day-7-day-7</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/dl-day7-day-7-day-7/"><![CDATA[<h3 id="최적화-문제란">최적화 문제란?</h3> <p>최적화 문제optimization problem는 모든 가능한 해 중에서 최적해optimal solution를 찾는 문제를 말한다. 예를 들어, 내비게이션을 개발하는 IT 기업 연구원은 “가장 빠른 교통 경로”를 사용자에게 제공하고자 할 것이고, 산업공학을 전공한 사람은 “수익성을 최대화하는 사업 모델”을 찾고자 할 것이다. 이들이 풀고자 하는 문제는 모두 최적화 문제에 속한다. 한 지점 $A$로부터 다른 지점 $B$까지 (같은 장소를 두 번 통과하지 않고) 대중교통을 타고 이동하는 경로는 여러 가지 있을 것이고, 회사가 돈을 벌 수 있게 하는 사업 모델은 무수히 많을 것이다. 하지만, 가장 빠른 시간 안에 도착할 수 있는 경로, 그리고 가장 돈을 잘 버는 사업 모델을 찾기를 원하는 것이 사람 심리 아니던가. 이와 같은 맥락에서 최적화 문제는 컴퓨터과학자와 산업공학자(혹은 경영·경제학자)에게 매우 친숙한 대상이다.</p> <p>보통 어떤 상태가 최적optimal 이라 말하기 위해서는 ‘잘했다’의 기준이 필요하다. 최적화 문제는 보통 얼마나 ‘잘했는지’를 측정해주는 함수로 표현하는데, 이것이 바로 목적 함수objective function이다. 최적화 문제에서는 목적 함수를 최대화 혹은 최소화하고자 한다. 예를 들어, 첫 수학 시험을 치루는 중학생인 철수가 시험 점수를 잘 받고자 하는 상황을 생각해보자. 이 상황을 최적화 문제로 바꾸어 생각해본다면, 시험 점수가 목적 함수가 되고, 최적화 문제는 “시험 점수를 최대화”하는 것이 된다. 시험을 가장 잘 치면 100점을 받을 수 있다. 즉, 시험 점수=목적 함수=100인 상태가 최적해가 된다.</p> <p>그런데 앞서 목적 ‘함수’라고 했는데, 대체 무엇의 함수라는 걸까? 철수의 경우에는 철수의 “뇌상태”가 시험 점수를 결정할 것이다. 그렇다면 이 “뇌상태”의 수학적 표현은 무엇일까? 시냅스 연결 강도를 나타내는 변수들이 $k_1, k_2, k_3, k_4, k_5, \cdots$처럼 엄청 많이 있을 때, 우리는 이를 묶어 $\theta = [k_1, k_2, k_3,k_4,k_5,\cdots]^T$처럼 벡터로 나타낼 수 있다. 즉, 시냅스 연결이 총 $n$개가 있을 때, “$n$차원 벡터 $\theta$가 시험 점수 $s$를 결정한다”과 가정할 수 있다. 따라서 시험 점수 $s$는 $\theta$의 함수라고 할 수 있다. 또한, 철수가 똑똑하다면 학습을 통해 시냅스 연결 상태 $\theta$를 적절히 변경하여 시험 점수 $s(\theta)$를 최대화할 수 있다. 철수가 열심히 공부해서 시험을 가장 잘 쳤을 때는 100점을 받게 된다. 즉, $s(\theta_0)=100$을 만족하는 $\theta_0$가 존재한다면 $\theta_0$가 최적화 문제의 해가 될 것이다(이를 최적해라고 한다).</p> <p>시험 점수가 실수로 표현된다고 가정하면, $s$는 $\mathbb{R}^n$에서 $\mathbb{R}$로 가는 함수라고 할 수 있다. 이처럼, 목적 함수는 대개 $\mathbb{R}^n$에서 $\mathbb{R}$로 가는 함수로 설정한다. 최적화 문제를 표현하는 방식에는 여러 가지가 있을 수 있지만, 아래는 그 중에서도 표준 형태의 (연속) 최적화 문제를 나타낸 것이다.</p> \[\begin{equation}\begin{aligned}&amp; \underset{\theta \in \mathbb{R}^n}{\text{minimize}}&amp; &amp; f(\theta) \\&amp; \text{subject to}&amp; &amp; g_i(\theta) \leq 0, \; i = 1, \ldots, m.\\&amp;&amp;&amp; h_j(\theta) = 0, \; j = 1, \ldots, p.\end{aligned}\end{equation}\] <p>여기서 $f$는 목적 함수, $g_i,h_j\;(i=1,\dots,m,\;j=1,\dots,p)$는 제약 조건 함수로 모두 $\mathbb{R}^n$에서 $\mathbb{R}$로 가는 함수이다. 좀전에 시험 점수를 최대화maximize한다고 했던 것과 달리, 표준 형태의 최적화 문제에서는 목적 함수 $f$를 최소화minimize하는 형태로 표현하는 것이 전통이다. 사실 최대화와 최소화는 부호 하나 차이에 불과하기 때문에, 최대화 문제를 최소화 문제로 바꾸려면 목적 함수 앞에 마이너스를 붙여주면 그만이다. 철수의 시험 점수 $s(\theta)$를 최대화하는 대신, 부호를 바꾼 $-s(\theta)$를 $f(\theta)$로 두고 최소화하면 시험 점수 최대화의 문제도 위와 같은 표준형의 형태로 나타낼 수 있다.</p> <p>$g_i,h_j$는 각각 부등호 제약 조건inequality constraint과 등호 제약 조건equality constraint이다. 예를 들어, 시냅스 연결 강도가 너무 커지다보면 신경이 손상될 수도 있으니 $g_i(\theta):= \theta_i -M\;(M&gt;0)$으로 둘 수도 있고, 시험 공부를 하더라도 어려운 것을 공부하다가 성격이 이상해지면(?) 안 되기 때문에 이전 성격 검사 결과를 $T_i$로 기록한 다음 현재 상태에서의 성격 검사 결과를 $t_i(\theta)$로 얻어내어 $h_i(\theta):= t_i(\theta)-T_i$로 둘 수 있겠다. 예시를 이해하기 힘들다면, 최적화 문제의 표준형에서 $\theta_i-M$과 $t_i(\theta)-T_i$를 각각 $g_i(\theta)$와 $h_i(\theta)$의 자리에 대입하고 $M$과 $T_i$를 오른쪽으로 이항한 뒤 다시 생각해보자.</p> <h3 id="문제-11">문제 1.1</h3> <p>(a) 한 광고 회사 “널리알리”에서 수익을 극대화하기 위한 최적 전략을 찾고자 한다. 널리알리가 자신의 서비스에서 자유롭게 조정할 수 있는 변수는 타게팅 대상의 성비 $g$, 전체 인구 대비 광고 타게팅 대상의 비율 $p$, 그리고 평균 광고 노출 시간 $t$(단위: 초)라고 한다. 회사의 순익 $C$가 위 세 변수에 대한 함수로 나타날 때, 이를 수식 $(1)$에서와 같이 최적화 문제로 표현하시오.</p> <p>(b) 전체 인구 대비 광고 타게팅 대상의 비율 $p$와 평균 광고 노출 시간 $t$의 곱이 일정한 수준 $K$를 유지하게 하려고 한다. 새로운 최적화 문제를 다시 표현하시오.</p> <h3 id="문제-12">문제 1.2</h3> <p>(a) 이산 최적화 문제의 표준형을 제안해보시오.</p> <p>(b) 이산변수와 연속변수가 모두 포함된 경우에 대한 표준형을 제안해보시오.</p>]]></content><author><name></name></author><category term="education"/><category term="deep-learning"/><category term="tutorial"/><category term="korean"/><category term="education"/><category term="series"/><summary type="html"><![CDATA[딥러닝의 기초 - Day 7]]></summary></entry></feed>