<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://codingjang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://codingjang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-11T12:43:45+00:00</updated><id>https://codingjang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yejun Jang is an AI researcher specializing in reinforcement learning and deep learning at Seoul National University. </subtitle><entry xml:lang="en"><title type="html">Information Compression, Neural Networks, and AI for Science: Toward a Unified Theory</title><link href="https://codingjang.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science/" rel="alternate" type="text/html" title="Information Compression, Neural Networks, and AI for Science: Toward a Unified Theory"/><published>2025-10-11T05:00:00+00:00</published><updated>2025-10-11T05:00:00+00:00</updated><id>https://codingjang.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science</id><content type="html" xml:base="https://codingjang.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When learning introductory information science, one encounters Huffman coding as the most representative compression algorithm. Huffman coding is a lossless compression technique that minimizes string length without loss of information by eliminating repetition from the string representing the information. In other words, “it achieves the purpose of compression by representing more frequently repeated characters with fewer bits.”</p> <p>Meanwhile, JPEG compression achieves its purpose by removing high-frequency signals through Fourier transformation and eliminating information in areas that humans cannot properly perceive. Therefore, unlike Huffman coding, JPEG compression is a lossy compression technique where information is lost.</p> <p>It is well known that neural networks with sufficient depth can approximate arbitrary continuous functions. Let’s twist this idea slightly. Could the number of weights needed in a neural network to approximate a function within a certain error provide a good foundation for exploring the minimum amount of information needed to represent that function?</p> <p>One of the key concepts in linear algebra—the one-to-one correspondence between matrices and linear transformations—reveals that all linear functions are merely arrangements of numbers on a grid. If this fact could be extended to nonlinear functions, that is, if nonlinear functions are also nothing more than arrangements of numbers, there is ample room to apply information compression theory to neural network approximation theory. Furthermore, we cannot exclude the possibility that these two fields could be unified under the shadow of a grand theory.</p> <h2 id="the-concept-of-condensed-expression">The Concept of condensed expression</h2> <p>One of the main functions of intelligence, as I define here, is the generation of condensed expressions. We experience various phenomena occurring in the world directly, but we can also indirectly experience the world through condensed expressions that represent it—such as sentences, images, and videos. Moreover, humans have the ability to generate and share condensed expressions based on their direct experiences.</p> <p>The generation of condensed expressions is closely connected with pattern discovery. In particular, patterns that commonly appear and are repeated multiple times are commonly subject to condensation. For example, in Huffman Coding, the basic principle behind creating compressed files like .zip, the goal of condensation (compression, in this case) is achieved by representing more frequently repeated characters with fewer bits (binary numbers of 0 or 1). Just as it is more efficient to say “repeat a 20 times” rather than typing “aaaaaaaaaaaaaaaaaaaa” to someone, the removal of repetition is undoubtedly the most essential element in generating condensed expressions.</p> <h3 id="shared-knowledge-and-communication">Shared Knowledge and Communication</h3> <p>We can also eliminate repetition between ‘you’ and ‘me’. When communicating with condensed expressions, we can omit information that the other person and I already share. This is the fundamental principle that allows humans to transmit vast amounts of information with sentences of limited word count. Therefore, to convey information to someone, it is desirable to generate condensed expressions by distinguishing between what the other person and I share and what we do not share. The reason communication is difficult between two people with different temporal and cultural contexts, and the reason why shared knowledge among team members directly translates into comprehensive performance in sports, are all related to the use of condensed expressions.</p> <h3 id="context-dependent-information">Context-Dependent Information</h3> <p>Additionally, “unimportant” information can be omitted. Information that is not useful in context should be omitted for efficient communication. For a waiter working in a restaurant, the contextually important information is the type and quantity of menu items ordered at each table, so unlike in general situations, the customer omits their name (which is usually important) and only mentions the menu they want to order. As another example, in .jpg format compressed image files we use daily, information at high frequencies that cannot be properly distinguished by the human eye is removed to achieve a compression ratio of approximately 10:1. This too is possible because information that does not need to be conveyed in the context of human visual perception is boldly omitted.</p> <h3 id="representation-of-reality">Representation of Reality</h3> <p>Finally, we represent the world through condensed expressions. The universe is very complex, and our brains cannot simulate all of it. Therefore, from the perspective of survival, we need to express and store only the most important information as condensed expressions. From an evolutionary perspective, self-consciousness is presumed to have developed through the process of organisms craving food, and particularly the ability to effectively represent and remember the world helps us imagine and seek out prey that is outside our detection range (i.e., cannot be seen). Furthermore, humans not only represent the world in an effective way but also share condensed information in the form of language with those similar to themselves, thereby achieving common goals.</p> <h2 id="the-connection-to-neural-network-theory">The Connection to Neural Network Theory</h2> <p>Examples of diagonalization and SVD (Singular Value Decomposition) correspond well to examples of lossless compression and lossy compression, respectively. In the sense that a matrix diagonalized by finding an appropriate basis completely represents the linear transformation only with the components of a diagonal matrix, and that a similar linear transformation can be made approximately by omitting singular values of small magnitude, approximation using diagonalization and SVD are similar to the concepts of lossless compression and lossy compression, respectively.</p> <p>SVD makes most terms zero through basis transformation and even arranges the bases in order of importance. This makes us imagine something that is an extension of SVD into a nonlinear version. If we define information compression as finding an appropriate “nonlinear basis,” might neural networks already be performing that role?</p> <h3 id="research-directions-in-information-compression-theory">Research Directions in Information Compression Theory</h3> <p>Research applying the perspective of information compression to linear algebra should precede. Drawing ideas from rate-distortion theory, for a matrix $X$ sampled from a distribution $\mathcal{D}$ and $\tilde{X}$ a list of nonzero singular values calculated from $X$ (which have been subsequently quantized via storing in low resolution floating point number representations, e.g., float16):</p> \[\begin{aligned} &amp;\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})\\ &amp;\text{subject to} \; \left&lt; d(x,\tilde{x}) \right&gt;_{p(x,\tilde{x})}\le D \end{aligned}\] <p>The problem is that $p(\tilde{x}|x)$ is deterministic for the case of SVD. What if we can allow for some randomness when applying SVD, so that we can further compress the representation? Note that $I(X; \tilde{X})=H(\tilde{X})-H(\tilde{X}|X)$ and that $H(\tilde{X}|X)=0$ when $\tilde{X}$ is deterministic given $X$. So we’re basically left with the minimization of $H(\tilde{X})$.</p> <h3 id="optimal-basis-for-task-distributions">Optimal Basis for Task Distributions</h3> <p>If you are to perform different but related tasks, how would you construct and train your model? By related, I mean that there are similarities for some parts in the task, just as if sipping coffee and watering a plant both involve “grabbing.”</p> <p>Given a distribution of tasks $\mathcal{D}$, where each sampled task $F \sim \mathcal{D}$ is a Lebesgue-integrable function from $\mathbb{R}^n$ to $\mathbb{R}^m$, and given the norm $|\cdot|$ defined by the inner product $\left&lt;f,g\right&gt;=\int_{\mathbb{R}^n}w(\mathbf{x}){f(\mathbf{x})\cdot g(\mathbf{x})} d\mathbf{x}$, what is the most efficient parametrized basis $\mathcal{B}_\theta = {f_1(\theta), f_2(\theta), \cdots,f_d(\theta) }$, i.e.,</p> \[\begin{align*} \underset{\theta \in \mathbb{R}^p} {\textrm{minimize}} \;\;\mathbb{E}_{F \sim \mathcal{D}} \left[ \left\| \sum_{i=1}^dC_i(\theta)f_i(\theta) - F \right\|^2 \right] \end{align*}\] <p>where $C_i(\theta):=\left&lt;f_i(\theta), F\right&gt;$.</p> <h3 id="connections-to-perturbation-theory">Connections to Perturbation Theory</h3> <p>We use the original eigenstates of the Hamiltonian, even after the perturbation. <strong>Why not make corrections to the basis state itself?</strong></p> <p>Why do you have to use the original energy eigenstates to approximate the perturbed state? Is it actually a good basis, in the sense of minimizing the amount of data needed to describe the full system?</p> <p>What if the reason we need to solve the Schrödinger equation is actually because language has not developed sufficiently? There might be a mathematical language where the basis is naturally found from information about the potential. The language of computers is remarkably rich. Therefore, the attempt to simulate—that is, to analyze in the language of computers—is quite reasonable.</p> <p>When we see the symbol $\ket{\psi}$, we automatically associate the linear algebraic structure contained within it (Hilbert space, linearity, commutative law, …), but actually, looking at the symbol itself, it carries no information. When a complex but ordered pattern repeats, replacing it with the same symbol makes the notation easier to understand and simpler. What would we need to do to create an artificial intelligence model that can introduce new notation as needed?</p> <p>Physics formulas are a way of expression used by people to implicitly describe patterns of natural phenomena (obtained from observational data). However, while this thing called a formula may seem to contain a great deal of information, in reality, the amount of information actually contained is very small because it assumes the reader’s background knowledge. For example, when seeing the formula $\mathbf{F}=m\mathbf{a}$, if you don’t know the meaning of the equals sign, you won’t know that the left side and right side are the same, and if you don’t know that $\mathbf{F}$ means force, $m$ means mass, and $\mathbf{a}$ means acceleration—that is, the second derivative of position with respect to time—it’s all for naught. In other words, specific information about the formula is stored in the human brain, and <strong>the reality of the formula is merely the first spark to retrieve information stored in the brain.</strong> The system of knowledge created by humans ultimately represents nothing more than the connection relationships between different vectors.</p> <h2 id="ai-for-science-the-revolution-of-simulation">AI for Science: The Revolution of Simulation</h2> <h3 id="the-economics-of-experimentation">The Economics of Experimentation</h3> <p>Trial and error inherently involves pain. While some people enjoy this pain, continued repetitive work is generally only a source of fatigue. Laboratory work where people manually transfer solutions with pipettes, observe, and record results is extremely expensive considering labor costs. Even when automated with robots for High Throughput Screening (HTS), the cost ranges from a few tens of cents to as much as one dollar per pipetting action.</p> <p>The total budget invested in gravitational wave experiments, the enormous budget invested in CERN’s super-large particle accelerator—in these large experimental facilities, the situation where additional huge amounts are incurred because initial setting errors were not caught due to the absence of simulation is dizzying just to imagine. However, thanks to the dramatic development of computing, researchers have been able to conduct experiments without directly interacting with the physical world, which has brought tremendous cost savings—as if everyone gained the ability to conduct thought experiments like Einstein.</p> <h3 id="beyond-cost-savings">Beyond Cost Savings</h3> <p>To summarize the preceding story, computing is cheap and convenient while real-world experiments are expensive and tiring, and this is why simulation-based research has been activated since the 2000s. However, expecting simulation to merely reduce the trial and error and labor of experiments is extremely shortsighted.</p> <p>Empowered by improvements in parallel computing performance and developments in neural network theory, scientists have become able to solve differential equations that could not possibly be calculated in time, and have even reached the stage of autonomously exploring new circuit designs and proposing optimal circuit structures. Now artificial intelligence has reached the stage where it directly reasons in simulations, establishes theories, and formulates them, and groups that actively utilize this to conduct research are likely to gain the upper hand. Indeed, an era of new automation has opened where research is also conducted with artificial intelligence.</p> <h3 id="quantifying-knowledge">Quantifying Knowledge</h3> <p>How can we quantify the total amount of knowledge about a certain topic?</p> <p>Consider an AI chemist example: If there were an AI chemist conducting various experiments in a state connected with experimental equipment, it should conduct experiments in the direction that maximizes Knowledge most quickly. That is the essential role of a “chemist.”</p> <p>How do people know that they don’t know something? What makes them pursue new knowledge? Where does the sense of KNOWLEDGE come from? Can you make that into a loss function?</p> <h3 id="cloning-vs-simulation">Cloning vs. Simulation</h3> <p>Quantum computers “clone” a system. There exists a direct one-to-one correspondence of all physical properties between the original system and the cloned system. Neural networks “approximate” a system. They do not fully capture the physics of the original system, but describe the system accurately to a certain extent—this is what it means to “simulate.”</p> <h3 id="collaborative-learning">Collaborative Learning</h3> <p>Can inducing collaboration between agents yield better quality results with the same resources? Imagine a learning model like a group of researchers who write equations on a blackboard, playing a modeling game while reinforcement learning, growing while communicating.</p> <h2 id="ai-for-quantum-mechanics">AI for Quantum Mechanics</h2> <h3 id="can-machines-discover-the-schrödinger-equation">Can Machines Discover the Schrödinger Equation?</h3> <p><strong>On Observation, Pattern Finding, and Formulation</strong></p> <p>In my sophomore year, curious about quantum mechanics, I took the advanced course “Applications of Quantum Mechanics” one year early. From the first class, the professor expressed the opinion that even machine learning could not discover the Schrödinger equation, the first law of quantum mechanics. This question greatly attracted me—what exactly is quantum mechanics that even artificial intelligence cannot grasp its principles?</p> <p>To define “discovering the Schrödinger equation” more specifically means “discovering patterns of quantum wave phenomena from experimental data and presenting them in a form that can communicate with other physicists.” If machines could perform such work, it would be sufficient to recognize it as “formula discovery.” Can artificial intelligence discover the Schrödinger equation from data?</p> <h3 id="symbolic-regression">Symbolic Regression</h3> <p>The most representative example of a methodology where machines autonomously establish formulas and verify them with data is symbolic regression. Symbolic regression deals with methodology where machines autonomously generate equations to fit given experimental data. It was initiated by economist John Koza in the early 1990s and became formalized in the 2000s. Initially, solutions using genetic algorithms were mainstream, but with the recent development of deep learning, deep neural network-based algorithms such as AIFeynman have begun to appear.</p> <p>For example, suppose we want to “rediscover” Newton’s second law $\mathbf{F}=m\mathbf{a}$ from experimental data, but we don’t know that the equation we’re looking for is $\mathbf{F}=m\mathbf{a}$. Can machines, like Newton, discover natural laws on their own? For convenience, let’s assume that the force $\mathbf{F}$ applied to an object and position $\mathbf{x}$ are given as functions of time, and the object’s mass $m$ is also given. If an appropriate algorithm operates on the given variables and functions to derive a quantity that doesn’t change—that is, an invariant—we can say we have found a “law.” For example, if we figured out that $\mathbf{F}-md^2\mathbf{x}/dt^2=\mathbf{0}$ always holds, we can interpret this as having discovered the force-acceleration law.</p> <p>Therefore, symbolic regression can be thought of as minimizing the following loss function:</p> \[\mathcal{L}(f_\text{expr}):=\|f_\text{expr}( \mathbf{F},\mathbf{x}, m)\|^2\] <p>When $\mathcal{F} := { \mathbf{v}: [t_i, t_f] \rightarrow \mathbb{R}^3 }$, $\mathbf{F},\mathbf{x}\in\mathcal{F}$ are functions of time $t\in [t_i,t_f]$ and can be differentiated as much as desired, and $f_{\text{expr}}:\mathcal{F}\times\mathcal{F}\times\mathbb{R}^+ \rightarrow \mathcal{F}$ is a well-formed expression made using operators we know such as addition, multiplication, and differentiation.</p> <h3 id="complexity-and-overfitting">Complexity and Overfitting</h3> <p>If experimental data is fitted with an overly complex equation, measurement errors will be reflected in the equation, so to prevent overfitting, $f_\text{expr}$ should be ‘simple’. Here we obtain several points to consider:</p> <ol> <li>What methods exist for measuring the complexity of equations?</li> <li>What methods exist for structurally incorporating the concept of well-formed expressions into deep neural networks? <ul> <li>Can a good large language model be constrained to generate only equations that “make sense mathematically”?</li> </ul> </li> <li>Does there exist an appropriate embedding that embodies the meaning of equations? If so, how should it be learned?</li> </ol> <p>By answering questions like these, we can open new possibilities for developing artificial intelligence that establishes hypotheses (well-formed expressions) and autonomously verifies them with data.</p> <h3 id="quantum-computers-as-ai-playgrounds">Quantum Computers as AI ‘Playgrounds’</h3> <p>Just as humans revealed quantum mechanics through experiments, if we provide quantum computers as ‘playgrounds’ for artificial intelligence, very interesting discoveries will emerge. Having artificial intelligence spontaneously comprehend quantum phenomena with little experimental data is like trying to make people completely understand quantum mechanics with just a few cases. If humans could not interact with and experiment in their surrounding environment, humans might never have known about quantum phenomena. Therefore, a device that can freely experiment with quantum phenomena at high speed—that is, a quantum computer—will be a very attractive auxiliary device for artificial intelligence to understand quantum mechanics.</p> <p>At least so far, quantum mechanical phenomena do not appear to play an important role in the human brain. Meanwhile, AI systems including AlphaFold have already shown excellent performance in predicting and simulating the structure of complex molecules such as proteins without considering quantum phenomena. However, there was the disadvantage of having to use a very good (million-dollar) computer, and there was the limitation that the types of molecules that could be simulated were restricted to proteins.</p> <p>If symbolic regression can show that machines can autonomously devise core equations like the Schrödinger equation when given a quantum mechanics experimental dataset, it will provide new insights into the relationship between quantum mechanics and intelligence.</p> <h3 id="neural-networks-for-quantum-eigenvalue-problems">Neural Networks for Quantum Eigenvalue Problems</h3> <p>Let’s devise a simple symbolic regression methodology that uses machine learning for quantum computation. First, writing the Schrödinger equation:</p> \[\hat{H} \Psi (\mathbf{r}, t) = i\hbar \frac{\partial}{\partial t} \Psi(\mathbf{r}, t)\] <p>When the Hamiltonian is invariant with respect to time, we find solutions to the eigenvalue problem $\hat{H}\psi(\mathbf{r})= E\psi(\mathbf{r})$ and multiply by the phase factor $e^{-iEt/\hbar}$ to evolve them—game over.</p> <p>The action of an operator on a wave function can be interpreted as a linear transformation acting on a vector. And the eigenvalue problem can be thought of as finding the axis of symmetry whose direction is invariant before and after applying the transformation. Can we approximate and obtain these “axes of symmetry,” i.e., eigenfunctions, with neural networks?</p> <p>Replace the wave function $\psi:\mathbb{R}^n\rightarrow\mathbb{C}$ satisfying the eigenvalue equation $\hat{H}\psi = E\psi$ with the neural network $\psi_\theta$. Then the loss function can be expressed as follows for some norm $|\cdot|$:</p> \[\mathcal{L}(\theta, E)=\|\hat{H}\psi_\theta - E\psi_\theta\|^2\] <p>There are still some unresolved problems with the above approach. For example, how do we define the above norm? One possibility is to define the norm of function $f$ as $|f|=\sqrt{\mathbb{E}\left[|f(X)|^2\right]},\;\;X \sim \mathcal{N}(\mathbf{0}, I_{n\times n})$. However, the wave function $\psi_\theta$ defined by a neural network is extremely complex, and considerable computational resources are consumed to calculate the expectation value used in the norm.</p> <p>Additionally, a method has not been prepared for calculating the action of the Hamiltonian on the (neural network-defined) wave function. Suppose we approximate the Hamiltonian again with a neural network. When the dimension of $\theta$ is $N$, we need to newly define an operator $\hat{H}_\Theta: \mathbb{R}^N \rightarrow \mathbb{R}^N$ defined by a neural network.</p> <p>We can confirm that computational complexity increases exponentially according to the complexity of the system being simulated. What does this mean? If we utilize artificial intelligence, physics at the level of small molecules can be simulated without much difficulty. However, it does not seem possible to simulate the dynamics of larger quantum systems of ~10,000 level without compromising on accuracy.</p> <h3 id="the-paradox-of-quantum-computing-for-ai">The Paradox of Quantum Computing for AI</h3> <p>If we can sufficiently describe quantum mechanics just by obtaining the simulation function, there is no need to insist on quantum computers. However, paradoxically, the cheapest way to obtain large-scale quantum experimental data is quantum computing. We must explore whether quantum parallelism can provide practically significant help, and if so, how much.</p> <p>According to what has been revealed so far about neural networks, through training, they can learn patterns and structures embedded in multidimensional data, and can compressively represent revealed information through dimensionality reduction techniques. And it is also quite possible to map information stored as vectors this way into formulas that humans can see by using natural language processing.</p> <p>The fact that machines cannot discover quantum phenomena on their own seems rather implausible given the speed of AI development, but considering the characteristic of quantum phenomena where computational complexity increases exponentially according to the complexity of the system, the professor’s statement may not be so wrong after all.</p> <h2 id="research-questions-and-future-directions">Research Questions and Future Directions</h2> <h3 id="gordons-escape-theorem-and-dataset-intrinsic-dimension">Gordon’s Escape Theorem and Dataset Intrinsic Dimension</h3> <p>Our current research direction focuses on Gordon’s escape theorem combined with incorporating dataset intrinsic dimension. We need practical estimation algorithms for the dataset’s intrinsic dimension (e.g., PCA). <strong>We need to give researchers a tool that can estimate the minimum amount of parameters needed to train for a certain task.</strong></p> <p>Gordon’s escape theorem states that in high-dimensional spaces, a random subspace of sufficient dimension will “escape” through any mesh of low complexity with high probability. This theorem provides a powerful tool for understanding the behavior of random projections and has important applications in compressed sensing and dimensionality reduction.</p> <h3 id="information-bottleneck-and-optimal-transport">Information Bottleneck and Optimal Transport</h3> <p>The Information Bottleneck Method introduces the bottleneck $\tilde{X}$ to form the Markov chain $X \rightarrow \tilde{X} \rightarrow Y$, and drawing ideas from rate-distortion theory we obtain:</p> \[\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})-\beta I(X;Y)\] <p>An alternative approach is exploring “optimal transport.” If a data measure exists on a manifold, it can be represented by manifold structure, and we can find the optimal transport that moves it. This has significance in that it explicitly incorporates the manifold hypothesis into generalization. However, there is little discussion about predicting neural network parameters.</p> <h3 id="fractal-structures-and-compression">Fractal Structures and Compression</h3> <p>Complex structures like the Mandelbrot Set or Bifurcation Diagram are embedded in extremely simple formulas. Can we devise information compression algorithms that borrow such structures? Can we analyze fractal structures or bifurcation diagrams with neural networks to derive insights into chaotic systems? Fractal compression and the collage theorem are worth exploring.</p> <h3 id="connection-to-complexity-theory">Connection to Complexity Theory</h3> <p>There are $O(n^2)$ and $O(n\log n)$ algorithms which all perform the same task—sorting. Can we argue that one is a lossless compression of the other, since it uses less computation?</p> <h2 id="conclusion-toward-a-unified-framework">Conclusion: Toward a Unified Framework</h2> <p>The question originally posed—”Can the integration of information compression theory and neural network approximation theory be achieved?”—reveals itself to be not just a technical question but a profound inquiry into the nature of representation, learning, and scientific discovery itself.</p> <p>We have seen that:</p> <ol> <li> <p><strong>Compression and intelligence are deeply related</strong>: The generation of condensed expressions, the removal of redundancy, and the efficient encoding of patterns are central to both information theory and intelligence.</p> </li> <li> <p><strong>Neural networks may be nonlinear basis finders</strong>: Just as SVD finds optimal linear bases for compression, neural networks may be discovering optimal nonlinear bases for representing complex functions.</p> </li> <li> <p><strong>AI is transforming scientific practice</strong>: The ability to simulate, discover patterns, and even formulate theories autonomously represents a fundamental shift in how science can be conducted.</p> </li> <li> <p><strong>Quantum mechanics presents unique challenges</strong>: The exponential scaling of quantum systems means that even advanced AI requires quantum computers as “playgrounds” to truly understand quantum phenomena.</p> </li> </ol> <p>In summary, the question originally posed—”Can artificial intelligence discover the Schrödinger equation from data?”—and the broader question “Can the integration of information compression theory and neural network approximation theory be achieved?” lead us to new insights about intelligence, learning, and scientific discovery. With the same question, we conclude this essay: Can artificial intelligence truly understand quantum phenomena? And more broadly, can we develop a unified theory that connects information compression, neural network approximation, and the fundamental laws of nature?</p> <p>The answers may not only revolutionize AI and science but also deepen our understanding of what it means to know, to understand, and to discover.</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>This essay synthesizes ideas from ongoing research on information compression theory, neural network approximation, and AI for science. Special thanks to 홍철, 록기, 승환, and other collaborators for discussions on Gordon’s escape theorem, information bottleneck, and optimal transport approaches.</p> <h2 id="references">References</h2> <p>Key papers and resources mentioned:</p> <ul> <li>How many degrees of freedom do we need to train deep networks: a loss landscape perspective (arXiv:2107.05802)</li> <li>The information bottleneck method (arXiv:physics/0004057)</li> <li>Deep Learning and the Information Bottleneck Principle (arXiv:1503.02406)</li> <li>Intrinsic dimension of data representations in deep neural networks (arXiv:1905.12784)</li> <li>Gordon’s escape theorem and related work on high-dimensional geometry</li> <li>Symbolic regression literature including AIFeynman</li> <li>Generalization bounds for deep learning (arXiv:2012.04115)</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="artificial-intelligence"/><category term="information-theory"/><category term="neural-networks"/><category term="quantum-computing"/><category term="scientific-discovery"/><category term="compression"/><category term="english"/><summary type="html"><![CDATA[Exploring the deep connections between information compression theory, neural network approximation, and the future of scientific discovery through AI]]></summary></entry><entry xml:lang="en"><title type="html">Signing Right Away</title><link href="https://codingjang.github.io/blog/2025/signing-right-away/" rel="alternate" type="text/html" title="Signing Right Away"/><published>2025-10-05T11:00:00+00:00</published><updated>2025-10-05T11:00:00+00:00</updated><id>https://codingjang.github.io/blog/2025/signing-right-away</id><content type="html" xml:base="https://codingjang.github.io/blog/2025/signing-right-away/"><![CDATA[<p><em>This work contains AI-generated paragraphs and sentences. The original whitepaper of this work has been written by myself in English. The original experiment notes (undisclosed) were written in Korean by myself and my teammates, Wonbeen Yoon and Minjun Yi from Seoul National University. Gemini Deep Research was used to organize the work into the full whitepaper. Each work was fully reviewed and revised by myself.</em></p> <p>You can find the full whitepaper here: <strong><a href="/assets/pdf/SRA-2025-10-05.pdf">PDF</a></strong> <br/> The original whitepaper can be viewed here: <strong><a href="/assets/pdf/SRA-2024-05-26.pdf">PDF</a></strong></p> <h2 id="a-brief-motivation">A Brief Motivation</h2> <p>The proliferation of generative AI has made it trivial to create hyper-realistic fake images and videos, posing a serious threat to information integrity. This raises significant concerns over disinformation and fraud. While many approaches try to solve this with software classifiers, the root of the problem is arguably in hardware.</p> <p>In most current systems, the camera module sends an unencrypted, raw bitstream to the main processor over an interface like MIPI CSI-2. This link is vulnerable; a simple adapter can be used to intercept the feed or inject entirely synthetic data, and the system would have no way of knowing.</p> <p>This suggests that a robust solution requires securing content provenance at the source.</p> <h2 id="an-early-attempt-and-a-hard-reset">An Early Attempt and a Hard Reset</h2> <p>The initial idea for SRA was formalized back in the spring of 2024. Shortly after, I began my mandatory military service, which put the project on hold. After being discharged recently, I gathered a few friends to reboot the project with fresh energy.</p> <p>Our goal was ambitious: to reverse-engineer and replicate a secure transport layer for the MIPI CSI-2 protocol without official documentation. To put it mildly, it was a failure. Our attempts to build on an unknown, undocumented foundation resulted in glitchy, unparseable camera feeds. The custom parsing logic we wrote would fail intermittently, and the entire pipeline was fundamentally unstable. It was a disaster.</p> <p>But the experience, while painful, was incredibly valuable. It taught us two critical lessons:</p> <ol> <li> <p><strong>Hardware limitations are real.</strong> Our FPGA platform needed significantly more memory to buffer and process full image frames in real-time.</p> </li> <li> <p><strong>Reverse engineering has its limits.</strong> To build a stable image processing pipeline, we couldn’t rely on guesswork alone. We needed access to at least some confidential documentation or, failing that, a far more powerful and flexible hardware platform to allow for rapid, iterative testing.</p> </li> </ol> <h2 id="the-sra-architecture">The SRA Architecture</h2> <p>The core architecture of SRA was established in our original 2024 whitepaper, based on fundamental cryptographic principles of confidentiality, integrity, authentication, and replay protection. We initially designed our system around authenticated encryption schemes like ChaCha20-Poly1305. During development, we discovered that the MIPI Alliance’s Camera Security Framework had independently standardized similar approaches, which validated our architectural choices. While the prototyping experience taught us crucial lessons about implementation strategy—particularly the need for hardware-accelerated cryptography and better development platforms—the fundamental architectural design remained consistent.</p> <p>The architecture involves two main components:</p> <h3 id="1-authenticated--encrypted-camera-to-processor-link">1. Authenticated &amp; Encrypted Camera-to-Processor Link</h3> <p>The first step is to secure the physical data path. The camera module and processor would first perform a mutual authentication handshake. Once trust is established, all data transmitted over the CSI-2 interface would be protected by an authenticated encryption (AEAD) scheme, like AES-GCM. This ensures both confidentiality and integrity, as any modification would be detected via MAC verification.</p> <h3 id="2-immediate-signing-in-a-trusted-execution-environment-tee">2. Immediate Signing in a Trusted Execution Environment (TEE)</h3> <p>The encrypted feed is sent directly to a TEE, an isolated, secure enclave on the processor. Inside the TEE, the data is decrypted, processed, and cryptographically signed along with its metadata (e.g., timestamp, device ID). The private signing keys never leave the TEE, protecting them from a compromised OS. The final output is a standard image file with an embedded, verifiable C2PA Content Credential. This design ensures that by the time an application or user has access to an image, it has already been signed within a secure hardware environment.</p> <h2 id="aligning-with-the-broader-ecosystem">Aligning with the Broader Ecosystem</h2> <p>Our prototyping experience led to a critical strategic insight: the most effective path to widespread adoption is not to reinvent the wheel, but to align with and build upon the secure hardware capabilities that are already being integrated into commercial System-on-Chips (SoCs).</p> <p>Mobile SoC vendors like Qualcomm have already integrated the necessary hardware primitives—such as secure Image Signal Processors (ISPs), hardware crypto accelerators, and robust Trusted Execution Environments (TEEs)—into their platforms. The emergence of the Qualcomm Snapdragon 8 Gen 3 as the first C2PA-compliant mobile platform validates this trend.</p> <h3 id="our-strategy-open-and-interoperable">Our Strategy: Open and Interoperable</h3> <p>Rather than pursuing custom silicon or proprietary solutions, SRA’s strategy is to position itself as an <strong>open, interoperable reference architecture</strong> that can be implemented on any SoC that provides the necessary trusted hardware components. By leveraging existing secure camera APIs and TEE SDKs, SRA can be deployed as a firmware or software solution that “lights up” the latent security capabilities of modern devices.</p> <p>This approach dramatically reduces cost and time-to-market compared to a custom silicon strategy, and it fosters a competitive, multi-vendor ecosystem rather than a single proprietary solution. The initial plan to design custom ASICs was abandoned in favor of this more pragmatic path that builds on the industry’s existing investments in secure hardware.</p> <p>Industry pioneers like Truepic have already demonstrated similar architectures in practice with their Foresight system, which leverages the Qualcomm TEE and secure hardware pipeline. This serves as proof-of-concept for our model and demonstrates a clear path to market through ecosystem collaboration.</p> <h2 id="conclusion">Conclusion</h2> <p>The goal of SRA is to help create a digital ecosystem where the authenticity of content can be programmatically verified. The problem is challenging and involves navigating hardware, cryptography, and industry standards, but we believe it’s a critical step toward rebuilding trust in digital media.</p>]]></content><author><name></name></author><category term="whitepaper"/><category term="digital-signature"/><category term="c2pa"/><category term="content-provenance"/><category term="fake-news"/><category term="ai-generated-images"/><summary type="html"><![CDATA[A Hardware-Rooted Trust Architecture for Verifiable Digital Provenance]]></summary></entry><entry xml:lang="en"><title type="html">AI, Science, and the Humanities</title><link href="https://codingjang.github.io/blog/2025/ai-science-and-humanities/" rel="alternate" type="text/html" title="AI, Science, and the Humanities"/><published>2025-05-02T01:00:00+00:00</published><updated>2025-05-02T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2025/ai-science-and-humanities</id><content type="html" xml:base="https://codingjang.github.io/blog/2025/ai-science-and-humanities/"><![CDATA[<p><em>I Acknowledge “Hagibunmi” from the Physics Research open chat for his valuable feedback. This essay was originally written in Korean and was machine translated by Claude 4.5 Sonnet. The translated essay was fully reviewed and revised by myself.</em></p> <h2 id="introduction">Introduction</h2> <p>Finding and valuing what makes humans unique is almost instinctive. Anyone experiencing the dramatic transformation of the AI era has likely felt this urge at least once. I am human before I am anything else, and this fact forms the foundation of my identity. We must therefore think deeply about what characteristics are uniquely human.</p> <p>In this rapidly changing “flood of technology,” I argue that AI researchers must actively pursue an understanding of humans and society beyond mere technical knowledge if they are to navigate with steady hands on the helm. However, I found it difficult to paint a three-dimensional picture using the traditional approach of simply listing cases where humanities matter and offering two or three supporting reasons. Instead, I aim to lead the discussion naturally from questions about the essence of humanity to conversations about society and legal frameworks. Ultimately, I hope to sketch a vision of a future where humans and AI exist in harmony by exploring how legal and ethical systems might evolve.</p> <h2 id="what-makes-us-human">What Makes Us Human?</h2> <p>What is the essence of humanity—the unique characteristic that distinguishes humans from everything else? Though its manifestations vary by era, humans have always sought to be unique. French sociologist Pierre Bourdieu argued through his concept of “distinction” that people pursue social hierarchies and differentiation to set themselves apart from others. The ancient Greek philosopher Aristotle claimed that humans are the only beings possessing reason (logos). Christianity regards humans as noble beings created in God’s image. Throughout history and across cultures, humans have constructed frameworks of thought to elevate themselves.</p> <p>Of course, scientific theories challenged this view. Copernicus’s heliocentric theory in the 1500s and Darwin’s theory of evolution in the 1800s shook traditional religious worldviews. The revelation that Earth is not the center of the universe, and that humanity is merely one species among countless others, shocked people deeply. Yet humans remained distinctive—we use tools freely, create and use writing systems. The importance of reason, as Aristotle claimed, remained valid. Even as science and technology refuted parts of anthropocentric thinking, they became tools that accelerated civilization and elevated humanity’s status. New interpretations of anthropocentrism prevented science and technology from diminishing human worth.</p> <h2 id="the-rise-of-computing-and-ai">The Rise of Computing and AI</h2> <blockquote> <p>“I believe that in about fifty years’ time it will be possible to programme computers, with a storage capacity of about $10^9$, to make them play the imitation game so well that an average interrogator will not have more than 70 percent chance of making the right identification after five minutes of questioning. … I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.”</p> <p>— A. M. TURING, I.—COMPUTING MACHINERY AND INTELLIGENCE, <em>Mind</em>, Volume LIX, Issue 236, October 1950, Pages 433–460</p> </blockquote> <p>Today’s computers are physical implementations of the logical structures underlying rational thought, abstracted into computable forms. However, mere computational ability does not imply intelligence, and it actually took a long time before human intelligence could be meaningfully simulated by computers. In 1950, Turing predicted in his paper “Computing Machinery and Intelligence” that machines capable of “not exceeding 70 percent probability of correct identification between machine and human after five minutes of questioning” would appear around 2000, fifty years later. While it’s remarkable that the founder of computers early predicted AI development and devised a test (the Turing Test) to evaluate it, “Moravec’s paradox”—where tasks easy for one system are difficult for another operating on entirely different physical foundations—persisted for a long time.</p> <p>Thanks to Moravec’s paradox, among various domains of intelligence, “learning” seemed to remain a uniquely human domain. However, due to rapid developments in machine learning, AI has developed enough to replace much of human intellectual labor. We are recently witnessing cases across all fields where AI produces superior intellectual outputs compared to experts in each field. OpenAI’s recently released GPT-5 Pro model, capable of various tool use, shows 89.4% accuracy on GPQA Diamond. This benchmark is designed to evaluate graduate-level knowledge and reasoning - and yet, most frontier language models surpasses the 81.3% accuracy of PhD-level expert groups in each field. It shows 42.0% accuracy on the “Humanity’s Last Exam” benchmark - a benchmark with questions drawn from expert-level knowledge across numerous disciplines, including advanced mathematics, physics, biology, and specialized fields like ancient Roman inscriptions or avian anatomy. Setting aside perceptual changes needed for the society to accept AI and solely judging from the indicators, many white-collar jobs including civil servants, developers, and consultants appear likely to become automatable by machines fairly soon. Like machines during the Luddite movement, AI in modern society approaches many as an existential threat.</p> <h2 id="the-irony-of-automation">The Irony of Automation</h2> <p>Now it seems all that remains for us is correcting mistakes when these machines cause errors. In his paper “Ironies of Automation,” Lisanne Bainbridge argued that in an automated society, humans only play roles of monitoring and intervening when automation fails, but since such roles occur very rarely, humans fail to acquire the skills and experience needed when they must actually intervene. Kant’s ought-implies-can principle assumes that to fulfill an obligation, one must necessarily possess the ability to perform that obligation. Accordingly, for an actor to bear moral responsibility, free will must come first. Although ways automation fails vary, the fact that humans must ultimately bear responsibility for all these situations seems clear within current legal frameworks. Can humans bear full responsibility for actions of current AI “agents” not considered to possess free will?</p> <h2 id="science-objectivity-and-value-judgments">Science, Objectivity, and Value Judgments</h2> <p>Of course, one could argue for or against holding AI responsible by quantifying concepts like consciousness and free will, then creating a metric for how capable a system is to take responsibility. For instance, introducing Integrated Information Theory to calculate the strength of consciousness, or adopting theories like Orchestrated Objective Reduction to clarify free will. Such acts of developing quantified scales to analyze problem situations or defining physical quantities for mathematical analysis are all routine procedures for scientists.</p> <p>According to classical positivism, as articulated by Auguste Comte, science exists “to explain objectively observable phenomena more clearly and rigorously through logical structures.” Logical positivism went further: only propositions provable through pure logic and mathematics, or grounded in observable facts, were believed to have cognitive meaning. During the 19th and 20th centuries, positivism was such a dominant ideology in science that anything not objective or observable was simply excluded.</p> <p>Of course, positivism is now over two hundred years old, and among contemporary philosophers of science, scientific realism enjoys majority support. Yet even scientific realism maintains that scientific objects exist independently of mind. Either way, the fences we’ve erected to approach truth form barriers that prevent individual value judgments from influencing science. When approaching problems of intelligence, self-awareness, free will, and social responsibility, we face considerable barriers that science alone cannot overcome. In short, science loses much of its power the moment value judgments become necessary.</p> <h2 id="the-need-for-humanities-in-ai-research">The Need for Humanities in AI Research</h2> <p>In AI academia today, as in other scientific and engineering fields, mathematical analysis and experimental evidence are essentially required for publication in major conferences. This makes sense—after all, why else would we borrow rigorous concepts like vectors, parameters, and datasets except to ground our thinking technically? Yet to accurately describe mental states, we need more than scientifically rigorous objects. We also need humanities concepts centered on subjective experience and value judgment. Whether purely scientific approaches can fully explain intelligent subjects like humans and AI remains an open question. Moreover, no matter how objectively researchers try to proceed, we must acknowledge that researchers themselves are human, and their theoretical assumptions and biases inevitably shape what they observe.</p> <p>Post-positivism, which critiques and revises positivism, recognizes researchers’ biases as problems to be solved in approaching absolute truth. Researchers must constantly work to recognize and correct their biases. But are these biases and values necessarily problematic? We’ve been obsessed with objectivity for so long. What if relaxing that obsession could help us propose new concepts and expand research horizons?</p> <p><img src="/assets/img/blog/ai-humanities/chatgpt-review.png" alt="Example of AI interaction"/></p> <p>Consider, for example, a researcher exploring how AI robots interact with society and how the public comes to accept them. Today’s society doesn’t require us to treat ChatGPT with moral consideration. But in a future where robots become truly indistinguishable from humans in both behavior and appearance, won’t some people argue that robots deserve to be treated as equals? The ChatGPT App Store review shown above reveals something important: regardless of one’s philosophical stance, the general public is already being persuaded by AI, finding comfort in it, and forming emotional connections. When designing “non-humans” who will share our daily lives, can engineering approaches focused solely on optimizing benchmark scores adequately capture the complex and historically unprecedented interactions between humans and machines?</p> <h2 id="similarity-empathy-and-legal-frameworks">Similarity, Empathy, and Legal Frameworks</h2> <p>We must now place individual subjectivity and value judgments at the center of AI research. Consider a child who grows attached to a nearly human-seeming AI housekeeping robot. There are no “objective” grounds for claiming that disposing of the robot would be unethical. Yet from the child’s subjective perspective, stuffed animals, puppies, and helpful robots all seem similar enough to warrant protection. The belief that others are my equals rests fundamentally on “mutual similarity.” If we didn’t believe others were equal despite their similarity to us, and if harm to others didn’t threaten us, there would be no reason to sanction wrongs done to them. But mutual similarity makes harm to others feel like a threat to ourselves, and thus leads us to recognize them as equals.</p> <p>Law protects people from wrongs committed by other people; it doesn’t protect machines. Most AI scholars don’t believe machines have minds, and many share Noam Chomsky’s view that LLMs are merely “statistical parrots.” From a physical standpoint, humans and machines differ fundamentally—in materials, in mechanisms. Yet similarity is judged subjectively, and perception shifts with subjectivity. Appealing to physical differences alone cannot explain the complex social phenomenon captured in the screenshot above.</p> <h2 id="conclusion-designing-a-harmonious-future">Conclusion: Designing a Harmonious Future</h2> <p>As we’ve seen, AI has moved beyond simple computation into learning, reasoning, and creativity, forcing us to reexamine what we thought were uniquely human characteristics—reason, consciousness, responsibility, social bonds. While positivism and logical positivism focused on objective facts and pure logic, post-positivism questions even researchers’ subjectivity and value judgments. But science is only a tool. We must now ask: what questions should we pursue? What kind of society do we want to build? This represents a necessary paradigm shift for AI research. Understanding AI in context—what it means, how far our responsibility extends—has become paramount.</p> <p>In future societies where AI agents take on extensive roles—in production, services, caregiving, and creative work—the old assumption that “machines are just machines” will no longer hold. We may need new frameworks based on concepts like mutual similarity and emotional bonds. Designing legal and ethical systems that address attachment to robots and justify their protection will require fundamentally expanding the notion of legal personhood beyond humans. We must specify who bears responsibility when care robots make errors, and consider whether AI itself might someday hold certain rights and duties.</p> <p>Ultimately, AI-era researchers and policymakers must think as deeply about what makes us human as they do about technical performance. Science explores objective truth, but we must simultaneously work to understand subjective experience and social context. Humanities, social sciences, and law must join engineering in the laboratory and at the policy table. Only then can we design a society where humans and AI coexist while preserving human dignity and responsibility.</p>]]></content><author><name></name></author><category term="essay"/><category term="artificial-intelligence"/><category term="philosophy"/><category term="humanities"/><category term="ethics"/><category term="consciousness"/><category term="responsibility"/><category term="english"/><summary type="html"><![CDATA[Exploring the essential role of humanities in AI research]]></summary></entry><entry><title type="html">TEST 01 : 정답</title><link href="https://codingjang.github.io/blog/2024/tutoring-test-similarity-answer/" rel="alternate" type="text/html" title="TEST 01 : 정답"/><published>2024-07-02T01:00:00+00:00</published><updated>2024-07-02T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/tutoring-test-similarity-answer</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/tutoring-test-similarity-answer/"><![CDATA[<p><em>본 자료는 인공지능을 활용하지 않고 작성되었음을 알립니다.</em></p> <p><a href="/blog/2021/tutoring-test-similarity/">문제로 돌아가기</a></p> <p><strong>문제 1 정답:</strong> $\frac{3}{2}\rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled.png" alt="Untitled"/></p> <p><strong>문제 2 정답:</strong> $\frac{168}{125} \rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled%201.png" alt="Untitled"/></p> <p><strong>문제 3 정답:</strong> $9:4$</p> <p><img src="/assets/img/blog/tutoring/Untitled%202.png" alt="Untitled"/></p> <p><strong>문제 4 정답:</strong> $\frac{27}{2}\rm{cm}$</p> <p><img src="/assets/img/blog/tutoring/Untitled%203.png" alt="Untitled"/></p> <p><strong>문제 5 정답:</strong> $b^2=ac$</p> <p><img src="/assets/img/blog/tutoring/Untitled%204.png" alt="Untitled"/></p>]]></content><author><name></name></author><category term="education"/><category term="education"/><category term="tutoring"/><category term="mathematics"/><category term="korean"/><summary type="html"><![CDATA[수학 교육 자료]]></summary></entry><entry xml:lang="en"><title type="html">Introduction to PettingZoo</title><link href="https://codingjang.github.io/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466/" rel="alternate" type="text/html" title="Introduction to PettingZoo"/><published>2023-10-12T01:00:00+00:00</published><updated>2023-10-12T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466/"><![CDATA[<p><a href="/blog/2023/reinforcement-learning-basics"><strong>Head back to contents</strong></a></p> <p><em>This work has been translated from Korean to English using Claude 4.5 Sonnet.</em></p> <h2 id="documentation-link">Documentation link</h2> <p><a href="https://pettingzoo.farama.org/">PettingZoo Documentation</a></p> <h2 id="brief-introduction-to-pettingzoo">Brief Introduction to PettingZoo</h2> <p>PettingZoo is a Python library that facilitates General Multi-Agent Reinforcement Learning (MARL) simulations.</p> <p>PettingZoo consists of two main APIs:</p> <ul> <li><strong>AEC API</strong>: Helps implement environments with turns between agents, similar to board games. <ul> <li>Environments with turns are called Agent Environment Cycle (AEC) environments.</li> </ul> </li> <li><strong>Parallel API</strong>: All agents act simultaneously within a single time step. <ul> <li>These are called Parallel environments, referring to parallel/concurrent execution.</li> </ul> </li> </ul> <p>There exist AEC-to-Parallel and Parallel-to-AEC converters that allow transformation between the two APIs. However, when developing for the first time, it’s recommended to focus on one API without worrying too much about conversion.</p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="MARL"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Multi-agent reinforcement learning library for Python]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 7. Function Approximation</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13/" rel="alternate" type="text/html" title="DeepMind X UCL | 7. Function Approximation"/><published>2023-08-23T02:00:00+00:00</published><updated>2023-08-23T02:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13/"><![CDATA[<p><a href="/blog/2023/reinforcement-learning-basics"><strong>Head back to contents</strong></a></p> <p><em>This work has not been AI-generated.</em></p> <h2 id="why-function-approximations">Why function approximations?</h2> <p>It is hard to build lookup tables if the state space $S$ or the action space $A$ is large. For example, the game of go has $10^{170}$ states, which is more than the number of atoms in the universe squared. Therefore, creating the lookup table $v(s)$ is simply infeasible. Not only that, the universe itself is continuous - and we cannot give up on RL just because it is.</p> <p>Therefore, we will estimate the value function using function approximation. In many cases, the function approximation will involve some parameter vector $\mathbf{w}$:</p> \[\begin{aligned} v_{\mathbf{w}}(s) &amp;\approx v^\pi(s) \;(\text{or} \; v^\star(s)), \\ q_{\mathbf{w}}(s,a)&amp;\approx q^\pi(s,a) \;(\text{or} \; q^\star(s,a)). \end{aligned}\] <p>The parameter vector $\mathbf{w}$ will usually be some high dimensional vector that we can adjust to approximate the the value function. In a deep RL setup, we can also imagine $v_\mathbf{w}(s)$ and $q_\mathbf{w} (s, a)$ as a neural network, a rich class of nonlinear function that maps the state space to a range of values $(v_\mathbf{w}(s):S \rightarrow \mathbb{R})$ or the state-action space to a range of values $(q_\mathbf{w}(s, a) : S \times A \rightarrow \mathbb{R})$.</p> <h2 id="properties-of-rl">Properties of RL</h2> <p>There are certain properties of RL we should consider when constructing function approximations for value functions. The most important of all is that the regression targets may be non-stationary because of either:</p> <ul> <li>Changing policies of the agent <ul> <li>The agent’s policy $\pi$ change $v^\pi(s)$, which then changes the target,</li> </ul> </li> <li>Bootstrapping <ul> <li>The target involves $v^\pi(s)$ itself, which is constantly being updated,</li> </ul> </li> <li>Non-stationary dynamics <ul> <li>There might be other learning agents in the environment,</li> </ul> </li> <li>Previously unobserved information <ul> <li>Previously unobserved facts might influence the value function.</li> </ul> </li> </ul> <h2 id="tabular-vs-linear-vs-non-linear">Tabular vs. Linear vs. Non-linear</h2> <p>When we use <strong>tabular function approximations</strong>, we have a good theory on how agents learn (such as the Bellman Equation). But it does not scale or generalize well to larger or continuous state-action spaces.</p> <p>With <strong>linear function approximations</strong>, we have a reasonably good theory on the learning process. But the actual performance is dependent on the fixed feature mapping $\mathbf{x}:S\rightarrow \mathbb{R}^n$ (which will be discussed below).</p> <p>With <strong>nonlinear function approximations (deep learning)</strong>, we do not yet have a good theory at all - but we know that it works well experimentally, and that it works even better when we do not hand-engineer the feature mapping and let the neural network learn the mapping by itself.</p> <p>The fact that we do not have to hand engineer features when using deep learning is quite useful, because it means that we can use it to study less well-known problems without having to worry about feature mappings.</p> <h2 id="linear-function-approximations">Linear Function Approximations</h2> <h3 id="approximating-state-value-functions">Approximating state-value functions</h3> <p>Since we have already explored the tabular case, let’s now explore the linear case. In a linear function approximation setup, we represent the state (or the observation) at time $t$ as a vector $\mathbf{x}(S_t)$ which we will also call $\mathbf{x}_t$. This mapping $\mathbf{x}:S\rightarrow \mathbb{R}^n$ is called the feature mapping, and is considered to be fixed:</p> \[\begin{align*} \mathbf{x}(s)=\begin{pmatrix} x_1(s) \\ \vdots \\ x_n(s) \end{pmatrix},\; \forall s \in S. \end{align*}\] <p>Then we approximate the value function by a linear combination of the weights and features:</p> \[v_\mathbf{w}(s)=\mathbf{w}^\top \mathbf{x}(s)=\sum_{j=1}^{n} \mathbf{w}_j\mathbf{x}_j(s)\] <h3 id="approximating-action-value-functions">Approximating action-value functions</h3> <p>But what about action-value functions? We can use two different techniques to approximate $q(s,a)$. The first one is similar to the previous case. With the feature mapping $\mathbf{x}:S\times A \rightarrow \mathbb{R}^n$:</p> \[\begin{align*} \mathbf{x}(s,a)=\begin{pmatrix} x_1(s,a) \\ \vdots \\ x_n(s,a) \end{pmatrix},\; \forall s \in S. \end{align*}\] <p>The approximation of the value function is almost identical as before:</p> \[q_\mathbf{w}(s, a)=\mathbf{w}^\top\mathbf{x}(s,a)=\mathbf{w}_j\mathbf{x}_j(s,a)\] <p>This is called the <strong>action-in approximation</strong> because we take the action $a$ as an input of the value function. Here, we reuse the same weights $\mathbf{w}$ for different actions $a$. We also have the <strong>action-out approximation</strong>:</p> \[\mathbf{q}_\mathbf{w}(s)=\mathbf{W}\mathbf{x}(s),\; \text{ where }\;q_\mathbf{w}(s, a)= \mathbf{q}_\mathbf{w}(s)[a]\] <p>Note that $\mathbf{W}$ is a matrix. Therefore, we have different set of weights for different actions. For the action-out case we do not reuse the weights. Instead, we share the feature vector $\mathbf{x}(s)$ for all action $a$’s.</p> <p><strong>Action-in</strong> approximation is easier if the actions space is <strong>large or continuous</strong>. But for <strong>(small) discrete action spaces</strong>, <strong>action out</strong> is common. An example of this is DQN.</p> <h2 id="objective-minimize-loss">Objective: Minimize Loss</h2> <p>Now, we will construct a quadratic loss $J(\mathbf{w})$ and minimize it by optimizing our weight vector:</p> \[J(\mathbf{w}) = \mathbb{E}_{S \sim d} \left[(v^\pi(S)-\mathbf{w}^\top \mathbf{x}(S))^2 \right]\] <p>The distribution $d$ is some distribution of states in which we can sample from. Suppose we already know the value function $v^\pi(s)$. Then we can apply stochastic gradient descent (SGD) with decaying step size $\alpha_t$ to find some local minimum / saddle point of a smooth function:</p> \[\begin{align*} \nabla_\mathbf{w} v_\mathbf{w}(S_t)=\mathbf{x}(S_t) = \mathbf{x}_t, \\ \Delta \mathbf{w}_t = \alpha_t(v^\pi(S_t)-v_\mathbf{w}(S_t)) \mathbf{x}_t. \end{align*}\] <p>Note that the gradient of our value function, $\nabla_\mathbf{w} v_\mathbf{w}(S_t)$, is simply $\mathbf{x}(S_t)$. Luckily, $J(\mathbf{w})$ is quadratic in $\mathbf{w}$, and it’s Hessian is positive semi-definite everywhere in $\mathbb{R}^n$ which makes it a semi-convex(?) function. Hence, every local minimum is a global minimum. SGD with decaying step size will then ensure $J(\mathbf{w})$’s convergence to the global minimum.</p> <h2 id="mc-and-td-with-linear-approximation">MC and TD with Linear Approximation</h2> <p>However, we do not “know” the value function $v^\pi(S_t)$, which means we can’t use it to update our weight vector $\mathbf{w}$. Therefore, we substitute $v^\pi(s)$ with a stochastic target $G_t$:</p> \[\Delta \mathbf{w}_t = \alpha_t(G_t-v_\mathbf{w}(S_t)) \mathbf{x}_t.\] <p>This is <strong>MC with Linear Approximation</strong>. What is cool is that we can apply a supervised learning setup to the online training data</p> \[\{(S_0, G_0), \cdots, (S_T, G_T)\}\;\;T=\text{terminal time-step},\] <p>since $G_t$ is unbiased. If the variance of $G_t$ is too large, we can replace the target with the TD target $R_{t+1} + \gamma v_\mathbf{w}(S_{t+1})$ to get:</p> \[\Delta \mathbf{w}_t = \alpha_t(R_{t+1}+\gamma v_\mathbf{w} (S_{t+1})-v_\mathbf{w}(S_t)) \mathbf{x}_t.\] <p>The above update is called <strong>TD with Linear Approximation</strong>.</p> <h2 id="convergence-of-mc-with-linear-approximation">Convergence of MC with Linear Approximation</h2> <p>With linear value function approximation and suitably decaying step size $\alpha_t \rightarrow 0$, it is known that MC converges to:</p> \[\mathbf{w}_\text{MC} =\operatorname{argmin}_\mathbf{w} {\mathbb{E}^\pi[(G_t-v_\mathbf{w}(S_t))^2]}=\mathbb{E}^\pi [\mathbf{x}_t \mathbf{x}_t^\top]^{-1} \mathbb{E}^\pi[G_t\mathbf{x}_t]\] <p>We can verify this by setting the gradient of ${\mathbb{E}^\pi[(G_t-v_\mathbf{w}(S_t))^2]}$ with respect to $\mathbf{w}$ to zero:</p> \[\begin{align} \nabla_\mathbf{w} \mathbb{E}^\pi[(G_t - v_\mathbf{w}(S_t))^2]=\nabla_\mathbf{w} \mathbb{E}^\pi[(G_t - \mathbf{w}^\top\mathbf{x}_t)^2]&amp;=0\\\mathbb{E}^\pi[(G_t - \mathbf{w}^\top\mathbf{x}(S_t))\nabla_\mathbf{w}(\mathbf{w}^\top\mathbf{x}_t)]&amp;=0 \\=\mathbb{E}^\pi[(G_t - \mathbf{w}^\top\mathbf{x}_t)\mathbf{x}_t]&amp;=0 \\= \mathbb{E}^\pi[G_t\mathbf{x}_t - \mathbf{x}_t^\top\mathbf{x}_t\mathbf{w}]&amp;=0 \\ \mathbb{E}^\pi[\mathbf{x}_t\mathbf{x}_t^\top]\mathbf{w}=\mathbb{E}^\pi[G_t\mathbf{x}_t] \\ \mathbf{w}=\mathbf{w}_\text{MC}=\mathbb{E}^\pi[\mathbf{x}_t \mathbf{x}_t^\top]^{-1}\mathbb{E}^\pi[G_t\mathbf{x}_t]\end{align}\] <h2 id="convergence-of-td-with-linear-approximation">Convergence of TD with Linear Approximation</h2> <p>With linear value function approximation and suitably decaying step size $\alpha_t \rightarrow 0$, it is known that TD converges to:</p> \[\mathbf{w}_\text{TD} =\mathbb{E}^\pi [\mathbf{x}_t (\mathbf{x}_t-\gamma \mathbf{x}_{t+1})^\top]^{-1} \mathbb{E}^\pi[R_{t+1}\mathbf{x}_t]\] <p>We can verify this by setting the expected value of $\Delta\mathbf{w}$ to zero. Assuming $\alpha_t$ does not correlate with $R_{t+1},\mathbf{x}_t,\mathbf{x}_{t+1}$:</p> \[\begin{align} \mathbb{E}^\pi[\Delta \mathbf{w}]=0&amp;=\mathbb{E}^\pi[\alpha_t(R_{t+1} + \gamma\mathbf{x}_{t+1}^\top\mathbf{w}-\mathbf{x}_t^\top\mathbf{w})\mathbf{x}_t] \\ 0 &amp;= \mathbb{E}^\pi[\alpha_tR_{t+1} \mathbf{x}_t] + \mathbb{E}^\pi[\alpha_t\mathbf{x}_t(\gamma\mathbf{x}_{t+1}^\top-\mathbf{x}_t^\top)\mathbf{w}] \\ \mathbb{E}^\pi[\alpha_t\mathbf{x}_t(\mathbf{x}_t^\top-\gamma\mathbf{x}_{t+1}^\top)]\mathbf{w} &amp;= \mathbb{E}^\pi[\alpha_tR_{t+1} \mathbf{x}_t] \\ \mathbf{w} = \mathbf{w}_\text{TD}&amp;= \mathbb{E}^\pi[\alpha_t\mathbf{x}_t(\mathbf{x}_t^\top-\gamma\mathbf{x}_{t+1}^\top)]^{-1} \mathbb{E}^\pi[R_{t+1} \mathbf{x}_t]\end{align}\] <p>This differs from the MC solution. Remember, TD updates have less variance but may be biased. But since they have less variance, they tend to converge faster.</p> <h2 id="residual-bellman-updates">Residual Bellman updates</h2> <p>Note that the TD update is not a gradient update, since it ignores the dependence of $v_\mathbf{w}(S_{t+1})$ on $\mathbf{w}$.</p> \[\Delta \mathbf{w}_t = \alpha \delta \nabla_\mathbf{w} v_\mathbf{w}(S_t)\;\text{ where }\; \delta_t= R_{t+1} + \gamma v_\mathbf{w}(S_{t+1})-v_\mathbf{w}(S_t)\] <p>To remedy this, we can use the Bellman residual gradient update, where the Bellman loss is given as $\mathbb{E}^\pi[\delta_t^2]$ and we take the gradient of it to update $\mathbf{w}$:</p> \[\nabla_\mathbf{w}\mathbb{E}^\pi[\delta_t^2]=\mathbb{E}^\pi[\nabla_\mathbf{w}(\delta_t^2)]=2\mathbb{E}^\pi[\delta_t \nabla_\mathbf{w}\delta_t]\] \[\Delta \mathbf{w}_t = \alpha\delta_t \nabla_\mathbf{w}\delta_t = \alpha \delta_t \nabla_\mathbf{w} (v_\mathbf{w}(S_t)-\gamma v_\mathbf{w}(s_{t+1}))\] <p>However, residual Bellman updates tend to work worse in practice.</p> <h2 id="the-deadly-triad">The Deadly Triad</h2> <p>Algorithms that combine:</p> <ul> <li><strong>Bootstrapping</strong></li> <li><strong>Off-policy learning</strong>, and</li> <li><strong>Function approximation</strong></li> </ul> <p>may diverge. This is called <strong>the deadly triad</strong>. However, just because an algorithm combines the three methods does not mean that it is divergent - rather, we <strong>cannot guarantee</strong> the convergence of an algorithm if is has combined the above three.</p> <h2 id="summary">Summary</h2> <p><img src="/assets/img/blog/reinforcement-learning/untitled.png" alt="Untitled"/></p> <p>In addition to the deadly triad, we cannot guarantee the convergence of MC or TD when we combine on-policy learning with bootstrapping and non-linear function approximation. In summary, the deadly triad is the theoretical risk due to combinations of different methods in reinforcement learning, but is rarely seen in practice.</p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 6. Model-free Control</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c/" rel="alternate" type="text/html" title="DeepMind X UCL | 6. Model-free Control"/><published>2023-08-23T01:00:00+00:00</published><updated>2023-08-23T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c/"><![CDATA[<p><a href="/blog/2023/reinforcement-learning-basics"><strong>Head back to contents</strong></a></p> <p><em>This work has not been AI-generated.</em></p> <h2 id="glie">GLIE</h2> <p>GLIE stands for <strong>Greedy in the Limit with Infinite Exploration</strong>. It is used to describe a set of desirable properties of a policy. GLIE is a combination of the following two properties:</p> <ol> <li> <p><strong>Greedy in the Limit</strong> means that the policy eventually converges to a greedy policy, i.e.</p> \[\lim_{t \rightarrow \infty} {\pi_t(a|s)}=I(a=\operatorname{argmax}_{a' \in A}{q_t(s,a')})\] </li> <li> <p><strong>Infinite Exploration</strong> means that all state-action pairs are explored infinitely many times, i.e.</p> </li> </ol> \[\forall s, a \;\;\lim_{t\rightarrow \infty} {N_t (s,a)=\infty}\] <p>If we have the Infinite Exploration property, samples for each state-action pairs will accumulate enough, allowing for accurate value prediction. The Greedy in the Limit property then ensures that the policy will converge to the optimal greedy policy.</p> <p>Greedy policy alone will not explore enough, and the $\epsilon$-greedy policy with fixed $\epsilon \in (0,1]$ will never fully exploit. By choosing $\epsilon$-greedy policy with $\epsilon_t=1/t$, where $t$ is the number of time-steps elapsed, we have a GLIE policy that will both explore and exploit sufficiently in the limit.</p> <h2 id="analogy-between-dp-and-model-free-algorithms">Analogy Between DP and Model-free Algorithms</h2> <p>In lecture 04, we covered different types of Bellman operators:</p> \[\begin{aligned} (T_V^\star f)(s)&amp;=\max_{a \in A} \biggl[ {r(s, a) + \gamma \mathbb{E} \left[f(s')|s, a\right]} \bigg], \;\forall f \in V \\ (T_V^\pi f)(s)&amp;=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s') \bigg| s, a \bigg], \;\forall f \in V \\(T_Q^\star f)(s,a)&amp;=\mathbb{E} \bigg[r(s, a) + \gamma \max_{a'\in A} f(s',a') \bigg|s, a\bigg], \;\forall f \in Q \\ (T_Q^\pi f)(s, a)&amp;=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s',a') \bigg| s, a \bigg], \;\forall f \in Q \end{aligned}\] <p>To apply a Bellman operator we need exact knowledge of the transition dynamics of the system. We can avoid this problem using a sampled version of the operator. It turns out that the sampled versions of the above Bellman operators correspond to different model-free algorithms, except for $(T_V^\star f)(s)$:</p> \[\begin{aligned} &amp;(T_V^\star f)(s) \leftrightarrow \text{(None)} \\ &amp;(T_V^\pi f)(s)\leftrightarrow \text{(TD)} \\ &amp;\leftrightarrow v_{t+1}(S_t)=v_t(S_t)+\alpha_t\bigg(R_{t+1}+\gamma v_t(S_{t+1})-v_t(S_t)\bigg)\\&amp;(T_Q^\star f)(s,a)\leftrightarrow \text{(Q-learning)} \\&amp; \leftrightarrow q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \alpha_t \bigg(R_{t+1} + \gamma \max_{a' \in A}{q_t(S_{t+1}, a')-q_t(S_t, A_t)\bigg)}\\ &amp;(T_Q^\pi f)(s, a) \leftrightarrow \text{(SARSA)} \\ &amp;\leftrightarrow q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \alpha_t \bigg(R_{t+1} + \gamma q_t(S_{t+1}, A_{t+1})-q_t(S_t, A_t)\bigg) \end{aligned}\] <p>It is evident that we cannot build a sampled version of the operator $(T_V^\star f)(s)$ - Since the $\max_{a \in A}$ and the $\mathbb{E}$ operator do not commute, $(T_V^\star f)(s)$ cannot be expressed as an expectation from which we can sample upon.</p> <p>SARSA is relatively simple - it’s simply the $q$-version of TD. However, Q-learning has some interesting properties that deserves attention of its own.</p> <h2 id="on--off-policy-learning">On &amp; Off-Policy Learning</h2> <p>As humans, we learn from our experience. But we can also learn from the experience of others. In on-policy learning, the agent learns about the <strong>behavior policy $\pi$</strong> from experience sampled from the same policy $\pi$. On the other hand, in off-policy learning, the agent learns about the <strong>target policy $\pi$</strong> from experience sampled from a separate behavior policy $\mu$.</p> <p>Using off-policy learning, we can:</p> <ul> <li>learn from observing humans or other agents</li> <li>re-use experience from old policies</li> <li>learn about multiple policies while following one policy</li> <li><strong>learn about greedy policy while following exploratory policy</strong></li> </ul> <h2 id="q-learning">Q-Learning</h2> <p>Q-learning can learn the greedy policy while following any (exploratory) policy. We can see this from the update equation:</p> \[q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \alpha_t \bigg(R_{t+1} + \gamma \max_{a' \in A}{q_t(S_{t+1}, a')-q_t(S_t, A_t)\bigg)}\] <p>Here, there is no policy $\pi$ involved - you can use any behavior policy $\mu$ to converge to the optimal value function $q^\star$, as long as it is a infinite exploration policy. Once $q^\star$ is learned, we can use the (optimal) greedy policy for exploitation:</p> \[{\pi^\star(a|s)}=I(a=\operatorname{argmax}_{a'\in A}{q^\star(s,a')}).\] <h3 id="theorem">Theorem</h3> <p>Q-learning converges to the optimal $q$-value function, $q\rightarrow q^\star$, as long as we take each action in each state indefinitely often AND decay the step sizes in such a way that $\sum_t\alpha_t=\infty$ and $\sum_t \alpha_t^2&lt;\infty$.</p> <p>For example,<br/> $\alpha_t= 1/t^\omega, \omega \in (0.5, 1)$.</p> <h2 id="overestimation-in-q-learning">Overestimation in Q-Learning</h2> <p>In the Q-learning update equation, let’s take a look at the maximization:</p> \[\max_{a' \in A}{q_t(S_{t+1}, a')}\] <p>To write things differently:</p> \[\max_{a' \in A}{q_t(S_{t+1}, a')}=q_t\left(S_{t+1}, \operatorname{argmax}_{a' \in A} q_t(S_{t+1}, a')\right)\] <p>Suppose that the value function is currently inaccurate and has high noise. For simplicity, assume that the optimal q-value function $q^\star(S_{t+1},a’)$ stays constant regardless of the action $a’$ taken. For some of the $a’$s, the noise will add up to increase $q$. Therefore, the $\operatorname{argmax}_{a’ \in A}$ will choose the $a’$ with the highest noise value then update $q(S_{t}, A_{t})$ towards the noise-added value. Similar logic applies to the case where $q^\star(S_{t+1},a’)$ is not constant with respect to $a’$. Hence, Q-learning tends to overestimate the optimal $q$-value function.</p> <h3 id="double-q-learning">Double Q-Learning</h3> <p>How can we solve this problem? We can store two action value functions, $q$ and $q’$, and alternate between the two targets below:</p> \[\begin{align*} \text{(target for }q \text{):}\;\;R_{t+1} + \gamma q'\left(S_{t+1}, \operatorname{argmax}_{a' \in A} q(S_{t+1},a')\right) \\\\ \text{(target for }q' \text{):}\;\;R_{t+1} + \gamma q\left(S_{t+1}, \operatorname{argmax}_{a' \in A} q'(S_{t+1},a')\right) \end{align*}\] <p>This eliminates the influence of noise by decoupling the selection ($\operatorname{argmax}$) step and the evaluation step.</p> <p><img src="/assets/img/blog/reinforcement-learning/screenshot_2023-08-30_at_3.44.02_pm.png" alt="Q-learning overestimates, whereas double Q-learning does not. (Source: DeepMind X UCL Deep RL lectures)"/></p> <p>Q-learning overestimates, whereas double Q-learning does not. (Source: DeepMind X UCL Deep RL lectures)</p> <p>The above plot shows how decoupling indeed eliminates the overestimation in Q-learning. We can also apply this method to SARSA whenever the behavior policy is (soft) greedy and has correlation with $q$ (we call this double SARSA).</p> <h2 id="importance-sampling">Importance Sampling</h2> <p>Suppose you want to evaluate</p> \[\mathbb{E}_{X \sim d}[f(X)]\] <p>for some distribution $d$. If we sample $X$ to yield the estimate as follows,</p> \[\mathbb{E}_{X \sim d}[f(X)] \simeq \hat{X} :=\frac{1}{N} \sum_{i=1}^{N} f(X_i), \;\text{for each}\;X_i \sim d,\] <p>It could be problematic if $f(X)$ deviates significantly from $\mathbb{E}_{X \sim d}[f(X)]$ for some rare events, since it will overestimate or underestimate whenever the rare event is not sufficiently sampled.</p> <p>Therefore, we can seek to sample from a different distribution $d’$ so that the rare events are sampled more. Now, suppose that we have samples $f(X_i)$ with $X_i \sim d’$. How can we evaluate the original expectation using these samples? We can first modify the original expectation as follows:</p> \[\begin{aligned}\mathbb{E}_{X \sim d}[f(X)]&amp;=\sum_x d(x)f(x) \\ &amp;= \sum_x d'(x)\frac{d(x)}{d'(x)}f(x) \\ &amp;= \mathbb{E}_{X \sim d'} \left[\frac{d(x)}{d'(x)}f(x)\right] \end{aligned}\] <p>Now, we have a new expectation that can be sampled from $d’$ instead. Note that $d’$ has to be positive for all $x$ for this to work. Sampling from $d’$ gives:</p> \[\mathbb{E}_{X \sim d}[f(X)] \simeq \hat{X}' := \frac{1}{N} \sum_{i=1}^{N} \frac{d(X_i)}{d'(X_i)}f(X_i), \;\text{for each}\;X_i \sim d'.\] <p>This technique of sampling from a new distribution $d’$ to yield an estimate for the original expectation $\mathbb{E}_{X \sim d}[f(X)]$ is called Importance Sampling.</p> <h3 id="importance-sampling-for-off-policy-mc">Importance Sampling for Off-Policy MC</h3> <p>Suppose you want to estimate the $v$-value function $v^\pi$ for some policy $\pi$ using MC, and that the trajectory $\tau_t={S_t, A_t, R_{t+1} , \cdots }$ is generated with some behavior policy $\mu$. We can get an importance sample for $G_t=G(\tau_t)=R_{t+1}+\gamma R_{t+2} + \cdots$ by reweighing the target with $\frac{p(\tau_t|\pi)}{p(\tau_t|\mu)}$ (Suppose $N=1$):</p> \[\frac{p(\tau_t|\pi)}{p(\tau_t|\mu)} G_t = \frac{p(A_t|S_t,\pi)p(R_{t+1},S_{t+1}|S_t,A_t)p(A_{t+1}|S_{t+1},\pi) \cdots}{p(A_t|S_t,\mu)p(R_{t+1},S_{t+1}|S_t,A_t)p(A_{t+1}|S_{t+1},\mu) \cdots} G_t\] <p>Luckily, the transition probability (in which most cases we do not know) cancels out and we are left with:</p> \[\begin{aligned}\frac{p(\tau_t|\pi)}{p(\tau_t|\mu)} G_t &amp;= \frac{p(A_t|S_t,\pi)p(A_{t+1}|S_{t+1},\pi) \cdots}{p(A_t|S_t,\mu)p(A_{t+1}|S_{t+1},\mu) \cdots} G_t \\ &amp;= \frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1}) \cdots}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1}) \cdots} G_t \end{aligned}\] <p>We can then update $v^\pi$ towards the importance sampled target to get:</p> \[v(S_t) \leftarrow v(S_t) + \alpha\left({\frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1}) \cdots}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1}) \cdots} G_t - v(S_t)} \right)\] <h3 id="importance-sampling-for-off-policy-td">Importance Sampling for Off-Policy TD</h3> <p>Now, suppose you want to go through the same procedure with MC. In this case, you only need a single correction:</p> \[v(S_t) \leftarrow v(S_t) + \alpha\left(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} (R_{t+1} +\gamma v(S_{t+1})) - v(S_t) \right)\] <p>The proof for this can be found in page 44 of the lecture material (<a href="https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%206%20-%20Model-free%20control.pdf">link</a>).</p> <h2 id="expected-sarsa-generalized-q-learning">Expected SARSA (Generalized Q-learning)</h2> <p>We can also attempt to apply importance sampling to SARSA. However, we quickly realize that we don’t actually need IS because the $q$-value function conditions on selecting some action $a$. Therefore, we can simply take the expectation for the next $q$-values conditioned on policy $\pi$, while creating the trajectory according to some other policy $\mu$:</p> \[q(S_t, A_t) \leftarrow q(S_t, A_t) + \alpha \left(R_{t+1}+ \gamma \sum_{a \in A} \pi(a |S_{t+1})q(S_{t+1}, a)-q(S_t, A_t) \right)\] <p>Expected SARSA is also called Generalized Q-learning because it reduces to Q-learning when the policy chosen to be $\pi=\pi_q$, where $\pi_q$ is the greedy policy generated from $q$.</p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 5. Model-free Prediction</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/" rel="alternate" type="text/html" title="DeepMind X UCL | 5. Model-free Prediction"/><published>2023-08-16T01:00:00+00:00</published><updated>2023-08-16T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/"><![CDATA[<p><a href="/blog/2023/reinforcement-learning-basics"><strong>Head back to contents</strong></a></p> <p><em>This work has not been AI-generated.</em></p> <p><strong>Note.</strong> The notation used here might be confusing. We use $S$ to denote the state space and $S_t$ to represent the state at time $t$. Similarly, $A$ represents the action space, and $A_t$ denotes the action at time $t$.</p> <h2 id="dp">DP</h2> \[v_{n+1}(S_t) = \mathbb{E}^\pi \left[R_{t+1}+\gamma v_n(S_{t+1})\,|\,S_t\right]\] <h3 id="what-does-this-mean">What does this mean?</h3> <p>DP stands for Dynamic Programming. There are several different versions of DP. In this version, we improve the value function by directly evaluating the expectation on the right-hand side, i.e. the Bellman Expectation Operator. Evaluating this operator can easily become computationally infeasible when the state-action space is large. Moreover, it’s impossible to evaluate without knowledge of the environment’s dynamics. This is why we need model-free algorithms such as MC and TD.</p> <h2 id="mc">MC</h2> \[\begin{aligned} G_t&amp;=R_{t+1}+\gamma G_{t+1}=\cdots=\sum_{k=0}^{T} {\gamma^k R_{t+k+1}} \text{ (target)} \\ v_{n+1}(S_t)&amp;=v_n(S_t)+\alpha (G_t-v_n(S_t)) \text{ (update)} \end{aligned}\] <h3 id="what-does-this-mean-1">What does this mean?</h3> <p>MC stands for <strong>Monte Carlo</strong>. In a Monte Carlo update, the sampled return $G_t$ is determined by processing the entire $n$-th episode up to the terminal time step $T$. Then, $v_n(S_t)$ is updated towards $G_t$ with a step-size $\alpha$. Unlike DP, MC updates can be performed even without the knowledge of the rules underlying the environment, as the updates are based on samples. However, since $G_t$ can have a large variance, we use a small step-size $\alpha$ to reduce noise during updates.</p> <h2 id="td">TD</h2> \[\begin{aligned} H_t&amp;=R_{t+1}+\gamma v_t(S_{t+1})\text{ (target)} \\ v_{t+1}(S_t)&amp;=v_t(S_t)+\alpha(H_t-v_t(S_t))\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-2">What does this mean?</h3> <p>TD stands for <strong>Temporal Difference</strong>. In a Temporal Difference update, for each time step $t$ of the episode we update the value $v_t(S_t)$ to $v_{t+1}(S_t)$ by updating it towards the bootstrapped return $H_t$. We can think of TD as the sampled version of DP. TD is a bootstrapping method in the sense that it uses the estimate $v_t(S_{t+1})$ itself to create the target $H_t$. Therefore, it does not calculate the full cumulative return $G_t$. Note that the step-size $\alpha$ is also used since $H_t$ is a random variable.</p> <h2 id="comparing-mc-and-td">Comparing MC and TD</h2> <p>Although the equations look similar, MC and TD differ substantially in the below aspects:</p> <ol> <li> <p><strong>computation</strong></p> <p>For a MC update, the episode needs to conclude before updating the value function, as it requires the calculation of $G_t$, which involves future terms. In contrast, TD can be updated as we go, since the target $H_t$ can be calculated for each time step.</p> </li> <li> <p><strong>bootstrapping</strong></p> <p>In a MC update, we do not bootstrap from the value function estimate $v_n(S_t)$ to calculate the the target $G_t$, since it is independently calculated from the rewards in the time range $[t+1, T]$. However, we do bootstrap in a TD update as can be seen from the definition of the target $H_t$ - it involves the value function itself, $v_t(S_{t+1})$.</p> </li> <li> <p><strong>bias and variance</strong></p> <p>In a MC update, the target $G_t$ is the unbiased estimator of the true value function value $v^{\pi}(S_t)$. However $G_t$ has large variance as it is the weighted sum of multiple rewards $R_k\;(k=t+1,\cdots,T)$, which are all random variables. The circumstances are different for TD updates, because the target $H_t$ only involves two random variables, $R_{t+1}$ and $v_t(S_{t+1})$. Since there are less random “components” in the target, the variance is kept low - however unbiasedness is sacrificed due to bias-variance tradeoff.</p> </li> </ol> <h2 id="n-step-td">$n$-step TD</h2> \[\begin{aligned} G_t^{(n)}&amp;=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n v_k(S_{t+n}) \text{ (target)} \\ v_{k+1}(S_t)&amp;=v_k(S_t)+\alpha\left(G_t^{(n)}-v_k(S_t)\right)\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-3">What does this mean?</h3> <p>Suppose you want to cut off the later terms in $G_t$, and bootstrap at some point instead. If we do so, we get the $n$-step TD, with $n$ reward terms ($R_{t+1},\cdots,R_{t+n}$) and one bootstrapping term ($v_k(S_{t+n})$). As expected, the $n$-step TD has intermediate bias and intermediate variance, and interpolates between MC and TD.</p> <h2 id="lambda-td">$\lambda$-TD</h2> \[\begin{aligned} G_t^{\lambda}&amp;=R_{t+1}+\gamma \left((1-\lambda)v_k(S_{t+1}) + \lambda G_{t+1}^\lambda\right) \text{ (target)} \\ v_{k+1}(S_t)&amp;=v_k(S_t)+\alpha\left(G_t^{\lambda}-v_k(S_t)\right)\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-4">What does this mean?</h3> <p>In MC, we continue sampling the rewards until the end of the episode, whereas in TD, we stop and bootstrap. We can also do something in the middle - that is, we can take a linear combination of rewards and bootstrapped value functions. This gives the $\lambda$-TD, which is another way of interpolating between MC ($\lambda=1)$ and TD ($\lambda=0$). The $\lambda$-TD can be represented as a weighted average of $n$-step returns: $G_t^\lambda=\sum_{n=1}^{T}(1-\lambda)\lambda^{n-1} G_t^{(n)}$.</p> <h2 id="n-step-td-vs-lambda-td">$n$-step TD vs. $\lambda$-TD</h2> <p><img src="/assets/img/blog/reinforcement-learning/untitled_deepmind_x_ucl_5_model-free_pr.png" alt="Comparing the $n$-step TD and $\lambda$-TD. The plots obtained from the $\lambda$-TD algorithm is similar to the $n$-step TD algorithm, especially when $n \approx 1/(1-\lambda)$. (Source: Deepmind X UCL Deep Reinforcement Learning Lecture 5, given by Prof. Hado van Hasselt.)"/></p> <p>Comparing the $n$-step TD and $\lambda$-TD. The plots obtained from the $\lambda$-TD algorithm is similar to the $n$-step TD algorithm, especially when $n \approx 1/(1-\lambda)$. (Source: Deepmind X UCL Deep Reinforcement Learning Lecture 5, given by Prof. Hado van Hasselt.)</p> <p>The $n$-step TD and the $\lambda$-TD are both interpolations between MC and TD, and they share commonalities. In fact, you can think of the value $1/(1-\lambda)$ as the “horizon” of the $\lambda$-TD in the sense that the $n$-step TD and $\lambda$-TD yield similar results when $n \approx 1/(1-\lambda)$. The $n$-step TD and the $\lambda$-TD both have intermediate bias and intermediate variance. Typically, intermediate values of $n$ and $\lambda$ are good as they trade off bias and variance in an appropriate way, e.g. $n=10$, $\lambda=0.9$. This gives a good starting point for training RL algorithms.</p> <h2 id="eligibility-traces-advanced">Eligibility Traces (Advanced)</h2> <h3 id="motivation-independence-of-temporal-span">Motivation: Independence of Temporal Span</h3> <p>In MC updates and $n$-step TD / $\lambda$-TD updates, the update depends on the temporal span of each episode. Having to wait until the end of the episode is problematic, because it prevents us from online learning (i.e., learning as new data becomes available). Can we implement MC in an online learning setup?</p> <h3 id="prerequisite-linear-function-approximation">Prerequisite: Linear Function Approximation</h3> <p>The tabular value function can be written as an inner product between the one-hot feature vector $\mathbf{x}(s)$ and some weight vector $\mathbf{w}$, for any state $s$:</p> \[v_\mathbf{w} (s)= \mathbf{w}^{T}\mathbf{x}(s)\] <p>If we want to update the values for a state $s=S_t$ using MC, we update the weight acoording to the following:</p> \[\Delta \mathbf{w} = \alpha(G_t-v(S_t))\mathbf{x}(S_t)\] <p>Normally, we cannot update the values of states in the middle of the $k$-th episode. Instead, we have to update it later, simultaneously:</p> \[\Delta \mathbf{w}_{k+1} = \sum_{t=0}^{T-1} {\alpha (G_t-v(S_t))\mathbf{x}(S_t)}\] <p>But what’s interesting is that we can split the MC error $G_t-v(S_t)$ into two parts:</p> <ol> <li>the TD error term, $\delta_t := R_{t+1}+\gamma v(S_{t+1})-v(S_t)$,</li> <li>and the non-TD error term, $\gamma (G_{t+1}-v(S_{t+1}))$.</li> </ol> \[\begin{aligned} G_t-v(S_t) &amp;= R_{t+1}+\gamma G_{t+1}-v(S_t)\\ &amp;= R_{t+1}+\gamma G_{t+1}-v(S_t) + \gamma v(S_{t+1}) - \gamma v(S_{t+1})\\ &amp;= (R_{t+1}+\gamma v(S_{t+1})-v(S_t)) + \gamma (G_{t+1} - v(S_{t+1})) \\ &amp;= \delta_t+\gamma(G_{t+1} -v(S_{t+1}))\end{aligned}\] <p>Let’s utilize this discovery to our advantage. Note that the non-TD error term turns out to be the discounted MC error term for the next time step. Continuing the recursion, we obtain the following:</p> \[\begin{aligned} G_t-v(S_t) &amp;= \delta_t+\gamma(G_{t+1} - v(S_{t+1})) \\&amp;=\delta_t +\gamma\delta_{t+1}+\gamma^2(G_{t+2}- v(S_{t+2})) \\&amp;= \cdots \\&amp;=\sum_{k=t}^{T-1} {\gamma^{k-t}\delta_k} \end{aligned}\] <p>Now, let’s plug this into the updating equation and change the order of summation:</p> \[\begin{aligned} \Delta \mathbf{w}_{k+1} &amp;= \sum_{t=0}^{T-1} {\alpha (G_t-v(S_t))\mathbf{x}(S_t)} \\ &amp;= \sum_{t=0}^{T-1} {\alpha \left(\sum_{k=t}^{T-1} {\gamma^{k-t}\delta_k}\right)\mathbf{x}(S_t)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\left(\sum_{t=0}^{k} {\gamma^{k-t}}\mathbf{x}(S_t)\right)} \end{aligned}\] <p>Defining the eligibility trace $\mathbf{e}_{k} := \sum_{t=0}^{k} {\gamma^{k-t}}\mathbf{x}(S_t)$ and renaming the summation index $k$ to $t$, we have:</p> \[\begin{aligned} \Delta \mathbf{w}_{k+1} &amp;= \sum_{k=0}^{T-1} {\alpha\delta_k\mathbf{e}_k} \\ &amp;= \sum_{t=0}^{T-1} {\alpha\delta_t\mathbf{e}_t}, \end{aligned}\] <p>plus the recursion relation of the eligibility trace:</p> \[\mathbf{e}_t=\gamma\mathbf{e}_{t-1}+\mathbf{x}_t\] <p>What’s “magical”, as Hado mentions in the lecture, is that the term $\alpha\delta_t\mathbf{e}_t$ now does not involve future terms at all! Therefore, we can now update the weights online, and obtain (almost) the same results as the original MC.</p> <p>Even if we choose not to update the values online and instead accumulate the summation until the end of the episode, the required memory remains independent of the episode’s duration. In this case, the result will exactly equal the result of the original MC.</p> <p>By altering the recursion relation as follows, we can generalize this method to the $\lambda$-TD case:</p> \[\tilde{\mathbf{e}}_t=\gamma\lambda\tilde{\mathbf{e}}_{t-1}+\mathbf{x}_t.\] <p>The derivation is similar:</p> \[\begin{aligned} G_t^\lambda-v(S_t) &amp;= R_{t+1}+\gamma((1-\lambda)v(S_{t+1})+\lambda G_{t+1}^\lambda)-v(S_t)\\ &amp;= R_{t+1}+\gamma((1-\lambda)v(S_{t+1})+\lambda G_{t+1}^\lambda)-v(S_t)+ \gamma\lambda v(S_{t+1}) - \gamma\lambda v(S_{t+1})\\ &amp;= (R_{t+1}+\gamma v(S_{t+1})-v(S_t)) + \gamma\lambda (G_{t+1}^\lambda - v(S_{t+1})) \\ &amp;= \delta_t+\gamma\lambda(G_{t+1}^\lambda - v(S_{t+1}))\\&amp;=\delta_t +\gamma\lambda\delta_{t+1}+\gamma^2\lambda^2(G_{t+2}^\lambda - v(S_{t+2})) \\&amp;= \cdots \\&amp;=\sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k},\\\Delta \mathbf{w}_{k+1} &amp;= \sum_{t=0}^{T-1} {\alpha (G_t^\lambda-v(S_t))\mathbf{x}(S_t)} \\ &amp;= \sum_{t=0}^{T-1} {\alpha \left(\sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k}\right)\mathbf{x}(S_t)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\left(\sum_{t=0}^{k} {(\gamma\lambda)^{k-t}}\mathbf{x}(S_t)\right)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\tilde{\mathbf{e}}_t} \\\text{where}\\ \tilde{\mathbf{e}}_t &amp;:= \sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k}. \end{aligned}\]]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">DeepMind X UCL | 4. Theoretical Fundamentals of Dynamic Programming</title><link href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam/" rel="alternate" type="text/html" title="DeepMind X UCL | 4. Theoretical Fundamentals of Dynamic Programming"/><published>2023-08-09T01:00:00+00:00</published><updated>2023-08-09T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam/"><![CDATA[<p><a href="/blog/2023/reinforcement-learning-basics"><strong>Head back to contents</strong></a></p> <p><em>This work has not been AI-generated.</em></p> <h2 id="the-banach-fixed-point-theorem">The Banach Fixed Point Theorem</h2> <p>Let $X$ be a complete normed vector space, equipped with a norm $|\cdot|$ and $T:X \rightarrow X$ a $\gamma$-contraction mapping, then:</p> <ol> <li>$T$ has a unique fixed point $x^\star \in X$ s.t. $T x^\star=x^\star$</li> <li> <p>$\forall x_0 \in X$, the sequence $x_{n+1}=Tx_n$ converges to $x^\star$ in a geometric fashion:</p> \[\|x_n-x^\star\| \le \gamma^n\|x_0-x^\star\|\] <p>Thus, $\lim_{n\rightarrow\infty}|x_n-x^\star|\le \lim_{n\rightarrow\infty}\gamma^n|x_0-x^\star|=0.$</p> </li> </ol> <h2 id="what-does-this-mean">What does this mean?</h2> <p>It means that if the distance between two points after applying some operator $T$ is no greater than the original distance multiplied by $\gamma \in [0, 1)$, then applying the operator repeatedly to any point $x_0$ to yield $x_n$ is a great way to search for the unique fixed point of the operator $T$.</p> <h2 id="definitions-of-the-bellman-operators">Definitions of the Bellman Operators</h2> <h3 id="definition-bellman-optimality-operator-t_vstar">Definition: Bellman Optimality Operator $T_V^\star$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $V=V_S$ be the space of bounded real-valued functions over $S$. We define, point-wise, the Bellman Optimality Operator $T_V^\star:V\rightarrow V$ as:</p> \[(T_V^\star f)(s)=\max_{a \in A} \biggl[ {r(s, a) + \gamma \mathbb{E} \left[f(s')|s, a\right]} \bigg], \;\forall f \in V\] <p>Sometimes we drop the index and use $T^\star=T_V^\star$.</p> <h3 id="what-is-this-operator">What is this operator?</h3> <p>This operator is a $\gamma$-contraction with the unique fixed point being $f=v^\star$, the optimal value function of the MDP. Therefore, if we apply this operator iteratively to some (value) function $v$, it will converge to the optimal value function $v^\star$. This is why we attempt to approximate this specific operator (possibly with a neural network).</p> <h3 id="definition-bellman-expectation-operator-tpi_v">Definition: Bellman Expectation Operator $T^\pi_V$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $V=V_S$ be the space of bounded real-valued functions over $S$. For any policy $\pi:S \times A \rightarrow [0, 1]$, we define, point-wise, the Bellman Expectation Operator $T_V^\pi:V\rightarrow V$ as:</p> \[(T_V^\pi f)(s)=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s') \bigg| s \bigg], \;\forall f \in V\] <p>Note: By the tower rule, this is equivalent to:</p> \[(T_V^\pi f)(s)=\mathbb{E}^\pi \biggl[ {r(s, a) + \gamma \mathbb{E} \left[f(s')|s, a\right]} \bigg| s \bigg], \;\forall f \in V\] <p>Which is the same as the Bellman Optimality Operator, except $\max_{a\in A}$ being replaced by $\mathbb{E}^\pi[\cdot|s]$.</p> <p>Sometimes we drop the index and use $T^\pi=T_V^\pi$.</p> <h3 id="what-is-this-operator-1">What is this operator?</h3> <p>Same as the Bellman Optimality Operator, this operator is a $\gamma$-contraction except the unique fixed point being $f=v^\pi$, the value function for policy $\pi$ in a given MDP. Therefore, we can evaluate the policy $\pi$ by repeatedly applying this operator to the initial (value) function $v$. We then know whether the policy $\pi$ is doing well or not.</p> <h3 id="definition-bellman-optimality-operator-t_qstar">Definition: Bellman Optimality Operator $T_Q^\star$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $Q=Q_{S, A}$ be the space of bounded real-valued functions over $S\times A$. We define the Bellman Optimality Operator $T_Q^\star:Q\rightarrow Q$ as:</p> \[(T_Q^\star f)(s, a)=\mathbb{E} \bigg[ r(s, a) + \gamma \max_{a'\in A}f(s',a') \bigg| s, a \bigg], \;\forall f \in Q\] <p>Note: You can push the expectation inside to get</p> \[(T_Q^\star f)(s,a)={r(s, a) + \gamma \mathbb{E} \left[\max_{a'\in A} f(s',a')\bigg|s, a\right]}, \;\forall f \in Q\] <h3 id="what-is-this-operator-2">What is this operator?</h3> <p>This is the q-version of the previous Bellman Optimality Operator $T_V^\star$. Similarly, this operator is a $\gamma$-contraction with the unique fixed point being $f=q^\star$, the optimal q-value function of the MDP. Therefore, if we apply this operator iteratively to some (value) function $q$, it will converge to the optimal value function $q^\star$. We may attempt to approximate this operator too.</p> <h3 id="definition-bellman-expectation-operator-tpi_q">Definition: Bellman Expectation Operator $T^\pi_Q$</h3> <p>Given an MDP, $M=\langle S, A, p, r, \gamma \rangle$, let $Q=Q_{S, A}$ be the space of bounded real-valued functions over $S\times A$. For any policy $\pi:S \times A \rightarrow [0, 1]$, we define, point-wise, the Bellman Expectation Operator $T_Q^\pi:Q \rightarrow Q$ as:</p> \[(T_Q^\pi f)(s, a)=\mathbb{E}^\pi \bigg[ r(s, a) + \gamma f(s',a') \bigg| s, a \bigg], \;\forall f \in Q\] <p>Note: You can push the expectation inside to get</p> \[(T_Q^\pi f)(s, a) = r(s, a) + \gamma \mathbb{E^\pi} \bigg[ f(s',a')\bigg|s, a\bigg], \;\forall f \in Q\] <h3 id="what-is-this-operator-3">What is this operator?</h3> <p>This is the q-version of the previous Bellman Expectation Operator $T_V^\pi$. It is also a $\gamma$-contraction, with the unique fixed point being $f=q^\pi$. Therefore, we can evaluate the policy $\pi$ by repeatedly applying this operator to the initial value function $q$. We then know the performance of the policy $\pi$. Since this is a q-value function, we can also use it to greedify our policy $\pi$ by $\pi \leftarrow \operatorname{argmax}_{a\in A} q^\pi(s, a)$.</p> <h2 id="properties-of-the-bellman-operators">Properties of the Bellman Operators</h2> <h3 id="properties-bellman-optimality-operator-t_vstar--tstar-">Properties: Bellman Optimality Operator $T_V^\star \;(= T^\star )$</h3> <ol> <li>$T^\star$ has a unique fixed point $v^\star$.</li> <li> <p>$T^\star$ is a $\gamma$-contraction with respect to $|\cdot|_\infty$:</p> \[\|T^\star v-T^\star u\|_\infty \le \gamma \|v-u\|_\infty, \forall u,v \in V\] </li> <li>$T^\star$ is monotonic:</li> </ol> \[\forall u,v \in V \text{ s.t. } u \le v \text{ component-wise, then } T^\star u \le T^\star v\] <p><strong>The properties are similar for all other operators.</strong></p> <h2 id="approximate-dp">Approximate DP</h2> <p>So far, we have assumed perfect knowledge of the MDP &amp; perfect/exact representation of the value functions. However, we often encounter situations where we don’t know the underlying MDP or cannot represent the value function exactly after each update.</p> <p>Therefore, we will have to use approximate versions of the value functions / Bellman Operators. However, when the approximation is really bad, iteratively applying the approximated Bellman Operator to an initial function may not guarantee convergence.</p> <p>An example of divergence induced by some approximation is explored in the lecture. However, in most cases, divergence is not an issue. In the lecture, it is mentioned that “sample versions of these algorithms converge under mild conditions, and even for the function approximation case, the theoretical danger of divergence is rarely materialised in practice.”</p> <h2 id="theorem-value-of-a-greedy-policy">Theorem (Value of a greedy Policy)</h2> <p>Consider an MDP. Let $q:S\times A \rightarrow \mathbb{R}$ be an arbitrary function and let $\pi$ be the greedy policy associated with $q$, then:</p> \[\|q^\star - q^\pi \|_\infty \le \frac{2\gamma}{1-\gamma} \|q^\star-q\|_{\infty}\] <p>where $q^\star$ is the optimal value function associated with this MDP.</p> <p>We can gain insights from this theorem:</p> <ol> <li>Small values of $\gamma$ give a better (lower) upper bound for the potential loss of the performance. (Why?)</li> <li>If $\gamma=0$, then $q^\star=q^\pi$. Therefore, the greedy policy associated with any $q$ yields the optimal value function.</li> <li>If $q=q^\star$, it means that the value function, from which you are about to make the greedy policy out of, is the optimal value function. The greedy policy is the optimal policy, hence $q^\star=q^\pi$ in this case.</li> </ol>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="tutorial"/><category term="english"/><category term="series"/><summary type="html"><![CDATA[Reinforcement Learning Basics Series]]></summary></entry><entry xml:lang="en"><title type="html">Reinforcement Learning Basics</title><link href="https://codingjang.github.io/blog/2023/reinforcement-learning-basics/" rel="alternate" type="text/html" title="Reinforcement Learning Basics"/><published>2023-02-01T01:00:00+00:00</published><updated>2023-02-01T01:00:00+00:00</updated><id>https://codingjang.github.io/blog/2023/reinforcement-learning-basics</id><content type="html" xml:base="https://codingjang.github.io/blog/2023/reinforcement-learning-basics/"><![CDATA[<p>This series consists of lecture summaries from the DeepMind X UCL reinforcement learning course and an introduction to the PettingZoo library.</p> <h2 id="deepmind-x-ucl-deep-rl-series">DeepMind X UCL Deep RL Series</h2> <ul> <li><a href="/blog/2023/rl-deepmind-x-ucl-4-theoretical-fundamentals-of-dynam/">DeepMind X UCL | 4. Theoretical Fundamentals of Dynamic Programming (8/9)</a></li> <li><a href="/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/">DeepMind X UCL | 5. Model-free Prediction (8/16)</a></li> <li><a href="/blog/2023/rl-deepmind-x-ucl-6-model-free-control-c55a856c97414c/">DeepMind X UCL | 6. Model-free Control (8/23)</a></li> <li><a href="/blog/2023/rl-deepmind-x-ucl-7-function-approximation-86cf033e13/">DeepMind X UCL | 7. Function Approximation (8/23)</a></li> </ul> <h2 id="other-resources">Other Resources</h2> <ul> <li><a href="/blog/2023/rl-introduction-to-pettingzoo-1f62ce393bc3449abd16466/">Introduction to PettingZoo</a></li> </ul>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="DeepMind"/><category term="tutorial"/><category term="english"/><summary type="html"><![CDATA[DeepMind X UCL reinforcement learning series summary notes and PettingZoo introduction]]></summary></entry></feed>