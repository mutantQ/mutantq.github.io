---
layout: about
title: about
permalink: /
subtitle: <a href='https://snu.ac.kr'>Seoul National University</a> • AI Researcher • Deep Learning & Reinforcement Learning

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Electrical and Computer Engineering</p>
    <p>Seoul National University</p>
    <p>Seoul, South Korea</p>

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am the pre-founder of **[mutual](/blog/2025/introducing-mutual/)**, a team building trust infrastructure for the post-AI media era. My long-term goal is to develop reinforcement learning agents that can make reliable decisions based on grounded factual information. To get there, I am starting one level deeper: making sure the “facts” themselves can be trusted.

At mutual, we are designing **[SRA (Signing Right Away)](/assets/pdf/SRA-2024-05-26.pdf)** — a hardware-rooted architecture that cryptographically signs camera sensor data at the moment of capture, aligned with the C2PA content authenticity standard. Our aim is to provide an integrity layer that downstream systems — from apps and blockchains to future RL agents — can safely rely on.

## Research Highlights

- **Reinforcement learning**  
  My recent work on **[Q-Guided Flow Q-Learning (QFQL)](https://openreview.net/pdf?id=MFY9i3uR7S)** explores how to decouple policy and value learning while maintaining stability, and was accepted to the CoRL 2025 Workshop RemembeRL. At [KAIST ACSS Lab](http://acss.kaist.ac.kr/), I studied group equivariant neural networks and explored how to apply symmetry-aware architectures to reinforcement learning.

- **AI for Security, Security for AI**  
  I have worked on deep Q-networks for penetration testing on large-scale networks ([KICS 2025](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE12132012)), and supply chain verification techniques to prevent model and data poisoning attacks on military AI systems ([CISC-W 2024](https://manuscriptlink-society-file.s3.ap-northeast-1.amazonaws.com/kiisc/conference/ciscw2024/(%E1%84%8E%E1%85%AC%E1%84%8C%E1%85%A9%E1%86%BC)+%E1%84%83%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%AE%E1%86%AF%E1%84%83%E1%85%A2%E1%84%92%E1%85%AC(CISC-W%E2%80%9924)_.pdf)).

## Philosophy & Writing

I believe that AI research must be grounded in a deep understanding of humanity, ethics, and society. In essays like **[AI, Science, and the Humanities](/blog/2025/ai-science-and-humanities/)**, and through co-authoring the book *[Living Ideas from Future Observers (미래 관찰자의 살아있는 아이디어)](https://product.kyobobook.co.kr/detail/S000210833186)* with KFAS and SNU’s Institute for Future Strategy, I explore how legal, ethical, and philosophical questions should shape AI research and digital society.

Having recently completed my mandatory military service in October 2025, I am now focused on building mutual and advancing my research in AI around that vision: creating systems that can both **trust** the information they see and **reason** reliably on top of it. If you’d like to grab a coffee and talk more about these ideas, especially around mutual and SRA, feel free to reach out on [LinkedIn](https://www.linkedin.com/in/mutantQ); you can also explore my [publications](/publications/), [blog posts](/blog/), and [GitHub profile](https://github.com/mutantQ/) to learn more about my work.
