<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Information Compression, Neural Networks, and AI for Science: Toward a Unified Theory | Yejun Jang </title> <meta name="author" content="Yejun Jang"> <meta name="description" content="Exploring the deep connections between information compression theory, neural network approximation, and the future of scientific discovery through AI"> <meta name="keywords" content="reinforcement-learning, deep-learning, multi-agent-systems, AI-research"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://codingjang.github.io/blog/2025/information-compression-neural-networks-and-ai-for-science/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yejun</span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Information Compression, Neural Networks, and AI for Science: Toward a Unified Theory</h1> <p class="post-meta"> Created on October 11, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> artificial-intelligence</a>   <a href="/blog/tag/information-theory"> <i class="fa-solid fa-hashtag fa-sm"></i> information-theory</a>   <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>   <a href="/blog/tag/quantum-computing"> <i class="fa-solid fa-hashtag fa-sm"></i> quantum-computing</a>   <a href="/blog/tag/scientific-discovery"> <i class="fa-solid fa-hashtag fa-sm"></i> scientific-discovery</a>   <a href="/blog/tag/compression"> <i class="fa-solid fa-hashtag fa-sm"></i> compression</a>   <a href="/blog/tag/english"> <i class="fa-solid fa-hashtag fa-sm"></i> english</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>When learning introductory information science, one encounters Huffman coding as the most representative compression algorithm. Huffman coding is a lossless compression technique that minimizes string length without loss of information by eliminating repetition from the string representing the information. In other words, “it achieves the purpose of compression by representing more frequently repeated characters with fewer bits.”</p> <p>Meanwhile, JPEG compression achieves its purpose by removing high-frequency signals through Fourier transformation and eliminating information in areas that humans cannot properly perceive. Therefore, unlike Huffman coding, JPEG compression is a lossy compression technique where information is lost.</p> <p>It is well known that neural networks with sufficient depth can approximate arbitrary continuous functions. Let’s twist this idea slightly. Could the number of weights needed in a neural network to approximate a function within a certain error provide a good foundation for exploring the minimum amount of information needed to represent that function?</p> <p>One of the key concepts in linear algebra—the one-to-one correspondence between matrices and linear transformations—reveals that all linear functions are merely arrangements of numbers on a grid. If this fact could be extended to nonlinear functions, that is, if nonlinear functions are also nothing more than arrangements of numbers, there is ample room to apply information compression theory to neural network approximation theory. Furthermore, we cannot exclude the possibility that these two fields could be unified under the shadow of a grand theory.</p> <h2 id="the-concept-of-condensed-expression">The Concept of condensed expression</h2> <p>One of the main functions of intelligence, as I define here, is the generation of condensed expressions. We experience various phenomena occurring in the world directly, but we can also indirectly experience the world through condensed expressions that represent it—such as sentences, images, and videos. Moreover, humans have the ability to generate and share condensed expressions based on their direct experiences.</p> <p>The generation of condensed expressions is closely connected with pattern discovery. In particular, patterns that commonly appear and are repeated multiple times are commonly subject to condensation. For example, in Huffman Coding, the basic principle behind creating compressed files like .zip, the goal of condensation (compression, in this case) is achieved by representing more frequently repeated characters with fewer bits (binary numbers of 0 or 1). Just as it is more efficient to say “repeat a 20 times” rather than typing “aaaaaaaaaaaaaaaaaaaa” to someone, the removal of repetition is undoubtedly the most essential element in generating condensed expressions.</p> <h3 id="shared-knowledge-and-communication">Shared Knowledge and Communication</h3> <p>We can also eliminate repetition between ‘you’ and ‘me’. When communicating with condensed expressions, we can omit information that the other person and I already share. This is the fundamental principle that allows humans to transmit vast amounts of information with sentences of limited word count. Therefore, to convey information to someone, it is desirable to generate condensed expressions by distinguishing between what the other person and I share and what we do not share. The reason communication is difficult between two people with different temporal and cultural contexts, and the reason why shared knowledge among team members directly translates into comprehensive performance in sports, are all related to the use of condensed expressions.</p> <h3 id="context-dependent-information">Context-Dependent Information</h3> <p>Additionally, “unimportant” information can be omitted. Information that is not useful in context should be omitted for efficient communication. For a waiter working in a restaurant, the contextually important information is the type and quantity of menu items ordered at each table, so unlike in general situations, the customer omits their name (which is usually important) and only mentions the menu they want to order. As another example, in .jpg format compressed image files we use daily, information at high frequencies that cannot be properly distinguished by the human eye is removed to achieve a compression ratio of approximately 10:1. This too is possible because information that does not need to be conveyed in the context of human visual perception is boldly omitted.</p> <h3 id="representation-of-reality">Representation of Reality</h3> <p>Finally, we represent the world through condensed expressions. The universe is very complex, and our brains cannot simulate all of it. Therefore, from the perspective of survival, we need to express and store only the most important information as condensed expressions. From an evolutionary perspective, self-consciousness is presumed to have developed through the process of organisms craving food, and particularly the ability to effectively represent and remember the world helps us imagine and seek out prey that is outside our detection range (i.e., cannot be seen). Furthermore, humans not only represent the world in an effective way but also share condensed information in the form of language with those similar to themselves, thereby achieving common goals.</p> <h2 id="the-connection-to-neural-network-theory">The Connection to Neural Network Theory</h2> <p>Examples of diagonalization and SVD (Singular Value Decomposition) correspond well to examples of lossless compression and lossy compression, respectively. In the sense that a matrix diagonalized by finding an appropriate basis completely represents the linear transformation only with the components of a diagonal matrix, and that a similar linear transformation can be made approximately by omitting singular values of small magnitude, approximation using diagonalization and SVD are similar to the concepts of lossless compression and lossy compression, respectively.</p> <p>SVD makes most terms zero through basis transformation and even arranges the bases in order of importance. This makes us imagine something that is an extension of SVD into a nonlinear version. If we define information compression as finding an appropriate “nonlinear basis,” might neural networks already be performing that role?</p> <h3 id="research-directions-in-information-compression-theory">Research Directions in Information Compression Theory</h3> <p>Research applying the perspective of information compression to linear algebra should precede. Drawing ideas from rate-distortion theory, for a matrix $X$ sampled from a distribution $\mathcal{D}$ and $\tilde{X}$ a list of nonzero singular values calculated from $X$ (which have been subsequently quantized via storing in low resolution floating point number representations, e.g., float16):</p> \[\begin{aligned} &amp;\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})\\ &amp;\text{subject to} \; \left&lt; d(x,\tilde{x}) \right&gt;_{p(x,\tilde{x})}\le D \end{aligned}\] <p>The problem is that $p(\tilde{x}|x)$ is deterministic for the case of SVD. What if we can allow for some randomness when applying SVD, so that we can further compress the representation? Note that $I(X; \tilde{X})=H(\tilde{X})-H(\tilde{X}|X)$ and that $H(\tilde{X}|X)=0$ when $\tilde{X}$ is deterministic given $X$. So we’re basically left with the minimization of $H(\tilde{X})$.</p> <h3 id="optimal-basis-for-task-distributions">Optimal Basis for Task Distributions</h3> <p>If you are to perform different but related tasks, how would you construct and train your model? By related, I mean that there are similarities for some parts in the task, just as if sipping coffee and watering a plant both involve “grabbing.”</p> <p>Given a distribution of tasks $\mathcal{D}$, where each sampled task $F \sim \mathcal{D}$ is a Lebesgue-integrable function from $\mathbb{R}^n$ to $\mathbb{R}^m$, and given the norm $|\cdot|$ defined by the inner product $\left&lt;f,g\right&gt;=\int_{\mathbb{R}^n}w(\mathbf{x}){f(\mathbf{x})\cdot g(\mathbf{x})} d\mathbf{x}$, what is the most efficient parametrized basis $\mathcal{B}_\theta = {f_1(\theta), f_2(\theta), \cdots,f_d(\theta) }$, i.e.,</p> \[\begin{align*} \underset{\theta \in \mathbb{R}^p} {\textrm{minimize}} \;\;\mathbb{E}_{F \sim \mathcal{D}} \left[ \left\| \sum_{i=1}^dC_i(\theta)f_i(\theta) - F \right\|^2 \right] \end{align*}\] <p>where $C_i(\theta):=\left&lt;f_i(\theta), F\right&gt;$.</p> <h3 id="connections-to-perturbation-theory">Connections to Perturbation Theory</h3> <p>We use the original eigenstates of the Hamiltonian, even after the perturbation. <strong>Why not make corrections to the basis state itself?</strong></p> <p>Why do you have to use the original energy eigenstates to approximate the perturbed state? Is it actually a good basis, in the sense of minimizing the amount of data needed to describe the full system?</p> <p>What if the reason we need to solve the Schrödinger equation is actually because language has not developed sufficiently? There might be a mathematical language where the basis is naturally found from information about the potential. The language of computers is remarkably rich. Therefore, the attempt to simulate—that is, to analyze in the language of computers—is quite reasonable.</p> <p>When we see the symbol $\ket{\psi}$, we automatically associate the linear algebraic structure contained within it (Hilbert space, linearity, commutative law, …), but actually, looking at the symbol itself, it carries no information. When a complex but ordered pattern repeats, replacing it with the same symbol makes the notation easier to understand and simpler. What would we need to do to create an artificial intelligence model that can introduce new notation as needed?</p> <p>Physics formulas are a way of expression used by people to implicitly describe patterns of natural phenomena (obtained from observational data). However, while this thing called a formula may seem to contain a great deal of information, in reality, the amount of information actually contained is very small because it assumes the reader’s background knowledge. For example, when seeing the formula $\mathbf{F}=m\mathbf{a}$, if you don’t know the meaning of the equals sign, you won’t know that the left side and right side are the same, and if you don’t know that $\mathbf{F}$ means force, $m$ means mass, and $\mathbf{a}$ means acceleration—that is, the second derivative of position with respect to time—it’s all for naught. In other words, specific information about the formula is stored in the human brain, and <strong>the reality of the formula is merely the first spark to retrieve information stored in the brain.</strong> The system of knowledge created by humans ultimately represents nothing more than the connection relationships between different vectors.</p> <h2 id="ai-for-science-the-revolution-of-simulation">AI for Science: The Revolution of Simulation</h2> <h3 id="the-economics-of-experimentation">The Economics of Experimentation</h3> <p>Trial and error inherently involves pain. While some people enjoy this pain, continued repetitive work is generally only a source of fatigue. Laboratory work where people manually transfer solutions with pipettes, observe, and record results is extremely expensive considering labor costs. Even when automated with robots for High Throughput Screening (HTS), the cost ranges from a few tens of cents to as much as one dollar per pipetting action.</p> <p>The total budget invested in gravitational wave experiments, the enormous budget invested in CERN’s super-large particle accelerator—in these large experimental facilities, the situation where additional huge amounts are incurred because initial setting errors were not caught due to the absence of simulation is dizzying just to imagine. However, thanks to the dramatic development of computing, researchers have been able to conduct experiments without directly interacting with the physical world, which has brought tremendous cost savings—as if everyone gained the ability to conduct thought experiments like Einstein.</p> <h3 id="beyond-cost-savings">Beyond Cost Savings</h3> <p>To summarize the preceding story, computing is cheap and convenient while real-world experiments are expensive and tiring, and this is why simulation-based research has been activated since the 2000s. However, expecting simulation to merely reduce the trial and error and labor of experiments is extremely shortsighted.</p> <p>Empowered by improvements in parallel computing performance and developments in neural network theory, scientists have become able to solve differential equations that could not possibly be calculated in time, and have even reached the stage of autonomously exploring new circuit designs and proposing optimal circuit structures. Now artificial intelligence has reached the stage where it directly reasons in simulations, establishes theories, and formulates them, and groups that actively utilize this to conduct research are likely to gain the upper hand. Indeed, an era of new automation has opened where research is also conducted with artificial intelligence.</p> <h3 id="quantifying-knowledge">Quantifying Knowledge</h3> <p>How can we quantify the total amount of knowledge about a certain topic?</p> <p>Consider an AI chemist example: If there were an AI chemist conducting various experiments in a state connected with experimental equipment, it should conduct experiments in the direction that maximizes Knowledge most quickly. That is the essential role of a “chemist.”</p> <p>How do people know that they don’t know something? What makes them pursue new knowledge? Where does the sense of KNOWLEDGE come from? Can you make that into a loss function?</p> <h3 id="cloning-vs-simulation">Cloning vs. Simulation</h3> <p>Quantum computers “clone” a system. There exists a direct one-to-one correspondence of all physical properties between the original system and the cloned system. Neural networks “approximate” a system. They do not fully capture the physics of the original system, but describe the system accurately to a certain extent—this is what it means to “simulate.”</p> <h3 id="collaborative-learning">Collaborative Learning</h3> <p>Can inducing collaboration between agents yield better quality results with the same resources? Imagine a learning model like a group of researchers who write equations on a blackboard, playing a modeling game while reinforcement learning, growing while communicating.</p> <h2 id="ai-for-quantum-mechanics">AI for Quantum Mechanics</h2> <h3 id="can-machines-discover-the-schrödinger-equation">Can Machines Discover the Schrödinger Equation?</h3> <p><strong>On Observation, Pattern Finding, and Formulation</strong></p> <p>In my sophomore year, curious about quantum mechanics, I took the advanced course “Applications of Quantum Mechanics” one year early. From the first class, the professor expressed the opinion that even machine learning could not discover the Schrödinger equation, the first law of quantum mechanics. This question greatly attracted me—what exactly is quantum mechanics that even artificial intelligence cannot grasp its principles?</p> <p>To define “discovering the Schrödinger equation” more specifically means “discovering patterns of quantum wave phenomena from experimental data and presenting them in a form that can communicate with other physicists.” If machines could perform such work, it would be sufficient to recognize it as “formula discovery.” Can artificial intelligence discover the Schrödinger equation from data?</p> <h3 id="symbolic-regression">Symbolic Regression</h3> <p>The most representative example of a methodology where machines autonomously establish formulas and verify them with data is symbolic regression. Symbolic regression deals with methodology where machines autonomously generate equations to fit given experimental data. It was initiated by economist John Koza in the early 1990s and became formalized in the 2000s. Initially, solutions using genetic algorithms were mainstream, but with the recent development of deep learning, deep neural network-based algorithms such as AIFeynman have begun to appear.</p> <p>For example, suppose we want to “rediscover” Newton’s second law $\mathbf{F}=m\mathbf{a}$ from experimental data, but we don’t know that the equation we’re looking for is $\mathbf{F}=m\mathbf{a}$. Can machines, like Newton, discover natural laws on their own? For convenience, let’s assume that the force $\mathbf{F}$ applied to an object and position $\mathbf{x}$ are given as functions of time, and the object’s mass $m$ is also given. If an appropriate algorithm operates on the given variables and functions to derive a quantity that doesn’t change—that is, an invariant—we can say we have found a “law.” For example, if we figured out that $\mathbf{F}-md^2\mathbf{x}/dt^2=\mathbf{0}$ always holds, we can interpret this as having discovered the force-acceleration law.</p> <p>Therefore, symbolic regression can be thought of as minimizing the following loss function:</p> \[\mathcal{L}(f_\text{expr}):=\|f_\text{expr}( \mathbf{F},\mathbf{x}, m)\|^2\] <p>When $\mathcal{F} := { \mathbf{v}: [t_i, t_f] \rightarrow \mathbb{R}^3 }$, $\mathbf{F},\mathbf{x}\in\mathcal{F}$ are functions of time $t\in [t_i,t_f]$ and can be differentiated as much as desired, and $f_{\text{expr}}:\mathcal{F}\times\mathcal{F}\times\mathbb{R}^+ \rightarrow \mathcal{F}$ is a well-formed expression made using operators we know such as addition, multiplication, and differentiation.</p> <h3 id="complexity-and-overfitting">Complexity and Overfitting</h3> <p>If experimental data is fitted with an overly complex equation, measurement errors will be reflected in the equation, so to prevent overfitting, $f_\text{expr}$ should be ‘simple’. Here we obtain several points to consider:</p> <ol> <li>What methods exist for measuring the complexity of equations?</li> <li>What methods exist for structurally incorporating the concept of well-formed expressions into deep neural networks? <ul> <li>Can a good large language model be constrained to generate only equations that “make sense mathematically”?</li> </ul> </li> <li>Does there exist an appropriate embedding that embodies the meaning of equations? If so, how should it be learned?</li> </ol> <p>By answering questions like these, we can open new possibilities for developing artificial intelligence that establishes hypotheses (well-formed expressions) and autonomously verifies them with data.</p> <h3 id="quantum-computers-as-ai-playgrounds">Quantum Computers as AI ‘Playgrounds’</h3> <p>Just as humans revealed quantum mechanics through experiments, if we provide quantum computers as ‘playgrounds’ for artificial intelligence, very interesting discoveries will emerge. Having artificial intelligence spontaneously comprehend quantum phenomena with little experimental data is like trying to make people completely understand quantum mechanics with just a few cases. If humans could not interact with and experiment in their surrounding environment, humans might never have known about quantum phenomena. Therefore, a device that can freely experiment with quantum phenomena at high speed—that is, a quantum computer—will be a very attractive auxiliary device for artificial intelligence to understand quantum mechanics.</p> <p>At least so far, quantum mechanical phenomena do not appear to play an important role in the human brain. Meanwhile, AI systems including AlphaFold have already shown excellent performance in predicting and simulating the structure of complex molecules such as proteins without considering quantum phenomena. However, there was the disadvantage of having to use a very good (million-dollar) computer, and there was the limitation that the types of molecules that could be simulated were restricted to proteins.</p> <p>If symbolic regression can show that machines can autonomously devise core equations like the Schrödinger equation when given a quantum mechanics experimental dataset, it will provide new insights into the relationship between quantum mechanics and intelligence.</p> <h3 id="neural-networks-for-quantum-eigenvalue-problems">Neural Networks for Quantum Eigenvalue Problems</h3> <p>Let’s devise a simple symbolic regression methodology that uses machine learning for quantum computation. First, writing the Schrödinger equation:</p> \[\hat{H} \Psi (\mathbf{r}, t) = i\hbar \frac{\partial}{\partial t} \Psi(\mathbf{r}, t)\] <p>When the Hamiltonian is invariant with respect to time, we find solutions to the eigenvalue problem $\hat{H}\psi(\mathbf{r})= E\psi(\mathbf{r})$ and multiply by the phase factor $e^{-iEt/\hbar}$ to evolve them—game over.</p> <p>The action of an operator on a wave function can be interpreted as a linear transformation acting on a vector. And the eigenvalue problem can be thought of as finding the axis of symmetry whose direction is invariant before and after applying the transformation. Can we approximate and obtain these “axes of symmetry,” i.e., eigenfunctions, with neural networks?</p> <p>Replace the wave function $\psi:\mathbb{R}^n\rightarrow\mathbb{C}$ satisfying the eigenvalue equation $\hat{H}\psi = E\psi$ with the neural network $\psi_\theta$. Then the loss function can be expressed as follows for some norm $|\cdot|$:</p> \[\mathcal{L}(\theta, E)=\|\hat{H}\psi_\theta - E\psi_\theta\|^2\] <p>There are still some unresolved problems with the above approach. For example, how do we define the above norm? One possibility is to define the norm of function $f$ as $|f|=\sqrt{\mathbb{E}\left[|f(X)|^2\right]},\;\;X \sim \mathcal{N}(\mathbf{0}, I_{n\times n})$. However, the wave function $\psi_\theta$ defined by a neural network is extremely complex, and considerable computational resources are consumed to calculate the expectation value used in the norm.</p> <p>Additionally, a method has not been prepared for calculating the action of the Hamiltonian on the (neural network-defined) wave function. Suppose we approximate the Hamiltonian again with a neural network. When the dimension of $\theta$ is $N$, we need to newly define an operator $\hat{H}_\Theta: \mathbb{R}^N \rightarrow \mathbb{R}^N$ defined by a neural network.</p> <p>We can confirm that computational complexity increases exponentially according to the complexity of the system being simulated. What does this mean? If we utilize artificial intelligence, physics at the level of small molecules can be simulated without much difficulty. However, it does not seem possible to simulate the dynamics of larger quantum systems of ~10,000 level without compromising on accuracy.</p> <h3 id="the-paradox-of-quantum-computing-for-ai">The Paradox of Quantum Computing for AI</h3> <p>If we can sufficiently describe quantum mechanics just by obtaining the simulation function, there is no need to insist on quantum computers. However, paradoxically, the cheapest way to obtain large-scale quantum experimental data is quantum computing. We must explore whether quantum parallelism can provide practically significant help, and if so, how much.</p> <p>According to what has been revealed so far about neural networks, through training, they can learn patterns and structures embedded in multidimensional data, and can compressively represent revealed information through dimensionality reduction techniques. And it is also quite possible to map information stored as vectors this way into formulas that humans can see by using natural language processing.</p> <p>The fact that machines cannot discover quantum phenomena on their own seems rather implausible given the speed of AI development, but considering the characteristic of quantum phenomena where computational complexity increases exponentially according to the complexity of the system, the professor’s statement may not be so wrong after all.</p> <h2 id="research-questions-and-future-directions">Research Questions and Future Directions</h2> <h3 id="gordons-escape-theorem-and-dataset-intrinsic-dimension">Gordon’s Escape Theorem and Dataset Intrinsic Dimension</h3> <p>Our current research direction focuses on Gordon’s escape theorem combined with incorporating dataset intrinsic dimension. We need practical estimation algorithms for the dataset’s intrinsic dimension (e.g., PCA). <strong>We need to give researchers a tool that can estimate the minimum amount of parameters needed to train for a certain task.</strong></p> <p>Gordon’s escape theorem states that in high-dimensional spaces, a random subspace of sufficient dimension will “escape” through any mesh of low complexity with high probability. This theorem provides a powerful tool for understanding the behavior of random projections and has important applications in compressed sensing and dimensionality reduction.</p> <h3 id="information-bottleneck-and-optimal-transport">Information Bottleneck and Optimal Transport</h3> <p>The Information Bottleneck Method introduces the bottleneck $\tilde{X}$ to form the Markov chain $X \rightarrow \tilde{X} \rightarrow Y$, and drawing ideas from rate-distortion theory we obtain:</p> \[\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})-\beta I(X;Y)\] <p>An alternative approach is exploring “optimal transport.” If a data measure exists on a manifold, it can be represented by manifold structure, and we can find the optimal transport that moves it. This has significance in that it explicitly incorporates the manifold hypothesis into generalization. However, there is little discussion about predicting neural network parameters.</p> <h3 id="fractal-structures-and-compression">Fractal Structures and Compression</h3> <p>Complex structures like the Mandelbrot Set or Bifurcation Diagram are embedded in extremely simple formulas. Can we devise information compression algorithms that borrow such structures? Can we analyze fractal structures or bifurcation diagrams with neural networks to derive insights into chaotic systems? Fractal compression and the collage theorem are worth exploring.</p> <h3 id="connection-to-complexity-theory">Connection to Complexity Theory</h3> <p>There are $O(n^2)$ and $O(n\log n)$ algorithms which all perform the same task—sorting. Can we argue that one is a lossless compression of the other, since it uses less computation?</p> <h2 id="conclusion-toward-a-unified-framework">Conclusion: Toward a Unified Framework</h2> <p>The question originally posed—”Can the integration of information compression theory and neural network approximation theory be achieved?”—reveals itself to be not just a technical question but a profound inquiry into the nature of representation, learning, and scientific discovery itself.</p> <p>We have seen that:</p> <ol> <li> <p><strong>Compression and intelligence are deeply related</strong>: The generation of condensed expressions, the removal of redundancy, and the efficient encoding of patterns are central to both information theory and intelligence.</p> </li> <li> <p><strong>Neural networks may be nonlinear basis finders</strong>: Just as SVD finds optimal linear bases for compression, neural networks may be discovering optimal nonlinear bases for representing complex functions.</p> </li> <li> <p><strong>AI is transforming scientific practice</strong>: The ability to simulate, discover patterns, and even formulate theories autonomously represents a fundamental shift in how science can be conducted.</p> </li> <li> <p><strong>Quantum mechanics presents unique challenges</strong>: The exponential scaling of quantum systems means that even advanced AI requires quantum computers as “playgrounds” to truly understand quantum phenomena.</p> </li> </ol> <p>In summary, the question originally posed—”Can artificial intelligence discover the Schrödinger equation from data?”—and the broader question “Can the integration of information compression theory and neural network approximation theory be achieved?” lead us to new insights about intelligence, learning, and scientific discovery. With the same question, we conclude this essay: Can artificial intelligence truly understand quantum phenomena? And more broadly, can we develop a unified theory that connects information compression, neural network approximation, and the fundamental laws of nature?</p> <p>The answers may not only revolutionize AI and science but also deepen our understanding of what it means to know, to understand, and to discover.</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>This essay synthesizes ideas from ongoing research on information compression theory, neural network approximation, and AI for science. Special thanks to 홍철, 록기, 승환, and other collaborators for discussions on Gordon’s escape theorem, information bottleneck, and optimal transport approaches.</p> <h2 id="references">References</h2> <p>Key papers and resources mentioned:</p> <ul> <li>How many degrees of freedom do we need to train deep networks: a loss landscape perspective (arXiv:2107.05802)</li> <li>The information bottleneck method (arXiv:physics/0004057)</li> <li>Deep Learning and the Information Bottleneck Principle (arXiv:1503.02406)</li> <li>Intrinsic dimension of data representations in deep neural networks (arXiv:1905.12784)</li> <li>Gordon’s escape theorem and related work on high-dimensional geometry</li> <li>Symbolic regression literature including AIFeynman</li> <li>Generalization bounds for deep learning (arXiv:2012.04115)</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/signing-right-away/">Signing Right Away</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ai-science-and-humanities/">AI, Science, and the Humanities</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/reinforcement-learning-basics/">Reinforcement Learning Basics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gradient-descent/">Gradient Descent</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yejun Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>