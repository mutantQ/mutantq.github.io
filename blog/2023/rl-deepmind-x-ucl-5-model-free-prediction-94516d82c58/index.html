<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DeepMind X UCL | 5. Model-free Prediction | Yejun Jang </title> <meta name="author" content="Yejun Jang"> <meta name="description" content="Reinforcement Learning Basics Series"> <meta name="keywords" content="reinforcement-learning, deep-learning, multi-agent-systems, AI-research"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://codingjang.github.io/blog/2023/rl-deepmind-x-ucl-5-model-free-prediction-94516d82c58/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yejun</span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">DeepMind X UCL | 5. Model-free Prediction</h1> <p class="post-meta"> Created on August 16, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/tutorial"> <i class="fa-solid fa-hashtag fa-sm"></i> tutorial</a>   <a href="/blog/tag/english"> <i class="fa-solid fa-hashtag fa-sm"></i> english</a>   <a href="/blog/tag/series"> <i class="fa-solid fa-hashtag fa-sm"></i> series</a>   ·   <a href="/blog/category/education"> <i class="fa-solid fa-tag fa-sm"></i> education</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="/blog/2023/reinforcement-learning-basics"><strong>목차로 돌아가기</strong></a></p> <p><em>This work has not been AI-generated.</em></p> <p><strong>Note.</strong> The notation used here might be confusing. We use $S$ to denote the state space and $S_t$ to represent the state at time $t$. Similarly, $A$ represents the action space, and $A_t$ denotes the action at time $t$.</p> <h2 id="dp">DP</h2> \[v_{n+1}(S_t) = \mathbb{E}^\pi \left[R_{t+1}+\gamma v_n(S_{t+1})\,|\,S_t\right]\] <h3 id="what-does-this-mean">What does this mean?</h3> <p>DP stands for Dynamic Programming. There are several different versions of DP. In this version, we improve the value function by directly evaluating the expectation on the right-hand side, i.e. the Bellman Expectation Operator. Evaluating this operator can easily become computationally infeasible when the state-action space is large. Moreover, it’s impossible to evaluate without knowledge of the environment’s dynamics. This is why we need model-free algorithms such as MC and TD.</p> <h2 id="mc">MC</h2> \[\begin{aligned} G_t&amp;=R_{t+1}+\gamma G_{t+1}=\cdots=\sum_{k=0}^{T} {\gamma^k R_{t+k+1}} \text{ (target)} \\ v_{n+1}(S_t)&amp;=v_n(S_t)+\alpha (G_t-v_n(S_t)) \text{ (update)} \end{aligned}\] <h3 id="what-does-this-mean-1">What does this mean?</h3> <p>MC stands for <strong>Monte Carlo</strong>. In a Monte Carlo update, the sampled return $G_t$ is determined by processing the entire $n$-th episode up to the terminal time step $T$. Then, $v_n(S_t)$ is updated towards $G_t$ with a step-size $\alpha$. Unlike DP, MC updates can be performed even without the knowledge of the rules underlying the environment, as the updates are based on samples. However, since $G_t$ can have a large variance, we use a small step-size $\alpha$ to reduce noise during updates.</p> <h2 id="td">TD</h2> \[\begin{aligned} H_t&amp;=R_{t+1}+\gamma v_t(S_{t+1})\text{ (target)} \\ v_{t+1}(S_t)&amp;=v_t(S_t)+\alpha(H_t-v_t(S_t))\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-2">What does this mean?</h3> <p>TD stands for <strong>Temporal Difference</strong>. In a Temporal Difference update, for each time step $t$ of the episode we update the value $v_t(S_t)$ to $v_{t+1}(S_t)$ by updating it towards the bootstrapped return $H_t$. We can think of TD as the sampled version of DP. TD is a bootstrapping method in the sense that it uses the estimate $v_t(S_{t+1})$ itself to create the target $H_t$. Therefore, it does not calculate the full cumulative return $G_t$. Note that the step-size $\alpha$ is also used since $H_t$ is a random variable.</p> <h2 id="comparing-mc-and-td">Comparing MC and TD</h2> <p>Although the equations look similar, MC and TD differ substantially in the below aspects:</p> <ol> <li> <p><strong>computation</strong></p> <p>For a MC update, the episode needs to conclude before updating the value function, as it requires the calculation of $G_t$, which involves future terms. In contrast, TD can be updated as we go, since the target $H_t$ can be calculated for each time step.</p> </li> <li> <p><strong>bootstrapping</strong></p> <p>In a MC update, we do not bootstrap from the value function estimate $v_n(S_t)$ to calculate the the target $G_t$, since it is independently calculated from the rewards in the time range $[t+1, T]$. However, we do bootstrap in a TD update as can be seen from the definition of the target $H_t$ - it involves the value function itself, $v_t(S_{t+1})$.</p> </li> <li> <p><strong>bias and variance</strong></p> <p>In a MC update, the target $G_t$ is the unbiased estimator of the true value function value $v^{\pi}(S_t)$. However $G_t$ has large variance as it is the weighted sum of multiple rewards $R_k\;(k=t+1,\cdots,T)$, which are all random variables. The circumstances are different for TD updates, because the target $H_t$ only involves two random variables, $R_{t+1}$ and $v_t(S_{t+1})$. Since there are less random “components” in the target, the variance is kept low - however unbiasedness is sacrificed due to bias-variance tradeoff.</p> </li> </ol> <h2 id="n-step-td">$n$-step TD</h2> \[\begin{aligned} G_t^{(n)}&amp;=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n v_k(S_{t+n}) \text{ (target)} \\ v_{k+1}(S_t)&amp;=v_k(S_t)+\alpha\left(G_t^{(n)}-v_k(S_t)\right)\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-3">What does this mean?</h3> <p>Suppose you want to cut off the later terms in $G_t$, and bootstrap at some point instead. If we do so, we get the $n$-step TD, with $n$ reward terms ($R_{t+1},\cdots,R_{t+n}$) and one bootstrapping term ($v_k(S_{t+n})$). As expected, the $n$-step TD has intermediate bias and intermediate variance, and interpolates between MC and TD.</p> <h2 id="lambda-td">$\lambda$-TD</h2> \[\begin{aligned} G_t^{\lambda}&amp;=R_{t+1}+\gamma \left((1-\lambda)v_k(S_{t+1}) + \lambda G_{t+1}^\lambda\right) \text{ (target)} \\ v_{k+1}(S_t)&amp;=v_k(S_t)+\alpha\left(G_t^{\lambda}-v_k(S_t)\right)\text{ (update)}\end{aligned}\] <h3 id="what-does-this-mean-4">What does this mean?</h3> <p>In MC, we continue sampling the rewards until the end of the episode, whereas in TD, we stop and bootstrap. We can also do something in the middle - that is, we can take a linear combination of rewards and bootstrapped value functions. This gives the $\lambda$-TD, which is another way of interpolating between MC ($\lambda=1)$ and TD ($\lambda=0$). The $\lambda$-TD can be represented as a weighted average of $n$-step returns: $G_t^\lambda=\sum_{n=1}^{T}(1-\lambda)\lambda^{n-1} G_t^{(n)}$.</p> <h2 id="n-step-td-vs-lambda-td">$n$-step TD vs. $\lambda$-TD</h2> <p><img src="/assets/img/blog/reinforcement-learning/untitled_deepmind_x_ucl_5_model-free_pr.png" alt="Comparing the $n$-step TD and $\lambda$-TD. The plots obtained from the $\lambda$-TD algorithm is similar to the $n$-step TD algorithm, especially when $n \approx 1/(1-\lambda)$. (Source: Deepmind X UCL Deep Reinforcement Learning Lecture 5, given by Prof. Hado van Hasselt.)"></p> <p>Comparing the $n$-step TD and $\lambda$-TD. The plots obtained from the $\lambda$-TD algorithm is similar to the $n$-step TD algorithm, especially when $n \approx 1/(1-\lambda)$. (Source: Deepmind X UCL Deep Reinforcement Learning Lecture 5, given by Prof. Hado van Hasselt.)</p> <p>The $n$-step TD and the $\lambda$-TD are both interpolations between MC and TD, and they share commonalities. In fact, you can think of the value $1/(1-\lambda)$ as the “horizon” of the $\lambda$-TD in the sense that the $n$-step TD and $\lambda$-TD yield similar results when $n \approx 1/(1-\lambda)$. The $n$-step TD and the $\lambda$-TD both have intermediate bias and intermediate variance. Typically, intermediate values of $n$ and $\lambda$ are good as they trade off bias and variance in an appropriate way, e.g. $n=10$, $\lambda=0.9$. This gives a good starting point for training RL algorithms.</p> <h2 id="eligibility-traces-advanced">Eligibility Traces (Advanced)</h2> <h3 id="motivation-independence-of-temporal-span">Motivation: Independence of Temporal Span</h3> <p>In MC updates and $n$-step TD / $\lambda$-TD updates, the update depends on the temporal span of each episode. Having to wait until the end of the episode is problematic, because it prevents us from online learning (i.e., learning as new data becomes available). Can we implement MC in an online learning setup?</p> <h3 id="prerequisite-linear-function-approximation">Prerequisite: Linear Function Approximation</h3> <p>The tabular value function can be written as an inner product between the one-hot feature vector $\mathbf{x}(s)$ and some weight vector $\mathbf{w}$, for any state $s$:</p> \[v_\mathbf{w} (s)= \mathbf{w}^{T}\mathbf{x}(s)\] <p>If we want to update the values for a state $s=S_t$ using MC, we update the weight acoording to the following:</p> \[\Delta \mathbf{w} = \alpha(G_t-v(S_t))\mathbf{x}(S_t)\] <p>Normally, we cannot update the values of states in the middle of the $k$-th episode. Instead, we have to update it later, simultaneously:</p> \[\Delta \mathbf{w}_{k+1} = \sum_{t=0}^{T-1} {\alpha (G_t-v(S_t))\mathbf{x}(S_t)}\] <p>But what’s interesting is that we can split the MC error $G_t-v(S_t)$ into two parts:</p> <ol> <li>the TD error term, $\delta_t := R_{t+1}+\gamma v(S_{t+1})-v(S_t)$,</li> <li>and the non-TD error term, $\gamma (G_{t+1}-v(S_{t+1}))$.</li> </ol> \[\begin{aligned} G_t-v(S_t) &amp;= R_{t+1}+\gamma G_{t+1}-v(S_t)\\ &amp;= R_{t+1}+\gamma G_{t+1}-v(S_t) + \gamma v(S_{t+1}) - \gamma v(S_{t+1})\\ &amp;= (R_{t+1}+\gamma v(S_{t+1})-v(S_t)) + \gamma (G_{t+1} - v(S_{t+1})) \\ &amp;= \delta_t+\gamma(G_{t+1} -v(S_{t+1}))\end{aligned}\] <p>Let’s utilize this discovery to our advantage. Note that the non-TD error term turns out to be the discounted MC error term for the next time step. Continuing the recursion, we obtain the following:</p> \[\begin{aligned} G_t-v(S_t) &amp;= \delta_t+\gamma(G_{t+1} - v(S_{t+1})) \\&amp;=\delta_t +\gamma\delta_{t+1}+\gamma^2(G_{t+2}- v(S_{t+2})) \\&amp;= \cdots \\&amp;=\sum_{k=t}^{T-1} {\gamma^{k-t}\delta_k} \end{aligned}\] <p>Now, let’s plug this into the updating equation and change the order of summation:</p> \[\begin{aligned} \Delta \mathbf{w}_{k+1} &amp;= \sum_{t=0}^{T-1} {\alpha (G_t-v(S_t))\mathbf{x}(S_t)} \\ &amp;= \sum_{t=0}^{T-1} {\alpha \left(\sum_{k=t}^{T-1} {\gamma^{k-t}\delta_k}\right)\mathbf{x}(S_t)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\left(\sum_{t=0}^{k} {\gamma^{k-t}}\mathbf{x}(S_t)\right)} \end{aligned}\] <p>Defining the eligibility trace $\mathbf{e}_{k} := \sum_{t=0}^{k} {\gamma^{k-t}}\mathbf{x}(S_t)$ and renaming the summation index $k$ to $t$, we have:</p> \[\begin{aligned} \Delta \mathbf{w}_{k+1} &amp;= \sum_{k=0}^{T-1} {\alpha\delta_k\mathbf{e}_k} \\ &amp;= \sum_{t=0}^{T-1} {\alpha\delta_t\mathbf{e}_t}, \end{aligned}\] <p>plus the recursion relation of the eligibility trace:</p> \[\mathbf{e}_t=\gamma\mathbf{e}_{t-1}+\mathbf{x}_t\] <p>What’s “magical”, as Hado mentions in the lecture, is that the term $\alpha\delta_t\mathbf{e}_t$ now does not involve future terms at all! Therefore, we can now update the weights online, and obtain (almost) the same results as the original MC.</p> <p>Even if we choose not to update the values online and instead accumulate the summation until the end of the episode, the required memory remains independent of the episode’s duration. In this case, the result will exactly equal the result of the original MC.</p> <p>By altering the recursion relation as follows, we can generalize this method to the $\lambda$-TD case:</p> \[\tilde{\mathbf{e}}_t=\gamma\lambda\tilde{\mathbf{e}}_{t-1}+\mathbf{x}_t.\] <p>The derivation is similar:</p> \[\begin{aligned} G_t^\lambda-v(S_t) &amp;= R_{t+1}+\gamma((1-\lambda)v(S_{t+1})+\lambda G_{t+1}^\lambda)-v(S_t)\\ &amp;= R_{t+1}+\gamma((1-\lambda)v(S_{t+1})+\lambda G_{t+1}^\lambda)-v(S_t)+ \gamma\lambda v(S_{t+1}) - \gamma\lambda v(S_{t+1})\\ &amp;= (R_{t+1}+\gamma v(S_{t+1})-v(S_t)) + \gamma\lambda (G_{t+1}^\lambda - v(S_{t+1})) \\ &amp;= \delta_t+\gamma\lambda(G_{t+1}^\lambda - v(S_{t+1}))\\&amp;=\delta_t +\gamma\lambda\delta_{t+1}+\gamma^2\lambda^2(G_{t+2}^\lambda - v(S_{t+2})) \\&amp;= \cdots \\&amp;=\sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k},\\\Delta \mathbf{w}_{k+1} &amp;= \sum_{t=0}^{T-1} {\alpha (G_t^\lambda-v(S_t))\mathbf{x}(S_t)} \\ &amp;= \sum_{t=0}^{T-1} {\alpha \left(\sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k}\right)\mathbf{x}(S_t)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\left(\sum_{t=0}^{k} {(\gamma\lambda)^{k-t}}\mathbf{x}(S_t)\right)} \\&amp;= \sum_{k=0}^{T-1} { \alpha \delta_k\tilde{\mathbf{e}}_t} \\\text{where}\\ \tilde{\mathbf{e}}_t &amp;:= \sum_{k=t}^{T-1} {(\gamma\lambda)^{k-t}\delta_k}. \end{aligned}\] </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/signing-right-away/">Signing Right Away</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ai-science-and-humanities/">AI, Science, and the Humanities</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/reinforcement-learning-basics/">Reinforcement Learning Basics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/gradient-descent/">Gradient Descent</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yejun Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>