<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient Descent | Yejun Jang </title> <meta name="author" content="Yejun Jang"> <meta name="description" content="Understanding optimization problems, gradient descent algorithms, and local vs global minima"> <meta name="keywords" content="reinforcement-learning, deep-learning, multi-agent-systems, AI-research"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://codingjang.github.io/blog/2023/gradient-descent/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yejun</span> Jang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gradient Descent</h1> <p class="post-meta"> Created on January 29, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>   <a href="/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> gradient-descent</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/tutorial"> <i class="fa-solid fa-hashtag fa-sm"></i> tutorial</a>   <a href="/blog/tag/english"> <i class="fa-solid fa-hashtag fa-sm"></i> english</a>   ·   <a href="/blog/category/education"> <i class="fa-solid fa-tag fa-sm"></i> education</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This essay was originally written in Korean and was machine translated by GPT-4.</p> <h2 id="what-is-an-optimization-problem">What is an Optimization Problem?</h2> <p>An <strong>optimization problem</strong> seeks the <strong>optimal solution</strong> from all possible solutions. For instance, a researcher at an IT company developing navigation software would aim to provide users with the “fastest traffic route”. Similarly, someone with a background in industrial engineering might look for a “business model that maximizes profitability”. Both are tackling optimization problems. There are multiple ways to travel from point $A$ to $B$ using public transportation. Similarly, countless business models can make a company profitable. However, people naturally desire the quickest route and the most profitable business model. In this context, optimization problems are very familiar to computer scientists, industrial engineers, and even business and economics professionals.</p> <p>To determine the best or <strong>optimal</strong> state, we need a standard for what it good and what is not. Optimization problems are usually represented by a function that measures “how good” some task is done, called the <strong>objective function</strong>. The goal in optimization problems is to maximize or minimize this objective function. For example, consider a middle school student, Cheolsu, taking his first math test. If we frame his situation as an optimization problem, the test score becomes the objective function, and the optimization problem becomes “maximizing the test score”. If he does exceptionally well, he can score $100$, making the optimal solution when the test score (objective function) equals $100$.</p> <p>But what exactly is this “objective function” a function of? In Cheolsu’s case, the synaptic connections in his brain, represented by an $n$-dimensional vector $\theta$, determine his test score $s$. Hence, the test score, which is also the objective function $s$, can be considered a function of $\theta$. Cheolsu probably has a vast number of synapses, so $n$ would be a very large number. If cheolsu is smart enough, he can adjust his synaptic connections through learning to maximize his test score $s(θ)$. If he scores $100$ on the test, then there exists a $θ_0$ such that $s(θ_0) = 100$, making $θ_0$ the optimal solution.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>In general, the objective function in optimization problems is usually a function from $\mathbb{R}^n$ to $\mathbb{R}$, where $\mathbb{R}^n$ represents an $n$-dimensional real vector space. Although there may be various ways to express optimization problems, <strong>the standard form (continuous) optimization problem</strong> is usually represented as follows:</p> \[\begin{equation}\begin{aligned}&amp; \underset{\theta \in \mathbb{R}^n}{\text{minimize}}&amp; &amp; f(\theta) \\&amp; \text{subject to}&amp; &amp; g_i(\theta) \leq 0, \; i = 1, \ldots, m.\\&amp;&amp;&amp; h_j(\theta) = 0, \; j = 1, \ldots, p.\end{aligned}\end{equation}\] <p>Here, the objective function is called $f$, and the constraint functions are called $g_i$ and $h_j$ for $i=1,\dots,m$ and $j=1,\dots,p$. All of these functions maps $\mathbb{R}^n$ to $\mathbb{R}$, which means that it takes in as input an $n$-dimensional vector and outputs a single number. Unlike what was mentioned earlier about maximizing the test score, it’s traditional in standard optimization problems to minimize the objective function $f$. In fact, maximizing and minimizing differ only by a sign. To convert a maximization problem into a minimization problem, simply put a minus sign in front of the objective function. Instead of maximizing Cheol-su’s test score $s(\theta)$, if we set $-s(\theta)$ as $f(\theta)$ and mimimize it, the test score maximization problem can be represented in the standard form.</p> <p>The functions $g_i$ and $h_j$ represent <strong>inequality and equality constraints</strong>, respectively. For instance, consider a case where a neuron can be damaged if the synaptic connection strength becomes too strong. In such a case, we can set $g_i(\theta):= \theta_i -M\;(M&gt;0)$. Alternatively, let’s say that studying something too difficult for an exam can worsen one’s quality of life (QoL). In that case, we can record the previous QoL as $T_i$ and obtain the current QoL as $t_i(\theta)$. Then, we can set $h_i(\theta):= t_i(\theta)-T_i$. If these examples are hard to understand, remember that both the objective function and constraints are chosen appropriately based on real-life situations, and there’s no fixed standard.</p> <h2 id="single-variable-optimization-problems"><strong>Single-variable Optimization Problems</strong></h2> <p>In the previous example, Cheolsu’s brain had numerous synaptic connections represented by a vector $θ$. We’re trying to find the optimal $θ_0$ by adjusting this vector. To simplify, let’s start with the simplest case where $n=1$ and gradually generalize. How can we optimize a single-variable function $f(θ)$? Consider the standard optimization problem:</p> \[\begin{equation}\begin{aligned}&amp; \underset{\theta \in \mathbb{R}}{\text{minimize}}&amp; &amp; f(\theta)=\theta^2 \end{aligned}\end{equation}\] <p>Since there’s no “subject to” phrase, there are no specific constraints. We’ve already tackled the problem of minimizing the above function back in middle school. At that time, we accepted and used the fact that the square of a real number, $x^2$, is always non-negative and that the necessary and sufficient condition for $x^2$ to be zero is when $x=0$. Knowing these two facts, it’s easy to deduce that the minimum value of $x^2$ is $0$.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> As we progress to high school level, we can approach this problem using more advanced tools like the second derivative test. (Or, we might have just memorized that the minimum value of that function is $0$…)</p> <p>In any case, the above scenario is a very simple one, so finding the optimal solution isn’t particularly challenging. However, real-world problems aren’t always this straightforward. Most of the time, there isn’t an optimal solution that can be memorized using a formula. And although it is true that examining the point where the derivative becomes zero is a powerful method applicable to many general situations, there are cases where the equation $f’(\theta)=0$ becomes unsolvable and doesn’t lead to the general solution.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> In that case, we need a way to solve optimization problems without having to solve the above equation. How can we minimize the function value without being constrained by the function’s form?</p> <h2 id="univariate-gradient-descent">Univariate Gradient Descent</h2> <p>It’s challenging to search for points where $f’(\theta) = 0$ across the entire range of real numbers, but it’s fairly easy to calculate $f’(\theta)$ at a particular point and check its sign. If so, it seems like it’s good idea to slightly decrease $\theta$ whenever $f’(\theta)$ is greater than $0$. Similarly, let’s slightly increase $\theta$ whenever $f’(\theta)$ is less than $0$. If we continue this process, $f(\theta)$ will keep decreasing.</p> <p><img src="/assets/img/blog/deep-learning/untitled.jpeg" alt="Untitled"></p> <p>The process can be expressed with the following equation:</p> \[\begin{equation}\theta^{(k+1)}=\theta^{(k)}-\alpha\:\text{sgn}\:f'(\theta^{(k)})\end{equation}\] <p>In this, $\alpha$ is a sufficiently small positive number. The notation $\text{sgn} \;\cdot$ represents the sign of $\cdot$.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> Also, $\theta^{(k)}$ denotes the value of $\theta$ at the $k$-th step. If our intuition is correct, the real variable $\theta$ starts from its initial value $\theta^{(0)}$ at the very first step, $k=0$, and gradually approaches the minimum point according to the above recurrence relation. Specifically, unless $f(\theta)$ is an unusual function, $\theta$ will approach the global minimum point of $f(\theta)$, which is $\theta=0$. Let’s verify that as the value of $k$ increases, $\theta^{(k)}$ generally decreases.</p> <p>However, there’s a problem with the above approach. The magnitude of $\alpha\:\text{sgn}\:f’(\theta^{(k)})$,</p> \[\begin{equation} \left|\alpha\:\text{sgn}\:f'(\theta^{(k)})\right|=\left|\alpha \cdot(\pm1)\right| \end{equation}\] <p>is always $\alpha$. This means that even when approaching the minimum value, $\theta^{(k)}$ is updated only by a fixed step-size of $\alpha$. In other words, if we only consider the sign, we won’t converge to the minimum value and will instead be oscillating by a magnitude of $\alpha$! Therefore, we should gradually decelerate as we approach the exact minimum value.</p> <p>On the other hand, we can know when we’re close to the minimum value by noticing that the magnitude of $\lvert f’(\theta)\rvert$ has decreased, i.e., $f’(\theta)$ is close to $0$. Can we use this information to improve the equation? In fact, since $f’(\theta^{(k)})$ is already a real number with a sign, there’s no need to use the $\text{sgn}$ function. Removing the $\text{sgn}$ function gives:</p> \[\begin{equation} \theta^{(k+1)}=\theta^{(k)}-\alpha f'(\theta^{(k)})\end{equation}\] <p>This equation addresses the issue in the previous equation where the step-size was fixed at $\alpha$. As $\theta^{(k)}$ approaches the minimum value, the magnitude (absolute value) of $f’(\theta^{(k)})$ decreases, which means that we’ll be decelerating. The direction of movement will remains the same as before, reducing the value of $f$. Thus, it seems all the previous issues have been resolved. The method of optimizing a function as in the above equation is called Gradient Descent (GD).</p> <p>Surprisingly, for simple functions like $f(\theta)=\theta^2$ we can use concepts of sequences and differentiation learned in high school to determine and prove the conditions under which $\theta^{(k)}$ converges to the state $\theta=0$, the minimum point. Let’s assume $\alpha=0.01$ and start from $\theta^{(0)}=1$. Since $f’(\theta)=2\theta$, substituting this into the original equation gives:</p> \[\begin{align} \theta^{(k+1)}&amp;=\theta^{(k)}-0.02\:\theta^{(k)}=0.98\:\theta^{(k)},\\ \theta^{(k)}&amp;=0.98^k \theta^{(0)}=0.98^k. \end{align}\] <p>Since $0&lt;0.98&lt;1$, the sequence $\theta^{(k)}$ converges to $0$ as $k$ approaches infinity.</p> \[\begin{equation} \theta^{(k)}=0.98^k \rightarrow 0 \;\;\text{as}\;\; k\rightarrow \infty. \end{equation}\] <p>However, if the magnitude of $\alpha$ is too large, the sequence $\theta^{(k)}$ might diverge! For instance, if $\alpha=2$ and we start from $\theta^{(0)}=1$ as before:</p> \[\begin{equation} \begin{aligned} \theta^{(k+1)}&amp;=\theta^{(k)}-4\:\theta^{(k)}=(-3)\,\theta^{(k)}, \\ \theta^{(k)}&amp;=(-3)^k, \\ \theta^{(0)}&amp;=(-3)^k. \end{aligned} \end{equation}\] <p>In this case, the sequence $\theta^{(k)}$ oscillates and diverges as $k$ approaches infinity.</p> \[\begin{equation} \theta^{(k)}=(-3)^k \text{ oscillates as } k\rightarrow \infty. \end{equation}\] <p><strong>Check Problem 1.</strong></p> <p>When applying gradient descent to the function $f(\theta) = \theta^2$, what are the conditions for $\alpha$ and $\theta^{(0)}$ for $\theta^{(k)}$ to converge to the optimal solution $\theta = 0$? Note that $\alpha &gt; 0$.</p> <ul> <li>Answer $0&lt;\alpha&lt;1$ or $\theta^{(0)}=0.$</li> </ul> <p><strong>Coding Problem 1.</strong></p> <p>Let’s apply gradient descent to the function $f(\theta) = (\theta + 3)^2$. Specifically, with a learning rate of $\alpha = 0.7$ and starting from the condition $\theta^{(0)} = -4$, update the value of $\theta$ according to equation $(4)$. List the values of $\theta$ from $\theta^{(0)}$ to $\theta^{(30)}$ and use the <code class="language-plaintext highlighter-rouge">print()</code> function to display them.</p> <ul> <li>Answer <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Code
</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">theta</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># Create an empty list
</span><span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>      <span class="c1"># Save initial theta
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># Apply gradient descent
</span>    <span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>             <span class="c1"># Print values for theta
</span></code></pre></div> </div> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Output
</span>
<span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.16</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.936</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.98976</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.004096</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9983616</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.00065536</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999737856</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0001048576000002</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999580569599997</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000016777216</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999932891136</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.00000268435456</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999998926258176</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000004294967297</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999998282013083</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000687194768</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999725122093</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000109951164</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999956019536</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0000000017592185</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999992963127</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000281475</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.99999999988741</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000045036</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9999999999819855</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000007206</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.999999999997118</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.000000000001153</span><span class="p">]</span>
</code></pre></div> </div> </li> </ul> <h2 id="local-minima-and-global-minima">Local Minima and Global Minima</h2> <p><img src="/assets/img/blog/deep-learning/untitled_day_9_161f0f24f931802a97f3e1b6.png" alt="Untitled"></p> <p>The <strong>local minimum point</strong> of a function $f(\theta)$, denoted as $\theta_\text{local}$, is a point where the function value is less than or equal to the values in the neighborhood<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> of $\theta$, i.e., $f(\theta)\ge f(\theta_\text{local})$ for $\theta$ in the neighborhood of $\theta_\text{local}$. In the graph of the univariate real function $f$ above, with the horizontal axis as the $\theta$-axis, the points $\theta=0$ and $\theta=10$ are local minima points because their function values are always less than those of their neighboring points. The function value at a minimum point is called the minimum value. Thus, the function $f$ has a local minimum value of $f(0)=0$ at $\theta=0$ and a local minimum value of $f(10)=50$ at $\theta=10$.</p> <p>On the other hand, the <strong>global minimum point</strong> of the function $f(\theta)$, denoted as $\theta_\text{global}$, is a point where the function value is less than or equal to the values at all points in its domain, i.e., $f(\theta)\ge f(\theta_\text{global})$ for all $\theta$ within the domain of $f$. In the above case, $\theta=0$ is both a local and global minimum, but $\theta=10$ is only a local minimum. This is because the function value at $\theta=0$, which is $f(0)=0$, is less than the function value at $\theta=10$, which is $f(10)=50$.</p> <p>When we applied the gradient descent method to the function $f(\theta)=\theta^2$, we observed that if $\alpha$ is sufficiently small, the sequence $\theta^{(k)}$ converges well to the optimal solution $\theta=0$. However, for $f(\theta)=\theta^2$, as there’s only one local minimum (and because the function values shoot to $\infty$ as $\theta$ approaches $\pm \infty$), the local minimum is the same as the global minimum. But for functions like $f$ in the above graph with multiple local minima, it’s uncertain whether $\theta$ will converge to the global minimum, as it can get stuck in a minimum that is local but not global.</p> <p>For instance, in some cases, a company’s profit structure might be hard to improve with minor adjustments. If any slight modification to the current structure only worsens profitability, the company might need to completely overhaul its structure to expect significant profit growth. This can be interpreted as an effort to escape from a local minimum. Another example is misconceptions during learning. Even with misconceptions, one might still be able to explain phenomena adequately. However, as one encounters more information, it’s more effective to break away from the misconception and relearn, enhancing the ability to explain phenomena. In other words, it’s preferable to move away from the misconception (local minimum) and relearn a more accurate concept (global minimum).</p> <p>While gradient descent ensures convergence to a local minimum under appropriate assumptions, it doesn’t guarantee convergence to a global minimum.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> Hence, optimization algorithms like Momentum and Adam, which include inertial terms to escape shallow local minima and head towards deeper global minima with smaller function values, have been developed. If you’re interested in exploring various optimization techniques like Momentum and Adam, consider checking out the referenced post below after reading Part II of this series.</p> <p><a href="https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/" rel="external nofollow noopener" target="_blank">A Comprehensive Guide on Optimizers in Deep Learning</a></p> <p><strong>Check Problem 2.</strong></p> <p>For the quartic function $f(\theta) = \theta^2(\theta-1)(\theta-2)$:</p> <p>(a) Find the local minimum point.</p> <p>(b) Find the global minimum point.</p> <p>(c) Determine the minimum value at the point which is a local minimum but not a global minimum.</p> <ul> <li>Answer (a) local minimum point: $\theta=0,\;\theta=\frac{9+\sqrt{17}}{8}\approx1.640$ (b) global minimum point: $\theta =\frac{9+\sqrt{17}}{8}\approx1.640$ (c) $f(0)=0$</li> </ul> <p><strong>Coding Problem 2.</strong></p> <p>Let’s apply the gradient descent method to the quartic function $f(\theta) = \theta^2(\theta-1)(\theta-2)$.</p> <p>(a) With a learning rate of $\alpha = 0.05$ and starting from $\theta^{(0)} = 3$, update the value of $\theta$ using equation $(4)$. List the values of $\theta$ from $\theta^{(0)}$ to $\theta^{(30)}$ and print them using the <code class="language-plaintext highlighter-rouge">print()</code> function.</p> <p>(b) Check if $\theta$ converges to the global minimum. If it does, find the convergence value of $f(\theta^{(k)})$.</p> <p>(c) Identify one initial value of $\theta^{(0)}$ for which $\theta$ does not converge to the global minimum. Note that $\alpha = 0.05$.</p> <ul> <li>Answer (a) <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Code
</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># Create empty list
</span><span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>      <span class="c1"># Save initial theta
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="c1"># Gradient descent
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">9</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
    <span class="n">thetas</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">thetas</span><span class="p">)</span>             <span class="c1"># Print values of theta
</span></code></pre></div> </div> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># 출력값
</span>
<span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.0499999999999998</span><span class="p">,</span> <span class="mf">1.1045999999999998</span><span class="p">,</span> <span class="mf">1.1631899369327998</span><span class="p">,</span> <span class="mf">1.2246451065084327</span><span class="p">,</span> <span class="mf">1.2872724414731267</span><span class="p">,</span> <span class="mf">1.3488794094514513</span><span class="p">,</span> <span class="mf">1.4070169240305987</span><span class="p">,</span> <span class="mf">1.4593836892273668</span><span class="p">,</span> <span class="mf">1.5042779940878397</span><span class="p">,</span> <span class="mf">1.540914144056721</span><span class="p">,</span> <span class="mf">1.5694643322257227</span><span class="p">,</span> <span class="mf">1.5908330465698923</span><span class="p">,</span> <span class="mf">1.6063037620026415</span><span class="p">,</span> <span class="mf">1.6172175322689708</span><span class="p">,</span> <span class="mf">1.6247689357696435</span><span class="p">,</span> <span class="mf">1.629921406541151</span><span class="p">,</span> <span class="mf">1.6334027143439558</span><span class="p">,</span> <span class="mf">1.6357390290643616</span><span class="p">,</span> <span class="mf">1.6372997348435627</span><span class="p">,</span> <span class="mf">1.6383390867159184</span><span class="p">,</span> <span class="mf">1.6390298045931315</span><span class="p">,</span> <span class="mf">1.6394881953333464</span><span class="p">,</span> <span class="mf">1.6397921226262633</span><span class="p">,</span> <span class="mf">1.6399935122140699</span><span class="p">,</span> <span class="mf">1.640126903506169</span><span class="p">,</span> <span class="mf">1.6402152319777135</span><span class="p">,</span> <span class="mf">1.6402737104854812</span><span class="p">,</span> <span class="mf">1.6403124220218963</span><span class="p">,</span> <span class="mf">1.6403380462312662</span><span class="p">,</span> <span class="mf">1.640355006705482</span><span class="p">]</span>
</code></pre></div> </div> <p>(b) We can observe that theta is converging to the global minimum point 1.640. To evaluate the function at this point:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Code
</span>
<span class="n">theta_global</span> <span class="o">=</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">30</span><span class="p">]</span>  <span class="c1"># Assume the 30th theta is sufficiently close to the global minimum point
</span><span class="nf">print</span><span class="p">(</span><span class="n">theta_global</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_global</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_global</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
</code></pre></div> </div> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Output
</span>
<span class="o">-</span><span class="mf">0.6196843457001793</span>
</code></pre></div> </div> <p>(c) If we run the code with $\theta^{(0)}=-1$,</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Output
</span>
<span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1499999999999999</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.10919999999999994</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.08173347786239996</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06227141782417553</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0480238616518109</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.037359106839121914</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.029248790740098913</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.023009056880295777</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.018166571714116383</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01438354734163941</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.011413143825789467</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00907160079235181</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0072200990529043794</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0057525455421655915</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.004587107060241312</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0036601976461915</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.002922119638721227</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0023338482682754252</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.001864624990714408</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0014901341241158653</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011911074126556803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0009522471605601266</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0007613895071587255</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0006088506461576267</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00048691365718682596</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00038942421445218494</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00031147111670195584</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0002491332309026941</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00019927865131451355</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00015940504907746946</span><span class="p">]</span>
</code></pre></div> </div> <p>We can observe that theta is converging to the point $\theta=0$ (which is a local minimum point, but not a global minimum point)</p> </li> </ul> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Let’s note that Cheolsu’s synaptic connection state $\theta_0$, which allows him to score $100$ points in the test, may not be unique. In other words, the solution to the optimization problem may not be unique. Moreover, if Cheolsu was not a middle school student but a lizard, the synaptic connection state $\theta_0$ that allows him to achieve a test score of $100$ points might physically be impossible and might not even exist. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2"> <p>Finding the minimum value of a function means that there exists an optimal solution $\theta_0$ that minimizes the function, such that for any $\theta$ within its domain, $f(\theta)\ge f(\theta_0)$. If we set $\theta_0=0$, the proposition holds true based on the two facts mentioned earlier. More will be discussed towards the end of Part I. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3"> <p>For example, consider a 6th-degree polynomial given by $f(\theta)=c_6\theta^6+c_5\theta^5+\cdots+c_1\theta+c_0$. We know that if $c_6&gt;0$, a minimum value exists somewhere in the set of real numbers. However, the derivative $f’(\theta)=6c_6\theta^5+5c_5\theta^4+\cdots+c_1=0$ is a $5$th-degree polynomial. Abel and Galois proved that polynomials of degree $5$ or higher do not have a general solution formula. Therefore, we cannot solve $f’(\theta)=0$except special cases. Even if it’s not a polynomial of degree $5$ or higher, the usual strategy of “finding the point where the derivative is zero” often doesn’t work well when the equation becomes even slightly complex. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:4"> <p>$+1$ if $\cdot$ is positive, $-1$ if negative. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:5"> <p>The ‘neighborhood’ of a point refers to a small open interval (or an open set) that includes that point. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:6"> <p>Specifically, if a function $f$ is differentiable and its derivative $f’$ is $L$-Lipschitz continuous, then $f’$ is guaranteed to converge to $0$ only when $0 &lt; \alpha &lt; 2/L$. Additionally, for $f(\theta^{(k)})$ to converge to a local minimum, it is necessary that the function $f$ does not have a saddle point, among other conditions. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mutual-cofounder-hardware/">Co-Founder (Hardware Engineer) — mutual</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mutual-cofounder-hardware-kr/">공동창업자 (하드웨어 엔지니어) — mutual</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/introducing-mutual/">Introducing mutual</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/introducing-mutual-kr/">mutual을 소개합니다</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/information-compression-neural-networks-and-ai-for-science/">Toward a Unified Theory</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yejun Jang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>