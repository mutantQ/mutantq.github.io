# SOUL 발표 대본 (폐기)

**거위=인공신경망, 거위를 전쟁전략에 사용=심층강화학습**

이상한 나라에 거위들이 살았는데, 이곳의 거위들은 모두 깨진 알을 낳았다. 아무리 맛있는 먹이를 줘도, 거위들은 항상 깨진 알을 낳았다. 농부들은 깨진 알을 낳는 거위를 쓸모없다고 생각해 아무도 신경 쓰지 않았다. 그러나 몇몇 근성 있는 농부들은 계속 먹이를 바꿔가며 거위가 성한 알을 낳도록 하는 데 전념했다.

그렇게 연구에 몰두하던 중, 한 거위 연구팀은 거위에게 종합비타민을 주자 거위가 말을 듣기 시작한다는 것을 알아냈다. 종합비타민을 먹은 거위는 정말 특별했다. 성한 알을 어떻게 낳는지 보여주고, 낳은 알이 성한 알과 비슷할수록 맛있는 먹이를 주니 점점 성한 알을 낳게 되었다. 한 농부는 황금알을 보여준 다음, 낳은 알이 황금색에 가까울수록 맛있는 먹이를 주었다. 그랬더니 거위는 황금알도 잘 낳게 되었다.

거위는 사실 어떤 알이든 낳을 수 있었다. 이상한 나라가 요상한 나라와 전쟁이 나자, 이상한 나라의 거위 연구팀은 전쟁 전략을 거위에서 찾았다. 많은 농부들이 연구에 참여해 군사들의 배치 상황을 보여주고, 진격의 의미로 빨간색, 후퇴의 의미로 파란색 알을 낳게 했다. 그리고는 군사들이 피해를 입으면 맛없는 먹이를, 전투에서 승리하면 맛있는 먹이를 주었다. 학습이 끝난 거위의 전략은 다른 어떤 전략보다 우월했고, 덕분에 이상한 나라는 요상한 나라와의 전쟁에서 승리했다.

깨진 알을 낳던 거위처럼, 인공신경망도 처음에는 의미없는 출력을 내놓는 입력-출력 구조에 불과했다. 그러나 여러 학습 기술의 발전과 컴퓨터 연산 속도의 증가로 황금알을 낳는 거위로 재탄생했다. 인공신경망이 최적 전략 탐색에 본격적으로 사용되기 시작한 것은 불과 10년 전으로, 알파고를 개발한 딥마인드 팀이 벽돌 깨기 게임에서 만점을 획득한 알고리즘 DQN을 개발한 것이 그 첫 신호탄이었다. 강화학습이 인공신경망과 만나 심층강화학습이라는 새로운 패러다임을 연 것이다.

안녕하세요, ~에 대한 설명을 모두 들으셨다면, 이를 검증하기 위한 시뮬레이션 실험이 어떻게 진행되었는지 설명해보도록 하겠습니다.

실험을 설명하기에 앞서 저희가 사용한 인공지능이 동작하는 원리에 대해 간략하게 설명할 필요성이 있습니다.

이상한 나라에 거위들이 살았습니다. 이곳에 사는 거위들은 모두 깨진 알을 낳습니다. 아무리 맛있는 먹이를 줘봐도, 거위들은 항상 깨진 알을 낳았습니다. 농부들은 깨진 알을 낳는 거위는 쓸모 없다 생각하여 아무도 쳐다보지 않았습니다. 하지만 몇몇 근성 있는 농부들은 계속 먹이를 바꿔가며 거위가 성한 알을 낳도록 하는 일에 전념했습니다.

그렇게 연구에 전념하던 어느 날, 한 거위 연구팀은 거위에 종합비타민을 주었더니 거위가 말을 듣기 시작한다는 것을 깨달았습니다. 종합비타민을 먹인 거위는 정말 특별했습니다. 성한 알이 어떻게 생겼는지 보여준 다음, 낳은 알이 성한 알에 비슷할수록 더 맛있는 먹이를 주었더니 점점 낳는 알이 성한 알로 변했습니다. 한 농부는 황금알을 예시로 보여주고, 낳는 알이 황금색에 가까울수록 맛있는 먹이를 주어보았습니다. 그랬더니 거위는 황금알도 곧잘 낳는 것이었습니다.

알고보니 거위는 어떤 알이든 낳을 수 있었습니다. 이상한 나라가 요상한 나라와 전쟁이 나자, 이상한 나라의 거위 연구팀은 전쟁 전략을 거위에서 찾았습니다. 수많은 농부들이 연구에 참여했고, 군사들의 배치 상황을 보여준 다음 진격의 의미로 빨간색, 그리고 후퇴의 의미로 파란색 알을 낳게 했습니다. 그리고는 군사들이 피해를 입으면 맛없는 먹이를, 전투에서 승리하면 맛있는 먹이를 주었습니다. 거위의 전략은 그 어떤 전략보다 우월했고, 덕분에 이상한 나라는 요상한 나라와의 전쟁에서 승리할 수 있었습니다.

깨진 알을 낳던 거위처럼, 인공신경망 역시 의미없는 출력을 내뱉는 입력-출력 구조에 불과했으나 여러 학습 테크닉의 발견과 컴퓨터의 연산 속도 증가에 힘입어 황금알을 낳는 거위로 재탄생 했습니다. 인공신경망이 최적 전략 학습에 본격적으로 활용되기 시작하는 것은 2013년이며, 알파고를 개발한 딥마인드 팀에서는 벽돌 깨기 게임에서 만점을 획득하는 인공지능인 DQN을 내놓았습니다.

거위 = neural network

거위의 먹이 = loss or reward

거위가 보는 것 = input of the neural network

거위가 낳는 것 = output of the neural network

종합비타민 = several techniques that made neural nets feasible

깨진 알과 성한 알 = bad output and good output

 

## 망했고 다시 하자

### 목차 정의

1. 왜 강화학습으로 하려고 하는지 
2. **강화학습에 대한 간략한 설명**
    1. **강화학습에서는 ‘똑똑한’ 에이전트agent가 행동action을 취해가면서 환경environment로부터 보상reward을 획득하는 일련의 결정 과정을 다룸. (이를 마르코프 결정 과정이라고 함)**
        - 간단한 예시로 설명하자면, 배가 고픈 상태state에 있으면 밥을 먹는 행동action을 하는 것이 이 세상(=환경environment) 속에서 생존(=보상reward)하는데 도움이 됨. (마인크래프트 게임 화면)
        - 어떤 상태에 놓였을 때, 행동에 따라 보상의 수준이 달라짐. 당연한 이야기이지만, 배가 고픈 상태에서 밥을 먹지 않고 굶는 행동을 취하면 우리는 고통받으며 낮은 수준의 보상을 경험하게 됨. (마인크래프트 you died)
    2. **강화학습에서는 MDP 내에서 처음부터 끝까지 받는 보상의 합을 최대화하는 방법론을 연구함.**
        - 환경으로부터 설정된 규칙 - 예를 들어, 게임 규칙 / 물리 법칙은 변하지 않으므로 유일하게 바꿀 수 있는 것은 전략(=정책)임.
        - 따라서 강화학습은 최적 전략을 찾는 문제로 귀결됨. 예를 들어, 내 인생 속에서 얻는 행복을 수치화할 수 있다면, 강화학습에서는 행복의 총합을 최대화할 수 있는 가장 좋은 인생의 행동 규칙을 학습하고자 함.
    3. **인공신경망은 이제 황금알을 낳는 거위가 되었음.**
        - 최근 “유명한” 인공지능은 모두 인공신경망을 활용함.
        - 이유는 인공신경망이 이론 상 어떤 입력-출력 관계든지 나타낼 수 있는 강력한 도구이기 때문.
            - 더 쉽게 풀자면 인공신경망이 만능에 가깝다는 것이 수학적으로 증명되었음.
            (단, 충분히 강력한 컴퓨터와 충분히 많은 데이터가 주어진다고 가정해야 함)
            - 인공신경망을 통해 인간만 풀 수 있는 것으로 여겨졌던 문제들 혹은 그 이상을 풀 수 있게 됨을 의미함. 대표적으로 이미지를 알맞은 그룹으로 분류하는 문제는 인간만이 풀 수 있다고 여겨졌으나, 이제는 인공지능이 인간보다 정확함.
            - 보상을 받는 게임 내에서의 전략 설정 역시 비슷한 예시 중 하나였으나, 최근 들어 다양한 분야에서 인간 수준을 능가하는 인공지능이 등장하고 있음.
    4. **이미지 분류 문제에서는 $\text{(image, label)}$ 순서쌍을 만들어 입출력 관계를 학습했다면,
    정책 기반 강화학습에서는 $\text{(state, action)}$ 순서쌍을 만들어 입출력 관계를 학습함.**
        - $\text{state}$를 넣으면 $\text{action}$이 나오는 입출력 관계를 정책policy이라고 부름.
        - 게임의 규칙만을 통해 유의미한 최적 전략을 도출할 수 있다는 것이 강화학습의 최대 장점.
            - 최근 컴퓨터가 인간이 쉽게 하는 일들을 오히려 못한다는, 모라벡의 역설이 점차 깨지고 있음.
            - 실험적이고 도전적인 SOUL 프로젝트의 목적 의식에 가장 잘 부합하는 시기적절한 방법론임.
3. **시뮬레이션 목표와 구현 과정**
    1. **$\text{agent}$는 각 국가로, $\text{state}$는 GDP/GDP성장률/물가/인플레이션율로, $\text{action}$은 매 기마다 정해지는 이자율, 마지막으로 보상은 매 기 GDP성장률과 인플레이션율을 반영한 후생 함수로 정의함.**
        - 여러 에이전트가 환경 속에서 상호작용하는 상황에서의 강화학습을 멀티에이전트 강화학습 혹은 다중에이전트 강화학습이라고 하는데, 다국가 간 정치경제적 관계를 다루는 우리의 연구가 이에 해당함.
        - 리바이어던에서 등장하는 현실주의 이론을 바탕으로 각 에이전트가 경험하는 보상을 독립적으로 최대화하도록 설정함.
    - **시뮬레이션으로 이루고자 하는 목표:**
    1. **에이전트가 친밀도를 증가시킬 수 있는 초대/수락 메커니즘을 정의하여 정치 행동이 가능하도록 함.**
        - 어떤 국가가 서로 협력 관계에 있는지 알 수 있을 것으로 기대됨.
    2. **강화학습 연산 가속화를 위해 Ray RLlib과 Google Cloud VM을 사용함.**
        - 처음에는 고성능 컴퓨터에 그대로 프로그램을 옮겼더니 노트북보다 오히려 느리게 동작함. 대여한 고성능 컴퓨터의 성능을 100%로 활용하기 위해서는 생각보다 많은 노하우가 필요했음.
        - 일반 데스크탑 컴퓨터보다 약 10배 빠른 속도로 실험을 진행할 수 있었음.
4. **실험 결과** (일반 vs 방코르 적용 시 국제수지 흑자 적자 zero-out 되는지 확인)
    1. 내수 비율이 높은 국가일수록 GDP가 높게 나타남 (이게 맞나?)
    2. 에이전트 1은 GDP는 높지만 심한 디플레이션으로 인해 보상이 작게 나타남
        
        ![Untitled](SOUL%20%EB%B0%9C%ED%91%9C%20%EB%8C%80%EB%B3%B8%20(%ED%8F%90%EA%B8%B0)%20740959d003fa456d99ca3b2295f18452/Untitled.png)
        
5. 결론 및 제언
    1.