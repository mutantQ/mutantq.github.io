# 0. 정보 압축 이론과 인공신경망 근사 이론의 통합 가능성

### Information Compression in Neural Networks

## Meeting notes & Zoom link

- 2024.12.22
    - 홍철
        - Proposed to introduce low-rank assumption in the original proof of [https://arxiv.org/pdf/2107.05802](https://www.google.com/url?q=https://www.google.com/url?q%3Dhttps://arxiv.org/pdf/2107.05802%26amp;sa%3DD%26amp;source%3Deditors%26amp;ust%3D1737126095469724%26amp;usg%3DAOvVaw1r5OYcNOZHUBfk7wetu3zQ&sa=D&source=docs&ust=1737126095476271&usg=AOvVaw2-PFDlAwboV8Cz4n3VkGce) in Gordon’s Escape Theorem
        - The goal is to introduce connection in rank of datasets together with the degree of freedom of parameters to create a tighter bound
    - 예준
        - Suggested that we should explore options that could actually integrate information theory
- 2025.01.12
    - Should we ditch Information Bottleneck (IB)?
        - Since it seems like it would take too long, let's set it aside but keep the bottleneck concept in mind. Let's postpone thinking on this topic.
    - New direction: Gordon escape theorem + incorporating dataset intrinsic dimension.
        - need practical estimation algorithm for the dataset's intrinsic dimension. ex) PCA
        - **We need to give researchers a tool that can estimate the minimum amount of parameters needed to train for a certain task.**
    - TO DO:
        - 록기 + 홍철: need to review the proofs outlined in the original Gordon Escape Theorem paper
        - 예준: review the intrinsic dimension of datasets paper & present to the group. It’s okay to just share with the group via Discord.
        
        - 승환: review the experiment setup in the Gordon Escape Theorem original paper & the dataset intrinsic dimension paper
        - For example, how can we plot x = number of parameters vs y = intrinsic dimension?
    - Timeline
        - Jan: Problem formulation, proof and literature review
        - Feb: Prove our theorem
        - Mar: Experiments start at least by the end of March
        - April: Start writing paper + submit!
    - Next meeting: Tuesday
- 2025.01.21
    - @장예준 / 학생 / 전기·정보공학부 ­
        - Low intrinsic dimension does not necessarily mean an easier manifold (could be intertwined or crumbled to a complex shape)
        - Maybe the intrinsic dimension is irrelevant
    - @Hong Chul Nam
        - Make strong assumptions (assume 2D data in Gaussian, 2-layer MLP)
            - try to see if we can find some boundaries over this setting
        - LoRA seems to have some resemblance with projection setup except that the projection matrix is learnable in LoRA
- 2025.01.29
    - @Hong Chul Nam
        
        information bottleneck말고 다른 방법 탐색
        
        → “optimal transport”
        
        data mesure가 manifold 위에 있으면, manifold structure로 표현가능, 이를 옮기는 최적의 optimal transport
        
        거의 manifold hypothesis를 generalization 에 explicit 하게 넣은점에서 의의가 있음.
        
        neural network parameters 예측에 대해서는 크게 논의는 없음.
        
        문제: information theory가 parameter space(or geometry) 에 대해서만 분석하지 data space에 대해서 분석이 미비함?
        
        홍철: gordon’s escape theory를 더 발전시키기는 어렵워 보임. 파는 것과 동시에, gordon escape 말고도 다른 method 탐색도 필요함
        
        → 새로운 method 제안: 
        
        z \in R^n → z = (x, T(x)) x \in R^d, T(x) \in R^(n-d) → x-T → (x, T(x)) -f → prediction ⇒ I(f,Z)→I(f,Z)I(f,x)
        
        information bottleneck에서, z → model → y에서 I(z,y)를 통해 bound를 구함.
        
        n-d 차원은 determininistic한 T라는 함수로 구할 수 있다 (가정)
        
        그런데, row-dimension 이론을 사용하기 위해서, row-dim 데이터 x가 있다고하자. 이때, x→z→model→y 라고하면, z = (x,T(x)) 라 할 수 있을 것이다. 이에 대해서, I(X,Y)로 bound를 구할 수 있지 않을까?
        

## Papers Being Reviewed

Adjust priority as needed.

Can someone please categorize these papers?

- https://arxiv.org/abs/2107.05802
- https://arxiv.org/abs/physics/0004057
- https://arxiv.org/abs/1503.02406
- https://arxiv.org/abs/1711.01530
- [Intrinsic dimension of data representations in deep neural networks](https://arxiv.org/pdf/1905.12784) (p2)
- [Optimal Manifold Representation of Data: An Information Theoretic Approach](https://www.princeton.edu/~wbialek/our_papers/chigirev+bialek_04.pdf) (p2)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) (p2)
- [Proving the Lottery Ticket Hypothesis: Pruning is All You Need](https://arxiv.org/abs/2002.00585) (p2)
- [Neuronal Capacity](https://papers.nips.cc/paper_files/paper/2018/file/a292f1c5874b2be8395ffd75f313937f-Paper.pdf) (p3)
- [Manifolds, Random Matrices and Spectral Gaps: The geometric phases of generative diffusion](https://arxiv.org/pdf/2410.05898) (p3)
- [On Milman's inequality and random subspaces which escape through a mesh in *ℝ*ⁿ](https://link.springer.com/chapter/10.1007/BFb0081737) (p3)
    
    [BFb0081737.pdf](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/BFb0081737.pdf)
    
- [Some inequalities for Gaussian processes and applications](https://link.springer.com/article/10.1007/BF02759761) (p3)
    
    [BF02759761.pdf](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/BF02759761.pdf)
    
- [Sample complexity and effective dimension for regression on manifolds](https://arxiv.org/pdf/2006.07642) (p3)
- [How Does Information Bottleneck Help Deep Learning?](https://arxiv.org/abs/2305.18887) (p3)
- [Are All Layers Created Equal?](https://arxiv.org/abs/1902.01996) (p3)
- [Neural Basis Models for Interpretability](https://arxiv.org/abs/2205.14120) (p3)
- [MetaFun: Meta-Learning with Iterative Functional Updates](http://proceedings.mlr.press/v119/xu20i/xu20i.pdf) (p3)

## Additional Resources

https://arxiv.org/abs/2012.04115

https://mjt.cs.illinois.edu/dlt/

- https://slideslive.com/38938349
- [Network information theory](https://www-isl.stanford.edu/~abbas/presentations/Allerton09.pdf)
    
    [01.0_pp_i_iv_Frontmatter-merged.pdf](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/01.0_pp_i_iv_Frontmatter-merged.pdf)
    
- [Entropy and mutual information](https://www.ece.tufts.edu/ee/194NIT/lect01.pdf)
    - definition for joint entropy, conditional entropy and mutual information:
        
        $$
        \begin{align*}    H(X, Y) &= -\sum_{x, y} p(x, y) \log p(x, y), \\[10pt]    H(X|Y) &= -\sum_{x, y} p(x, y) \log p(x|y), \\[10pt]    I(X; Y) &= \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)} \end{align*}
        $$
        
    - Note that the notation $H(p, q)$ is also used for the cross entropy:
        
        $$
        H(p, q)=\sum_{i=1}^\mathcal{X} p_i \log q_i .
        $$
        
    - The KL divergence, or relative entropy is defined as:
        
        $$
        D_{KL}(p||q)=\sum_{i=1}^\mathcal{|X|}p_i \log \frac{p_i}{q_i}.
        $$
        
- [Information Bottleneck Method](https://arxiv.org/abs/physics/0004057)
    - We introduce the bottleneck $\tilde{X}$ to form the Markov chain $X \rightarrow \tilde{X} \rightarrow Y$, and drawing ideas from the rate-distortion theory we obtain:
        
        $$
        \underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})-\beta I(X;Y)
        $$
        
- [Neural tangent kernel](https://en.m.wikipedia.org/wiki/Neural_tangent_kernel)
    - The gradient descent of neural networks could be understood as the kernel gradient descent of the neural tangent kernel:
    - $\Theta(x, x'; \theta) := \nabla_\theta f(x; \theta) \cdot \nabla_\theta f(x'; \theta)$
- [Gordon’s escape theorem](https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-of-data-science-fall-2015/f9261308512f6b90e284599f94055bb4_MIT18_S096F15_Ses15_16.pdf)
    
    Gordon's escape theorem states that in high-dimensional spaces, a random subspace of sufficient dimension will "escape" through any mesh of low complexity with high probability. This theorem provides a powerful tool for understanding the behavior of random projections and has important applications in compressed sensing and dimensionality reduction.
    

## Where it all started from…

정보과학의 개론 과목을 배우다보면, 가장 대표적인 압축 알고리즘으로 허프만 코딩(Huffman coding)을 접하게 된다. 허프만 코딩은 정보를 나타내는 문자열로부터 반복을 제거함으로써, 정보의 훼손 없이 문자열의 길이를 최소화하는 비손실 압축 기법이다. 달리 말하자면, “더 자주 반복되는 문자를 더 적은 비트로 표현함으로써 압축의 목적을 달성한다”[[INFO0](https://www.notion.so/INFO0-Does-the-removal-of-repetition-play-a-certain-role-in-intelligence-44649a6871fd4cc091e59ca9a98fc127?pvs=21)].

한편 JPEG 압축 기법은 푸리에 변환을 통해 고주파 신호를 제거하고, 그 외 인간이 제대로 인지하지 못하는 영역의 정보를 제거함으로써 압축의 목적을 달성한다. 따라서 JPEG 압축 기법은 허프만 코딩 기법과 달리 정보가 소실되는 손실 압축 기법이다[[INFO1](https://www.notion.so/INFO1-JPEG-is-a-lossy-compression-which-achieves-10-1-compression-ratio-by-omitting-the-informatio-92e940e205004a8b991c7f27063bd8a7?pvs=21)][[INFO1a](https://www.notion.so/INFO1a-Better-explained-version-of-JPEG-b9798725c33c466fbfb1f9a0bdfb7c7f?pvs=21)].

깊이가 충분히 깊은 인공신경망은 임의의 연속함수를 근사할 수 있는 것으로 알려져 있다[[ARINT0](https://www.notion.so/ARINT0-Sufficiently-large-neural-networks-can-approximate-any-continuous-functions-from-mathbb-R--c5da884bac624b0698239138f12653d2?pvs=21)]. 여기서 발상을 조금 비틀어보자. 함수를 일정 오차 이내로 근사하기 위해 필요한 인공신경망 내의 가중치의 개수가, 해당 함수를 표현하기 위한 최소 정보량을 탐구하는 좋은 발판을 제공할 수 있지 않을까?

선형대수학의 핵심 개념 중 하나인 행렬과 선형 변환의 일대일 대응 관계는 모든 선형 함수가 그저 격자 상의 수의 나열에 불과하다는 것을 밝혀준다[[MATH0](https://www.notion.so/MATH0-We-should-study-matrices-which-is-a-table-of-numbers-instead-of-studying-linear-transforma-b154f6b2b86c4ba5a4186b27615b894a?pvs=21)]. 만일 이 사실이 비선형 함수에 대해서도 확장될 수 있다면, 즉 비선형 함수 또한 수의 나열에 불과하다면, 정보 압축 이론을 인공 신경망 근사 이론에 적용할 여지가 충분하다. 나아가, 두 학문이 거대 이론의 그늘 아래 통합될 가능성 또한 배제할 수 없다.

우선 정보 압축의 관점을 선형대수학에 적용하는 연구가 선행되어야 할 것이다. 대각화[[MATH1](https://www.notion.so/MATH1-If-we-find-a-certain-basis-of-symmetry-we-can-simplify-the-entries-of-a-matrix-in-a-diagonal-7941e0d1845b45b9aa39cba3d9633531?pvs=21)]나 SVD[[MATH2](https://www.notion.so/MATH2-We-can-sort-the-singular-values-Sigma_-ii-in-descending-order-and-omit-the-smallest-ones-t-87de4bb7dfca4c4590e615bc20e3cfa1?pvs=21)]의 예시는 각각 비손실 압축과 손실 압축의 예시와 잘 대응된다. 적절한 기저를 찾아 대각화를 거친 행렬이 대각 행렬의 성분만으로 선형 변환을 온전하게 표현한다는 점, 크기가 작은 특이값을 생략하더라도 근사적으로 비슷한 선형 변환을 만들 수 있다는 점에서 대각화와 SVD를 이용한 근사는 각각 비손실 압축, 손실 압축의 개념과 유사하다.

## Research questions & ideas

[**Weakly equivariant neural networks**](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/Weakly%20equivariant%20neural%20networks%20198f0f24f93180ae93d9d4be37fff35f.md)

- **one possible idea is that we can modify the definition of loss sublevel set.**
    - Right now we have $S(\epsilon) = \{w: L(w) \le \epsilon\}$ with $L$ as empirical cross entropy loss. We can maybe redefine it so that we have it rather as a distance toward the manifold as well (this we need to define). Then we changed the Gaussian width of the level set as in the bound.
    - The tricky part might be that the loss function is not a metric... We could have done something like a metric $d(f(x), y) \le d(f(x), M) + d(M, y),$  where $M$ is a manifold, and then we can bound them by treating them separately by controlling $\epsilon$.
- **Maybe we can find connections with optimal transport.**
    - Cross entropy is defined as $H(p, q) = - E_{p}[\log q] = H(p) + D_{KL}(p || q)$. There’s a paper connecting manifold hypothesis as $p_M(x) = \text{indicate}\{M\}(x) \rho(x)$.... So we might do triangular inequality to get $D_{KL}(p||q) \le D_{KL}(p || p_M) + D_{KL}(p_M || q)$. Maybe we can set some bounds on these two terms. For instance, we may set our level set to be $S'(w) = \{w  : L(s) = D_{KL}(p || q_M)  <\epsilon/2,\;D_{KL}(q_M|| q) <\epsilon/2\}$, and somehow find a relationship  with the gaussian width e.g. $w(S) > w(S')$.
        - $D_{KL}(p_M||q) = 0$ because we assume manifold hypothesis?
- **사실 인간의 뇌 안에 저장된 prior knowledge(=기저)를 생략했기 때문에 손실 압축처럼 느껴질 뿐, 알고보니 jpeg 이미지는 비손실 압축에 가까운 것이 아닐까?**
    - 인지하는 정보의 관점으로 볼 때는 원본과 jpeg의 차이가 미미하다. 우리의 visual cortex가 저장하고 있는 기저는 무엇일까? [0b. 함축어](0b%20%ED%95%A8%EC%B6%95%EC%96%B4%20a6a816e65ee84e5aad89b1bed982ff9f.md)에서 ”‘너’와 ‘나’ 사이의 정보를 생략한다“는 개념과 연관해서 생각해보자.
    - “진폭이 크고 느리게 진동하는 정보는 눈에 띄지만 자글자글한 디테일에 관한 정보는 인지하기 어려우므로 생략한다”는 것 자체가 visual cortex에 내장된 기저를 활용한 구조 아닐까?
    - A neural network that has three distinct types of layer that rearranges itself quick / slow / fast.
        - 홍철: On the Spectral Bias of Neural Networks 추천함 [[ARINT7]](https://www.notion.so/ARINT7-Neural-networks-prioritize-learning-low-frequency-mappings-first-164f0f24f93180d28bd1eb5284a55e45?pvs=21).
- **If you are to perform different but related tasks, how would you construct and train your model? By related, I mean that there are similarities for some parts in the task, just as if sipping coffee and watering a plant both involves "grabbing".**
    - A model that analyzes the similarities and differences of tasks it needs to handle, and learns each part separately to later combine them into one.
    - Given a distribution of tasks $\mathcal{D}$, where each sampled task $F \sim \mathcal{D}$ is a Lebesgue-integrable function from $\mathbb{R}^n$ to $\mathbb{R}^m$, and  given the norm $\|\cdot\|$ defined by the inner product $\left<f,g\right>=\int_{\mathbb{R}^n}w(\mathbf{x})\{f(\mathbf{x})\cdot g(\mathbf{x})\} d\mathbf{x}$, what is the most efficient parametrized basis $\mathcal{B}_\theta = \{f_1(\theta), f_2(\theta), \cdots,f_d(\theta) \}$, i.e.,
        
        $$
        \underset{\theta \in \mathbb{R}^p} {\textrm{minimize}} \;\;\mathbb{E}_{F \sim \mathcal{D}} \left[ \left\| \sum_{i=1}^dC_i(\theta)f_i(\theta) - F \right\|^2 \right]
        $$
        
        where $C_i(\theta):=\left<f_i(\theta), F\right>$.
        
- **Connection to perturbation theory : We use the original eigenstates of the Hamiltonian, even after the perturbation. Why not make corrections to the basis state itself?**
    - Why do you have to use the original energy eigenstates to approximate the perturbed state? Is it actually a good basis, in the sense of minimizing the amount of data needed to describe the full system?
    - 슈뢰딩거 방정식을 풀어야하는 이유가 사실은 언어가 충분히 발달하지 못해서였다면? 퍼텐셜에 대한 정보로부터 기저가 자연스럽게 찾아지는 수학적인 언어가 있을 수도 있지 않을까. 컴퓨터의 언어는 대단히 rich하다. 그래서 시뮬레이션을 한다는 것, 즉 컴퓨터의 언어로 분석한다는 시도는 꽤나 타당하다.
    - $\ket{\psi}$라는 기호를 볼 때 우리는 그 안에 들어 있는 선형대수적인 구조(힐베르트 공간, 선형성, 교환 법칙, …)를 저절로 연상하지만, 실제로 기호 자체만 놓고 보면 아무런 정보를 가지고 있지 않다. 복잡하지만 정돈된 패턴이 반복될 때, 그것을 같은 기호로 치환하면 표기가 이해하기도 쉽고 간단해진다. 새로운 표기를 필요에 따라 도입할 수 있는 인공지능 모델을 만들고자 하면 무엇을 해야 하는가? (도입어: 적합한 표현의 탄생)
    - 물리학 공식은 사람들끼리 (관측 데이터로부터 얻어낸) 자연 현상의 패턴을 함축적으로 기술하기 위해 사용하는 표현 방식이다. 그런데 이 공식이라는 것이 대단히 많은 정보를 포함하고 있는 것으로 보일지 몰라도, 사실은 읽는 이의 배경지식을 가정하기 때문에 실제로 포함된 있는 정보량은 매우 적다. 예를 들어서 $\mathbf{F}=m\mathbf{a}$라는 공식을 보았을 때, 등호의 의미를 모르면 좌변과 우변이 같다는 사실을 모를 것이고, $\mathbf{F}$가 힘을 의미하고 $m$이 질량을 의미하고 $\mathbf{a}$가 가속도, 즉 위치의 시간에 대한 이계도함수를 의미한다는 것을 모르면 말짱 도루묵이다. 즉, 공식에 대한 구체적인 정보는 사람의 뇌 안에 저장되어 있는거고, **공식의 실체는 뇌에 저장된 정보를 인출하기 위한 첫번째 불꽃에 불과하다.** 사람이 만들어낸 지식의 체계는 결국은 서로 다른 벡터들의 연결 관계를 나타내는 것일 뿐이다.
- **SVD는 기저 변환을 통해 대부분의 항들을 0으로 만들어버리고 기저들을 중요도 순으로 나열까지 해버린다. SVD를 비선형 버전으로 확장한 무엇인가를 상상하게 만든다. 정보 압축을 적절한 “비선형 기저”를 찾는 것으로 정의한다면, 인공신경망이 이미 그 역할을 수행하고 있는 것이지는 않을까?**
    - 홍철: kernelized SVD 읽어볼 것
    - Extend ideas from rate-distortion theory. For $X$ a matrix sampled from a distribution $\mathcal{D}$ and $\tilde{X}$ a list of nonzero singular values calculated from $X$ (which have been subsequently quantized via storing in low resolution floating point number representations, ex: float16)
        
        $$
        \begin{aligned}&\underset{p(\tilde{x}|x)}{\text{minimize}}\;I(X;\tilde{X})\\ &\text{subject to} \; \left< d(x,\tilde{x}) \right>_{p(x,\tilde{x})}\le D \end{aligned}
        $$
        
    - The problem is that $p(\tilde{x}|x)$ is deterministic, for the case of SVD. What if we can allow for some randomness when applying SVD, so that we can further compress the representation? But is this even a good idea?
    - Note that $I(X; \tilde{X})=H(\tilde{X})-H(\tilde{X}|X)$ and that $H(\tilde{X}|X)=0$ when $\tilde{X}$ is deterministic given $X$. So we’re basically left with the minimization of $H(\tilde{X})$.
- **Mandelbrot Set이나 Bifurcation Diagram과 같이 복잡한 구조가 굉장히 단순한 수식에 내포되어 있기도 하다. 이런 구조를 차용한 정보 압축 알고리즘을 고안할 수 있을까?**
    - 인공신경망으로 MB set과 같은 프랙탈 구조나 BF diagram을 분석하여 카오스적인 계에 대한 인사이트를 도출할 수는 없을까?
    - Fractal compression과 collage theorem 찾아볼 것
- **Connection to complexity theory: There are $O(n^2)$ and  $O(n\log n)$ algorithms which all perform the same task - sorting. Can we argue that one is a lossless compression of the other, since it uses less computation?**

[Optimal Transport](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/Optimal%20Transport%2081e55a01ffab48edac85d3007511040c.md)

[Mean Flow Q-Learning](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/Mean%20Flow%20Q-Learning%20239f0f24f93180d8974bf72030505bfa.md)

[Autoregression RL Model Feedback](0%20%EC%A0%95%EB%B3%B4%20%EC%95%95%EC%B6%95%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EA%B7%BC%EC%82%AC%20%EC%9D%B4%EB%A1%A0%EC%9D%98%20%ED%86%B5%ED%95%A9%20%EA%B0%80%EB%8A%A5%EC%84%B1%2066c9bb6b4d324a05a29398c689cca37c/Autoregression%20RL%20Model%20Feedback%2024af0f24f93180c1a55ec0debe788f51.md)