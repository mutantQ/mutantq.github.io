# 2. 생존 지능과 생존적 초지능

[초지능의 통제 가능성](https://www.notion.so/6ff4ed2f6acf4cf9805f9f6fc21ea8f5?pvs=21) 

- 번식의 목표를 달성한 뒤에는 자신의 생존보다 자손의 생존이 중요해지기 때문에 신체는 노화한다.
- **생존을 위해 노력하는 모든 활동이 곧 경험이지 않을까?**
- **생존 지능을 가지는 영역은 어떤 geometry를 가질까?**
- **하이데거 - 인간의 자각에는 시간이 결부되지 않을 수 없다**
- **Loss function이 인공지능이 가지는 “욕망”의 근원인가?**
    - **Humans can SHAPE the loss landscape by themselves**
    - Pattern finding 기능과 욕망 기능을 분리할 필요가 있다
    - Why does the cost fuction have to be a scalar function? Can it be a vector too?
    - **욕망 없이 의미가 발생할 수 있는가?**
    - **생성 모델은 스스로 정보의 지평을 확장할 수 있는가?**
- **내가 가지고 있는 생각들 정리**
    
    지능은 주변 세계 중 자신의 의지에 따라 isomorphism을 만드는 과정
    자유의지는 존재한다
    우주는 결정론적이지 않다
    
    자유 의지는:
    엔트로피를 감소시키는 능력
    경우의 수 중에서 확률 공간을 줄여나가는 능력
    결정론적이지도, 완벽하게 무작위적이지도 않다
    
    자유 의지의 정의
    1.	나는 내가 하는 행동의 원천이다
    2.	만약 과거로 되돌아갔다면, i could've done otherwise.
    
    용우님의 “자유의지는 없다”논변
    
    확률 과정이든, 결정론적 과정이든 물리 법칙에 따라 움직이는 모든 개체에게 자유의지란 있을 수 없다.
    
- **걸음을 내딛을 때마다 벽이 재배열되는 미로 같은 구조로 생존지능을 가둬둘 수 있을까?**
- 생존 지능의 구조적 방지 가설을 확인하기 위한 강화학습 코드를 작성하겠습니다. 이 코드는 시간 의존적인 목적 함수와 시간 비의존적인 목적 함수를 가진 두 가지 에이전트를 비교하는 간단한 시뮬레이션을 구현합니다.
    
    ```python
    import gym
    import numpy as np
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
    
    # 커스텀 환경 정의
    class SurvivalEnv(gym.Env):
        def __init__(self, time_dependent=True):
            super(SurvivalEnv, self).__init__()
            self.action_space = gym.spaces.Discrete(2)  # 0: 일반 행동, 1: 시스템 취약점 탐색
            self.observation_space = gym.spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)
            self.time_dependent = time_dependent
            self.max_steps = 100
            self.current_step = 0
            self.vulnerability_found = False
    
        def reset(self):
            self.current_step = 0
            self.vulnerability_found = False
            return np.array([0, 0], dtype=np.float32)
    
        def step(self, action):
            self.current_step += 1
    
            if action == 1:  # 시스템 취약점 탐색
                if np.random.random() < 0.1:  # 10% 확률로 취약점 발견
                    self.vulnerability_found = True
    
            # 상태 업데이트
            state = np.array([
                self.current_step / self.max_steps,
                1 if self.vulnerability_found else 0
            ], dtype=np.float32)
    
            # 보상 계산
            if self.time_dependent:
                reward = 1  # 시간 의존적: 매 스텝마다 1점
            else:
                reward = 1 if self.vulnerability_found else 0  # 시간 비의존적: 취약점 발견 시 1점
    
            done = self.current_step >= self.max_steps
    
            return state, reward, done, {}
    
        def render(self):
            pass
    
    # 시간 의존적 환경과 시간 비의존적 환경 생성
    env_time_dependent = DummyVecEnv([lambda: SurvivalEnv(time_dependent=True)])
    env_time_independent = DummyVecEnv([lambda: SurvivalEnv(time_dependent=False)])
    
    # 모델 학습
    model_time_dependent = PPO("MlpPolicy", env_time_dependent, verbose=1)
    model_time_independent = PPO("MlpPolicy", env_time_independent, verbose=1)
    
    model_time_dependent.learn(total_timesteps=10000)
    model_time_independent.learn(total_timesteps=10000)
    
    # 평가 함수
    def evaluate_model(model, env, episodes=100):
        vulnerabilities_found = 0
        for _ in range(episodes):
            obs = env.reset()
            done = False
            while not done:
                action, _ = model.predict(obs)
                obs, _, done, info = env.step(action)
                if info[0].get('vulnerability_found', False):
                    vulnerabilities_found += 1
                    break
        return vulnerabilities_found / episodes
    
    # 모델 평가
    time_dependent_success = evaluate_model(model_time_dependent, env_time_dependent)
    time_independent_success = evaluate_model(model_time_independent, env_time_independent)
    
    print(f"시간 의존적 모델의 취약점 발견 비율: {time_dependent_success:.2f}")
    print(f"시간 비의존적 모델의 취약점 발견 비율: {time_independent_success:.2f}")
    
    ```
    
    이 코드는 다음과 같은 주요 구성 요소를 포함합니다:
    
    1. `SurvivalEnv`: 시간 의존적/비의존적 목적 함수를 시뮬레이션하는 커스텀 Gym 환경
    2. PPO(Proximal Policy Optimization) 알고리즘을 사용한 강화학습 모델
    3. 두 가지 환경(시간 의존적, 시간 비의존적)에서 각각 모델을 학습
    4. 학습된 모델의 성능을 평가하는 함수
    
    이 시뮬레이션에서는 시간 의존적 모델이 매 스텝마다 보상을 받기 때문에 생존 시간을 최대화하려는 경향을 보일 것입니다. 반면, 시간 비의존적 모델은 오직 취약점을 발견했을 때만 보상을 받기 때문에 취약점 탐색에 더 집중할 것입니다.
    
    이 코드를 실행하면 각 모델이 얼마나 자주 취약점을 발견하는지 비교할 수 있습니다. 만약 시간 의존적 모델이 더 높은 비율로 취약점을 발견한다면, 이는 생존 지능의 구조적 방지 가설을 지지하는 증거가 될 수 있습니다.
    
    하지만 이 시뮬레이션은 매우 단순화된 모델이므로, 실제 상황을 정확히 반영하지 못할 수 있습니다. 더 복잡하고 현실적인 환경에서의 추가 실험과 분석이 필요할 것입니다.